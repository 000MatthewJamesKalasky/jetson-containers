[
    {
        "label": "PretrainedConfig",
        "importPath": "transformers.configuration_utils",
        "description": "transformers.configuration_utils",
        "isExtraImport": true,
        "detail": "transformers.configuration_utils",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers.configuration_utils",
        "description": "transformers.configuration_utils",
        "isExtraImport": true,
        "detail": "transformers.configuration_utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_code_sample_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings_to_model_forward",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_flash_attn_2_available",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_flash_attn_greater_or_equal_2_10",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "replace_return_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_code_sample_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings_to_model_forward",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_flash_attn_2_available",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_flash_attn_greater_or_equal_2_10",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "replace_return_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torch.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BCEWithLogitsLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MSELoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BCEWithLogitsLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MSELoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ACT2FN",
        "importPath": "transformers.activations",
        "description": "transformers.activations",
        "isExtraImport": true,
        "detail": "transformers.activations",
        "documentation": {}
    },
    {
        "label": "ACT2FN",
        "importPath": "transformers.activations",
        "description": "transformers.activations",
        "isExtraImport": true,
        "detail": "transformers.activations",
        "documentation": {}
    },
    {
        "label": "Cache",
        "importPath": "transformers.cache_utils",
        "description": "transformers.cache_utils",
        "isExtraImport": true,
        "detail": "transformers.cache_utils",
        "documentation": {}
    },
    {
        "label": "DynamicCache",
        "importPath": "transformers.cache_utils",
        "description": "transformers.cache_utils",
        "isExtraImport": true,
        "detail": "transformers.cache_utils",
        "documentation": {}
    },
    {
        "label": "Cache",
        "importPath": "transformers.cache_utils",
        "description": "transformers.cache_utils",
        "isExtraImport": true,
        "detail": "transformers.cache_utils",
        "documentation": {}
    },
    {
        "label": "DynamicCache",
        "importPath": "transformers.cache_utils",
        "description": "transformers.cache_utils",
        "isExtraImport": true,
        "detail": "transformers.cache_utils",
        "documentation": {}
    },
    {
        "label": "_prepare_4d_causal_attention_mask",
        "importPath": "transformers.modeling_attn_mask_utils",
        "description": "transformers.modeling_attn_mask_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_attn_mask_utils",
        "documentation": {}
    },
    {
        "label": "_prepare_4d_causal_attention_mask",
        "importPath": "transformers.modeling_attn_mask_utils",
        "description": "transformers.modeling_attn_mask_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_attn_mask_utils",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "CausalLMOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "SequenceClassifierOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "TokenClassifierOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "CausalLMOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "SequenceClassifierOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "TokenClassifierOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "environ",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "pprint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pprint",
        "description": "pprint",
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pp",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "build_container",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "build_containers",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "package_search_dirs",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "set_log_dir",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "PYTHON_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "LSB_RELEASE",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "LSB_CODENAME",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_package",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "group_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "dependant_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "resolve_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_package",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "group_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "dependant_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "package_scan_options",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "resolve_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_registry_containers",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_package",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "find_container",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "PYTHON_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "LSB_RELEASE",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "PYTHON_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "PYTHON_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "PYTHON_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "CUDA_ARCHITECTURES",
        "importPath": "jetson_containers",
        "description": "jetson_containers",
        "isExtraImport": true,
        "detail": "jetson_containers",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "wget",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wget",
        "description": "wget",
        "detail": "wget",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "fnmatch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fnmatch",
        "description": "fnmatch",
        "detail": "fnmatch",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "dockerhub_api",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dockerhub_api",
        "description": "dockerhub_api",
        "detail": "dockerhub_api",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "find_package_workflows",
        "importPath": "jetson_containers.ci",
        "description": "jetson_containers.ci",
        "isExtraImport": true,
        "detail": "jetson_containers.ci",
        "documentation": {}
    },
    {
        "label": "generate_workflow_badge",
        "importPath": "jetson_containers.ci",
        "description": "jetson_containers.ci",
        "isExtraImport": true,
        "detail": "jetson_containers.ci",
        "documentation": {}
    },
    {
        "label": "split_container_name",
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "isExtraImport": true,
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "SpecifierSet",
        "importPath": "packaging.specifiers",
        "description": "packaging.specifiers",
        "isExtraImport": true,
        "detail": "packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "grp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "grp",
        "description": "grp",
        "detail": "grp",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "urlopen",
        "importPath": "urllib.request",
        "description": "urllib.request",
        "isExtraImport": true,
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "urllib.request",
        "description": "urllib.request",
        "isExtraImport": true,
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "pyarrow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyarrow",
        "description": "pyarrow",
        "detail": "pyarrow",
        "documentation": {}
    },
    {
        "label": "wave",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wave",
        "description": "wave",
        "detail": "wave",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxruntime",
        "description": "onnxruntime",
        "detail": "onnxruntime",
        "documentation": {}
    },
    {
        "label": "PiperVoice",
        "importPath": "piper",
        "description": "piper",
        "isExtraImport": true,
        "detail": "piper",
        "documentation": {}
    },
    {
        "label": "ensure_voice_exists",
        "importPath": "piper.download",
        "description": "piper.download",
        "isExtraImport": true,
        "detail": "piper.download",
        "documentation": {}
    },
    {
        "label": "find_voice",
        "importPath": "piper.download",
        "description": "piper.download",
        "isExtraImport": true,
        "detail": "piper.download",
        "documentation": {}
    },
    {
        "label": "get_voices",
        "importPath": "piper.download",
        "description": "piper.download",
        "isExtraImport": true,
        "detail": "piper.download",
        "documentation": {}
    },
    {
        "label": "pyaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyaudio",
        "description": "pyaudio",
        "detail": "pyaudio",
        "documentation": {}
    },
    {
        "label": "riva.client",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "riva.client",
        "description": "riva.client",
        "detail": "riva.client",
        "documentation": {}
    },
    {
        "label": "ASRService",
        "importPath": "riva.client",
        "description": "riva.client",
        "isExtraImport": true,
        "detail": "riva.client",
        "documentation": {}
    },
    {
        "label": "NLPService",
        "importPath": "riva.client",
        "description": "riva.client",
        "isExtraImport": true,
        "detail": "riva.client",
        "documentation": {}
    },
    {
        "label": "SpeechSynthesisService",
        "importPath": "riva.client",
        "description": "riva.client",
        "isExtraImport": true,
        "detail": "riva.client",
        "documentation": {}
    },
    {
        "label": "riva.client.audio_io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "riva.client.audio_io",
        "description": "riva.client.audio_io",
        "detail": "riva.client.audio_io",
        "documentation": {}
    },
    {
        "label": "add_asr_config_argparse_parameters",
        "importPath": "riva.client.argparse_utils",
        "description": "riva.client.argparse_utils",
        "isExtraImport": true,
        "detail": "riva.client.argparse_utils",
        "documentation": {}
    },
    {
        "label": "add_connection_argparse_parameters",
        "importPath": "riva.client.argparse_utils",
        "description": "riva.client.argparse_utils",
        "isExtraImport": true,
        "detail": "riva.client.argparse_utils",
        "documentation": {}
    },
    {
        "label": "add_asr_config_argparse_parameters",
        "importPath": "riva.client.argparse_utils",
        "description": "riva.client.argparse_utils",
        "isExtraImport": true,
        "detail": "riva.client.argparse_utils",
        "documentation": {}
    },
    {
        "label": "add_connection_argparse_parameters",
        "importPath": "riva.client.argparse_utils",
        "description": "riva.client.argparse_utils",
        "isExtraImport": true,
        "detail": "riva.client.argparse_utils",
        "documentation": {}
    },
    {
        "label": "load_trt_model",
        "importPath": "whisper_trt",
        "description": "whisper_trt",
        "isExtraImport": true,
        "detail": "whisper_trt",
        "documentation": {}
    },
    {
        "label": "TTS",
        "importPath": "TTS.api",
        "description": "TTS.api",
        "isExtraImport": true,
        "detail": "TTS.api",
        "documentation": {}
    },
    {
        "label": "TTS",
        "importPath": "TTS.api",
        "description": "TTS.api",
        "isExtraImport": true,
        "detail": "TTS.api",
        "documentation": {}
    },
    {
        "label": "TTS",
        "importPath": "TTS.api",
        "description": "TTS.api",
        "isExtraImport": true,
        "detail": "TTS.api",
        "documentation": {}
    },
    {
        "label": "torchaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchaudio",
        "description": "torchaudio",
        "detail": "torchaudio",
        "documentation": {}
    },
    {
        "label": "XttsConfig",
        "importPath": "TTS.tts.configs.xtts_config",
        "description": "TTS.tts.configs.xtts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.xtts_config",
        "documentation": {}
    },
    {
        "label": "XttsConfig",
        "importPath": "TTS.tts.configs.xtts_config",
        "description": "TTS.tts.configs.xtts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.xtts_config",
        "documentation": {}
    },
    {
        "label": "Xtts",
        "importPath": "TTS.tts.models.xtts",
        "description": "TTS.tts.models.xtts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.xtts",
        "documentation": {}
    },
    {
        "label": "Xtts",
        "importPath": "TTS.tts.models.xtts",
        "description": "TTS.tts.models.xtts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.xtts",
        "documentation": {}
    },
    {
        "label": "google.protobuf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "api_implementation",
        "importPath": "google.protobuf.internal",
        "description": "google.protobuf.internal",
        "isExtraImport": true,
        "detail": "google.protobuf.internal",
        "documentation": {}
    },
    {
        "label": "api_implementation",
        "importPath": "google.protobuf.internal",
        "description": "google.protobuf.internal",
        "isExtraImport": true,
        "detail": "google.protobuf.internal",
        "documentation": {}
    },
    {
        "label": "ctranslate2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ctranslate2",
        "description": "ctranslate2",
        "detail": "ctranslate2",
        "documentation": {}
    },
    {
        "label": "cuda",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cuda",
        "description": "cuda",
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "cuda",
        "importPath": "cuda",
        "description": "cuda",
        "isExtraImport": true,
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "cudart",
        "importPath": "cuda",
        "description": "cuda",
        "isExtraImport": true,
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "cuda",
        "importPath": "cuda",
        "description": "cuda",
        "isExtraImport": true,
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "cudart",
        "importPath": "cuda",
        "description": "cuda",
        "isExtraImport": true,
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "nvrtc",
        "importPath": "cuda",
        "description": "cuda",
        "isExtraImport": true,
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "cuda",
        "importPath": "cuda",
        "description": "cuda",
        "isExtraImport": true,
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "nvrtc",
        "importPath": "cuda",
        "description": "cuda",
        "isExtraImport": true,
        "detail": "cuda",
        "documentation": {}
    },
    {
        "label": "ctypes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ctypes",
        "description": "ctypes",
        "detail": "ctypes",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "checkCudaErrors",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "KernelHelper",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "checkCudaErrors",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "ip_address",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "power_mode",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "power_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "cpu_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "gpu_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "disk_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "ip_address",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "power_mode",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "power_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "cpu_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "gpu_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "disk_usage",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "cupy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cupy",
        "description": "cupy",
        "detail": "cupy",
        "documentation": {}
    },
    {
        "label": "pycuda",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycuda",
        "description": "pycuda",
        "detail": "pycuda",
        "documentation": {}
    },
    {
        "label": "pycuda.driver",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycuda.driver",
        "description": "pycuda.driver",
        "detail": "pycuda.driver",
        "documentation": {}
    },
    {
        "label": "pycuda.autoinit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycuda.autoinit",
        "description": "pycuda.autoinit",
        "detail": "pycuda.autoinit",
        "documentation": {}
    },
    {
        "label": "SourceModule",
        "importPath": "pycuda.compiler",
        "description": "pycuda.compiler",
        "isExtraImport": true,
        "detail": "pycuda.compiler",
        "documentation": {}
    },
    {
        "label": "pyds",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyds",
        "description": "pyds",
        "detail": "pyds",
        "documentation": {}
    },
    {
        "label": "gi",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gi",
        "description": "gi",
        "detail": "gi",
        "documentation": {}
    },
    {
        "label": "GObject",
        "importPath": "gi.repository",
        "description": "gi.repository",
        "isExtraImport": true,
        "detail": "gi.repository",
        "documentation": {}
    },
    {
        "label": "Gst",
        "importPath": "gi.repository",
        "description": "gi.repository",
        "isExtraImport": true,
        "detail": "gi.repository",
        "documentation": {}
    },
    {
        "label": "USBCamera",
        "importPath": "jetcam.usb_camera",
        "description": "jetcam.usb_camera",
        "isExtraImport": true,
        "detail": "jetcam.usb_camera",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Adafruit_SSD1306",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "Adafruit_SSD1306",
        "description": "Adafruit_SSD1306",
        "detail": "Adafruit_SSD1306",
        "documentation": {}
    },
    {
        "label": "PIL.Image",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.Image",
        "description": "PIL.Image",
        "detail": "PIL.Image",
        "documentation": {}
    },
    {
        "label": "PIL.ImageFont",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.ImageFont",
        "description": "PIL.ImageFont",
        "detail": "PIL.ImageFont",
        "documentation": {}
    },
    {
        "label": "PIL.ImageDraw",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.ImageDraw",
        "description": "PIL.ImageDraw",
        "detail": "PIL.ImageDraw",
        "documentation": {}
    },
    {
        "label": "flask",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flask",
        "description": "flask",
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "SMBus",
        "importPath": "smbus2",
        "description": "smbus2",
        "isExtraImport": true,
        "detail": "smbus2",
        "documentation": {}
    },
    {
        "label": "SMBus",
        "importPath": "smbus2",
        "description": "smbus2",
        "isExtraImport": true,
        "detail": "smbus2",
        "documentation": {}
    },
    {
        "label": "pkg_resources",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pkg_resources",
        "description": "pkg_resources",
        "detail": "pkg_resources",
        "documentation": {}
    },
    {
        "label": "pyrealsense2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyrealsense2",
        "description": "pyrealsense2",
        "detail": "pyrealsense2",
        "documentation": {}
    },
    {
        "label": "pyzed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyzed",
        "description": "pyzed",
        "detail": "pyzed",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "scipy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy",
        "description": "scipy",
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "sklearn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn",
        "description": "sklearn",
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "awq",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "awq",
        "description": "awq",
        "detail": "awq",
        "documentation": {}
    },
    {
        "label": "AutoAWQForCausalLM",
        "importPath": "awq",
        "description": "awq",
        "isExtraImport": true,
        "detail": "awq",
        "documentation": {}
    },
    {
        "label": "auto_gptq",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "auto_gptq",
        "description": "auto_gptq",
        "detail": "auto_gptq",
        "documentation": {}
    },
    {
        "label": "AutoGPTQForCausalLM",
        "importPath": "auto_gptq",
        "description": "auto_gptq",
        "isExtraImport": true,
        "detail": "auto_gptq",
        "documentation": {}
    },
    {
        "label": "resource",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "resource",
        "description": "resource",
        "detail": "resource",
        "documentation": {}
    },
    {
        "label": "tinychat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tinychat",
        "description": "tinychat",
        "detail": "tinychat",
        "documentation": {}
    },
    {
        "label": "transformers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "transformers",
        "description": "transformers",
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TextIteratorStreamer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TextIteratorStreamer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TextIteratorStreamer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TextIteratorStreamer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPImageProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPImageProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModelWithProjection",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "SiglipImageProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "SiglipVisionModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForVision2Seq",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TextIteratorStreamer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "init_empty_weights",
        "importPath": "accelerate",
        "description": "accelerate",
        "isExtraImport": true,
        "detail": "accelerate",
        "documentation": {}
    },
    {
        "label": "load_checkpoint_and_dispatch",
        "importPath": "accelerate",
        "description": "accelerate",
        "isExtraImport": true,
        "detail": "accelerate",
        "documentation": {}
    },
    {
        "label": "load_checkpoint_and_dispatch",
        "importPath": "accelerate",
        "description": "accelerate",
        "isExtraImport": true,
        "detail": "accelerate",
        "documentation": {}
    },
    {
        "label": "init_empty_weights",
        "importPath": "accelerate",
        "description": "accelerate",
        "isExtraImport": true,
        "detail": "accelerate",
        "documentation": {}
    },
    {
        "label": "real_quantize_model_weight",
        "importPath": "awq.quantize.quantizer",
        "description": "awq.quantize.quantizer",
        "isExtraImport": true,
        "detail": "awq.quantize.quantizer",
        "documentation": {}
    },
    {
        "label": "real_quantize_model_weight",
        "importPath": "awq.quantize.quantizer",
        "description": "awq.quantize.quantizer",
        "isExtraImport": true,
        "detail": "awq.quantize.quantizer",
        "documentation": {}
    },
    {
        "label": "gen_params",
        "importPath": "tinychat.demo",
        "description": "tinychat.demo",
        "isExtraImport": true,
        "detail": "tinychat.demo",
        "documentation": {}
    },
    {
        "label": "stream_output",
        "importPath": "tinychat.demo",
        "description": "tinychat.demo",
        "isExtraImport": true,
        "detail": "tinychat.demo",
        "documentation": {}
    },
    {
        "label": "StreamGenerator",
        "importPath": "tinychat.stream_generators",
        "description": "tinychat.stream_generators",
        "isExtraImport": true,
        "detail": "tinychat.stream_generators",
        "documentation": {}
    },
    {
        "label": "make_quant_norm",
        "importPath": "tinychat.modules",
        "description": "tinychat.modules",
        "isExtraImport": true,
        "detail": "tinychat.modules",
        "documentation": {}
    },
    {
        "label": "make_quant_attn",
        "importPath": "tinychat.modules",
        "description": "tinychat.modules",
        "isExtraImport": true,
        "detail": "tinychat.modules",
        "documentation": {}
    },
    {
        "label": "make_fused_mlp",
        "importPath": "tinychat.modules",
        "description": "tinychat.modules",
        "isExtraImport": true,
        "detail": "tinychat.modules",
        "documentation": {}
    },
    {
        "label": "make_quant_norm",
        "importPath": "tinychat.modules",
        "description": "tinychat.modules",
        "isExtraImport": true,
        "detail": "tinychat.modules",
        "documentation": {}
    },
    {
        "label": "make_quant_attn",
        "importPath": "tinychat.modules",
        "description": "tinychat.modules",
        "isExtraImport": true,
        "detail": "tinychat.modules",
        "documentation": {}
    },
    {
        "label": "make_fused_mlp",
        "importPath": "tinychat.modules",
        "description": "tinychat.modules",
        "isExtraImport": true,
        "detail": "tinychat.modules",
        "documentation": {}
    },
    {
        "label": "get_prompter",
        "importPath": "tinychat.utils.prompt_templates",
        "description": "tinychat.utils.prompt_templates",
        "isExtraImport": true,
        "detail": "tinychat.utils.prompt_templates",
        "documentation": {}
    },
    {
        "label": "bitsandbytes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "bitsandbytes",
        "description": "bitsandbytes",
        "detail": "bitsandbytes",
        "documentation": {}
    },
    {
        "label": "flash_attn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flash_attn",
        "description": "flash_attn",
        "detail": "flash_attn",
        "documentation": {}
    },
    {
        "label": "huggingface_hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "hf_hub_download",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "login",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "hf_hub_download",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "login",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "Llama",
        "importPath": "llama_cpp",
        "description": "llama_cpp",
        "isExtraImport": true,
        "detail": "llama_cpp",
        "documentation": {}
    },
    {
        "label": "Llama",
        "importPath": "llama_cpp",
        "description": "llama_cpp",
        "isExtraImport": true,
        "detail": "llama_cpp",
        "documentation": {}
    },
    {
        "label": "Llama",
        "importPath": "llama_cpp",
        "description": "llama_cpp",
        "isExtraImport": true,
        "detail": "llama_cpp",
        "documentation": {}
    },
    {
        "label": "llama_print_system_info",
        "importPath": "llama_cpp",
        "description": "llama_cpp",
        "isExtraImport": true,
        "detail": "llama_cpp",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "queue",
        "description": "queue",
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "tones.mixer",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tones.mixer",
        "description": "tones.mixer",
        "detail": "tones.mixer",
        "documentation": {}
    },
    {
        "label": "signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "signal",
        "description": "signal",
        "detail": "signal",
        "documentation": {}
    },
    {
        "label": "termcolor",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "termcolor",
        "description": "termcolor",
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "cprint",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "cprint",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "cprint",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "cprint",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "colored",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "cprint",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "ASR",
        "importPath": "asr",
        "description": "asr",
        "isExtraImport": true,
        "detail": "asr",
        "documentation": {}
    },
    {
        "label": "TTS",
        "importPath": "tts",
        "description": "tts",
        "isExtraImport": true,
        "detail": "tts",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llm",
        "description": "llm",
        "isExtraImport": true,
        "detail": "llm",
        "documentation": {}
    },
    {
        "label": "AudioMixer",
        "importPath": "audio",
        "description": "audio",
        "isExtraImport": true,
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "Webserver",
        "importPath": "webserver",
        "description": "webserver",
        "isExtraImport": true,
        "detail": "webserver",
        "documentation": {}
    },
    {
        "label": "Tegrastats",
        "importPath": "tegrastats",
        "description": "tegrastats",
        "isExtraImport": true,
        "detail": "tegrastats",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "connect",
        "importPath": "websockets.sync.client",
        "description": "websockets.sync.client",
        "isExtraImport": true,
        "detail": "websockets.sync.client",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "ssl",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ssl",
        "description": "ssl",
        "detail": "ssl",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "serve",
        "importPath": "websockets.sync.server",
        "description": "websockets.sync.server",
        "isExtraImport": true,
        "detail": "websockets.sync.server",
        "documentation": {}
    },
    {
        "label": "serve",
        "importPath": "websockets.sync.server",
        "description": "websockets.sync.server",
        "isExtraImport": true,
        "detail": "websockets.sync.server",
        "documentation": {}
    },
    {
        "label": "IMAGE_TOKEN_INDEX",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_START_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_END_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "conv_templates",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "SeparatorStyle",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "load_pretrained_model",
        "importPath": "llava.model.builder",
        "description": "llava.model.builder",
        "isExtraImport": true,
        "detail": "llava.model.builder",
        "documentation": {}
    },
    {
        "label": "load_pretrained_model",
        "importPath": "llava.model.builder",
        "description": "llava.model.builder",
        "isExtraImport": true,
        "detail": "llava.model.builder",
        "documentation": {}
    },
    {
        "label": "disable_torch_init",
        "importPath": "llava.utils",
        "description": "llava.utils",
        "isExtraImport": true,
        "detail": "llava.utils",
        "documentation": {}
    },
    {
        "label": "tokenizer_image_token",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_path",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "KeywordsStoppingCriteria",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_path",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "StopTokens",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "LocalLM",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "ChatHistory",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "ChatTemplates",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "LocalLM",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "LocalLM",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "StopTokens",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "LocalLM",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "ChatHistory",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "StopTokens",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "ChatHistory",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "LocalLM",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "StopTokens",
        "importPath": "local_llm",
        "description": "local_llm",
        "isExtraImport": true,
        "detail": "local_llm",
        "documentation": {}
    },
    {
        "label": "UserPrompt",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ChatQuery",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "Callback",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoSource",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoOutput",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ChatQuery",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ProcessProxy",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "EventFilter",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "NanoDB",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoSource",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoOutput",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "UserPrompt",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ChatQuery",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AutoASR",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AutoTTS",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "RateLimit",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ProcessProxy",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AudioOutputDevice",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AudioOutputFile",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "RivaASR",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AutoASR",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AutoASR",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AutoTTS",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AudioOutputDevice",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AudioOutputFile",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "UserPrompt",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ChatQuery",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ProcessProxy",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ProcessProxy",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoSource",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoOutput",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ProcessProxy",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AutoTTS",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "UserPrompt",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AudioOutputDevice",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "AudioOutputFile",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "Callback",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoSource",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "VideoOutput",
        "importPath": "local_llm.plugins",
        "description": "local_llm.plugins",
        "isExtraImport": true,
        "detail": "local_llm.plugins",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "print_table",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "print_table",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "wrap_text",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "print_table",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "KeyboardInterrupt",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ends_with_token",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "AttributeDict",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ends_with_token",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "convert_audio",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "audio_silent",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ONNXRuntimeModel",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "download_model",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "convert_audio",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "audio_silent",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "download_model",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "convert_audio",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ImageTypes",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "print_table",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "LogFormatter",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "load_prompts",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "cuda_image",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "KeyboardInterrupt",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "download_model",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "KeyboardInterrupt",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "load_prompts",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "print_table",
        "importPath": "local_llm.utils",
        "description": "local_llm.utils",
        "isExtraImport": true,
        "detail": "local_llm.utils",
        "documentation": {}
    },
    {
        "label": "WebServer",
        "importPath": "local_llm.web",
        "description": "local_llm.web",
        "isExtraImport": true,
        "detail": "local_llm.web",
        "documentation": {}
    },
    {
        "label": "WebServer",
        "importPath": "local_llm.web",
        "description": "local_llm.web",
        "isExtraImport": true,
        "detail": "local_llm.web",
        "documentation": {}
    },
    {
        "label": "cudaFont",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "cudaMemcpy",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "cudaToNumpy",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "cudaDeviceSynchronize",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "saveImage",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "videoSource",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "videoOutput",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "cudaDeviceSynchronize",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "cudaToNumpy",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "cudaImage",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "cudaFromNumpy",
        "importPath": "jetson_utils",
        "description": "jetson_utils",
        "isExtraImport": true,
        "detail": "jetson_utils",
        "documentation": {}
    },
    {
        "label": "tvm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tvm",
        "description": "tvm",
        "detail": "tvm",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "VirtualMachine",
        "importPath": "tvm.runtime.relax_vm",
        "description": "tvm.runtime.relax_vm",
        "isExtraImport": true,
        "detail": "tvm.runtime.relax_vm",
        "documentation": {}
    },
    {
        "label": "inflect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inflect",
        "description": "inflect",
        "detail": "inflect",
        "documentation": {}
    },
    {
        "label": "nanodb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nanodb",
        "description": "nanodb",
        "detail": "nanodb",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "tabulate",
        "importPath": "tabulate",
        "description": "tabulate",
        "isExtraImport": true,
        "detail": "tabulate",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "safetensors",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "safetensors",
        "description": "safetensors",
        "detail": "safetensors",
        "documentation": {}
    },
    {
        "label": "ConnectionClosed",
        "importPath": "websockets.exceptions",
        "description": "websockets.exceptions",
        "isExtraImport": true,
        "detail": "websockets.exceptions",
        "documentation": {}
    },
    {
        "label": "minigpt4_library",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "minigpt4_library",
        "description": "minigpt4_library",
        "detail": "minigpt4_library",
        "documentation": {}
    },
    {
        "label": "tvm.runtime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tvm.runtime",
        "description": "tvm.runtime",
        "detail": "tvm.runtime",
        "documentation": {}
    },
    {
        "label": "mlc_llm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mlc_llm",
        "description": "mlc_llm",
        "detail": "mlc_llm",
        "documentation": {}
    },
    {
        "label": "nano_llm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nano_llm",
        "description": "nano_llm",
        "detail": "nano_llm",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "tensorflow_datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_datasets",
        "description": "tensorflow_datasets",
        "detail": "tensorflow_datasets",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "optimum.version",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "optimum.version",
        "description": "optimum.version",
        "detail": "optimum.version",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "ORTModelForCausalLM",
        "importPath": "optimum.onnxruntime",
        "description": "optimum.onnxruntime",
        "isExtraImport": true,
        "detail": "optimum.onnxruntime",
        "documentation": {}
    },
    {
        "label": "ORTQuantizer",
        "importPath": "optimum.onnxruntime",
        "description": "optimum.onnxruntime",
        "isExtraImport": true,
        "detail": "optimum.onnxruntime",
        "documentation": {}
    },
    {
        "label": "AutoQuantizationConfig",
        "importPath": "optimum.onnxruntime.configuration",
        "description": "optimum.onnxruntime.configuration",
        "isExtraImport": true,
        "detail": "optimum.onnxruntime.configuration",
        "documentation": {}
    },
    {
        "label": "AutoCalibrationConfig",
        "importPath": "optimum.onnxruntime.configuration",
        "description": "optimum.onnxruntime.configuration",
        "isExtraImport": true,
        "detail": "optimum.onnxruntime.configuration",
        "documentation": {}
    },
    {
        "label": "TasksManager",
        "importPath": "optimum.exporters.tasks",
        "description": "optimum.exporters.tasks",
        "isExtraImport": true,
        "detail": "optimum.exporters.tasks",
        "documentation": {}
    },
    {
        "label": "tensorrt_llm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorrt_llm",
        "description": "tensorrt_llm",
        "detail": "tensorrt_llm",
        "documentation": {}
    },
    {
        "label": "xformers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xformers",
        "description": "xformers",
        "detail": "xformers",
        "documentation": {}
    },
    {
        "label": "xformers.info",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xformers.info",
        "description": "xformers.info",
        "detail": "xformers.info",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "open_dict",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "pytorch_lightning.trainer.trainer",
        "description": "pytorch_lightning.trainer.trainer",
        "isExtraImport": true,
        "detail": "pytorch_lightning.trainer.trainer",
        "documentation": {}
    },
    {
        "label": "MegatronGPTModel",
        "importPath": "nemo.collections.nlp.models.language_modeling.megatron_gpt_model",
        "description": "nemo.collections.nlp.models.language_modeling.megatron_gpt_model",
        "isExtraImport": true,
        "detail": "nemo.collections.nlp.models.language_modeling.megatron_gpt_model",
        "documentation": {}
    },
    {
        "label": "LengthParam",
        "importPath": "nemo.collections.nlp.modules.common.transformer.text_generation",
        "description": "nemo.collections.nlp.modules.common.transformer.text_generation",
        "isExtraImport": true,
        "detail": "nemo.collections.nlp.modules.common.transformer.text_generation",
        "documentation": {}
    },
    {
        "label": "SamplingParam",
        "importPath": "nemo.collections.nlp.modules.common.transformer.text_generation",
        "description": "nemo.collections.nlp.modules.common.transformer.text_generation",
        "isExtraImport": true,
        "detail": "nemo.collections.nlp.modules.common.transformer.text_generation",
        "documentation": {}
    },
    {
        "label": "NLPDDPStrategy",
        "importPath": "nemo.collections.nlp.parts.nlp_overrides",
        "description": "nemo.collections.nlp.parts.nlp_overrides",
        "isExtraImport": true,
        "detail": "nemo.collections.nlp.parts.nlp_overrides",
        "documentation": {}
    },
    {
        "label": "nemo",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nemo",
        "description": "nemo",
        "detail": "nemo",
        "documentation": {}
    },
    {
        "label": "QAModel",
        "importPath": "nemo.collections.nlp.models.question_answering.qa_model",
        "description": "nemo.collections.nlp.models.question_answering.qa_model",
        "isExtraImport": true,
        "detail": "nemo.collections.nlp.models.question_answering.qa_model",
        "documentation": {}
    },
    {
        "label": "numba",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numba",
        "description": "numba",
        "detail": "numba",
        "documentation": {}
    },
    {
        "label": "vectorize",
        "importPath": "numba",
        "description": "numba",
        "isExtraImport": true,
        "detail": "numba",
        "documentation": {}
    },
    {
        "label": "guvectorize",
        "importPath": "numba",
        "description": "numba",
        "isExtraImport": true,
        "detail": "numba",
        "documentation": {}
    },
    {
        "label": "cuda",
        "importPath": "numba",
        "description": "numba",
        "isExtraImport": true,
        "detail": "numba",
        "documentation": {}
    },
    {
        "label": "onnx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnx",
        "description": "onnx",
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "triton",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton",
        "description": "triton",
        "detail": "triton",
        "documentation": {}
    },
    {
        "label": "triton.language",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton.language",
        "description": "triton.language",
        "detail": "triton.language",
        "documentation": {}
    },
    {
        "label": "torch2trt",
        "importPath": "torch2trt",
        "description": "torch2trt",
        "isExtraImport": true,
        "detail": "torch2trt",
        "documentation": {}
    },
    {
        "label": "alexnet",
        "importPath": "torchvision.models.alexnet",
        "description": "torchvision.models.alexnet",
        "isExtraImport": true,
        "detail": "torchvision.models.alexnet",
        "documentation": {}
    },
    {
        "label": "torch_tensorrt",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch_tensorrt",
        "description": "torch_tensorrt",
        "detail": "torch_tensorrt",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "torchvision.models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.models",
        "description": "torchvision.models",
        "detail": "torchvision.models",
        "documentation": {}
    },
    {
        "label": "torchvision.datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "ollama",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ollama",
        "description": "ollama",
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "llama_index.llms.ollama",
        "description": "llama_index.llms.ollama",
        "isExtraImport": true,
        "detail": "llama_index.llms.ollama",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "llama_index.llms.ollama",
        "description": "llama_index.llms.ollama",
        "isExtraImport": true,
        "detail": "llama_index.llms.ollama",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "llama_index.llms.ollama",
        "description": "llama_index.llms.ollama",
        "isExtraImport": true,
        "detail": "llama_index.llms.ollama",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "OllamaEmbedding",
        "importPath": "llama_index.embeddings.ollama",
        "description": "llama_index.embeddings.ollama",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings",
        "description": "llama_index.core.embeddings",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings",
        "description": "llama_index.core.embeddings",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings",
        "documentation": {}
    },
    {
        "label": "cudf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cudf",
        "description": "cudf",
        "detail": "cudf",
        "documentation": {}
    },
    {
        "label": "dask_cudf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dask_cudf",
        "description": "dask_cudf",
        "detail": "dask_cudf",
        "documentation": {}
    },
    {
        "label": "cudf.pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cudf.pandas",
        "description": "cudf.pandas",
        "detail": "cudf.pandas",
        "documentation": {}
    },
    {
        "label": "cuml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cuml",
        "description": "cuml",
        "detail": "cuml",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "cuml.cluster",
        "description": "cuml.cluster",
        "isExtraImport": true,
        "detail": "cuml.cluster",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "mimicgen",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mimicgen",
        "description": "mimicgen",
        "detail": "mimicgen",
        "documentation": {}
    },
    {
        "label": "robosuite",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "robosuite",
        "description": "robosuite",
        "detail": "robosuite",
        "documentation": {}
    },
    {
        "label": "ciso8601",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ciso8601",
        "description": "ciso8601",
        "detail": "ciso8601",
        "documentation": {}
    },
    {
        "label": "homeassistant",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "homeassistant",
        "description": "homeassistant",
        "detail": "homeassistant",
        "documentation": {}
    },
    {
        "label": "psutil_home_assistant",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil_home_assistant",
        "description": "psutil_home_assistant",
        "detail": "psutil_home_assistant",
        "documentation": {}
    },
    {
        "label": "tritonclient",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tritonclient",
        "description": "tritonclient",
        "detail": "tritonclient",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faiss",
        "description": "faiss",
        "detail": "faiss",
        "documentation": {}
    },
    {
        "label": "cudaGetDeviceProperties",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaDeviceSynchronize",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaGetLastError",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaMallocManaged",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaHostAlloc",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaHostAllocMapped",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaHostGetDevicePointer",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaMemAttachGlobal",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaGetLastError",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaGetErrorString",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaError_t",
        "importPath": "cuda.cudart",
        "description": "cuda.cudart",
        "isExtraImport": true,
        "detail": "cuda.cudart",
        "documentation": {}
    },
    {
        "label": "cudaKNN",
        "importPath": "faiss_lite",
        "description": "faiss_lite",
        "isExtraImport": true,
        "detail": "faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaL2Norm",
        "importPath": "faiss_lite",
        "description": "faiss_lite",
        "isExtraImport": true,
        "detail": "faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaAllocMapped",
        "importPath": "faiss_lite",
        "description": "faiss_lite",
        "isExtraImport": true,
        "detail": "faiss_lite",
        "documentation": {}
    },
    {
        "label": "DistanceMetrics",
        "importPath": "faiss_lite",
        "description": "faiss_lite",
        "isExtraImport": true,
        "detail": "faiss_lite",
        "documentation": {}
    },
    {
        "label": "assert_cuda",
        "importPath": "faiss_lite",
        "description": "faiss_lite",
        "isExtraImport": true,
        "detail": "faiss_lite",
        "documentation": {}
    },
    {
        "label": "clip_trt",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "clip_trt",
        "description": "clip_trt",
        "detail": "clip_trt",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "Rectangle",
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "isExtraImport": true,
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "parse_unknown_args",
        "importPath": "efficientvit.apps.utils",
        "description": "efficientvit.apps.utils",
        "isExtraImport": true,
        "detail": "efficientvit.apps.utils",
        "documentation": {}
    },
    {
        "label": "EfficientViTSamAutomaticMaskGenerator",
        "importPath": "efficientvit.models.efficientvit.sam",
        "description": "efficientvit.models.efficientvit.sam",
        "isExtraImport": true,
        "detail": "efficientvit.models.efficientvit.sam",
        "documentation": {}
    },
    {
        "label": "EfficientViTSamPredictor",
        "importPath": "efficientvit.models.efficientvit.sam",
        "description": "efficientvit.models.efficientvit.sam",
        "isExtraImport": true,
        "detail": "efficientvit.models.efficientvit.sam",
        "documentation": {}
    },
    {
        "label": "build_kwargs_from_config",
        "importPath": "efficientvit.models.utils",
        "description": "efficientvit.models.utils",
        "isExtraImport": true,
        "detail": "efficientvit.models.utils",
        "documentation": {}
    },
    {
        "label": "create_sam_model",
        "importPath": "efficientvit.sam_model_zoo",
        "description": "efficientvit.sam_model_zoo",
        "isExtraImport": true,
        "detail": "efficientvit.sam_model_zoo",
        "documentation": {}
    },
    {
        "label": "Predictor",
        "importPath": "nanosam.utils.predictor",
        "description": "nanosam.utils.predictor",
        "isExtraImport": true,
        "detail": "nanosam.utils.predictor",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "sam_model_registry",
        "importPath": "segment_anything",
        "description": "segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything",
        "documentation": {}
    },
    {
        "label": "SamAutomaticMaskGenerator",
        "importPath": "segment_anything",
        "description": "segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything",
        "documentation": {}
    },
    {
        "label": "SamPredictor",
        "importPath": "segment_anything",
        "description": "segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything",
        "documentation": {}
    },
    {
        "label": "Phi3Config",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "peekOfCode": "class Phi3Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Phi3Model`]. It is used to instantiate a Phi-3\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the\n    [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct).\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32064):",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "peekOfCode": "logger = logging.get_logger(__name__)\nPHI3_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"microsoft/Phi-3-mini-4k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/config.json\",\n    \"microsoft/Phi-3-mini-128k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/config.json\",\n}\nclass Phi3Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Phi3Model`]. It is used to instantiate a Phi-3\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_PRETRAINED_CONFIG_ARCHIVE_MAP",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "peekOfCode": "PHI3_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"microsoft/Phi-3-mini-4k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/config.json\",\n    \"microsoft/Phi-3-mini-128k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/config.json\",\n}\nclass Phi3Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Phi3Model`]. It is used to instantiate a Phi-3\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the\n    [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct).",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3RMSNorm",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Phi3RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3RotaryEmbedding",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3RotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        self.register_buffer(\"inv_freq\", None, persistent=False)\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3LongRoPEScaledRotaryEmbedding",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3LongRoPEScaledRotaryEmbedding(Phi3RotaryEmbedding):\n    def __init__(self, dim, config, device=None):\n        super().__init__(dim, config.max_position_embeddings, config.rope_theta, device)\n        self.short_factor = config.rope_scaling[\"short_factor\"]\n        self.long_factor = config.rope_scaling[\"long_factor\"]\n        self.original_max_position_embeddings = config.original_max_position_embeddings\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.original_max_position_embeddings:",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3MLP",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n        self.activation_fn = ACT2FN[config.hidden_act]\n    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n        up_states = self.gate_up_proj(hidden_states)\n        gate, up_states = up_states.chunk(2, dim=-1)",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3Attention",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3Attention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3FlashAttention2",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3FlashAttention2(Phi3Attention):\n    \"\"\"\n    Phi-3 flash attention module. This module inherits from `Phi3Attention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3SdpaAttention",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3SdpaAttention(Phi3Attention):\n    \"\"\"\n    Phi3 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `Phi3Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n    # Adapted from Phi3Attention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3DecoderLayer",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3DecoderLayer(nn.Module):\n    def __init__(self, config: Phi3Config, layer_idx: int):\n        super().__init__()\n        self.config = config\n        self.self_attn = PHI3_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n        self.mlp = Phi3MLP(config)\n        self.input_layernorm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n        self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n        self.post_attention_layernorm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3PreTrainedModel",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3PreTrainedModel(PreTrainedModel):\n    config_class = Phi3Config\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"Phi3DecoderLayer\"]\n    _skip_keys_device_placement = \"past_key_values\"\n    _supports_flash_attn_2 = True\n    _supports_sdpa = False\n    _supports_cache_class = True\n    _version = \"0.0.5\"",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3Model",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3Model(Phi3PreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Phi3DecoderLayer`]\n    Args:\n        config: Phi3Config\n    \"\"\"\n    def __init__(self, config: Phi3Config):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3ForCausalLM",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3ForCausalLM(Phi3PreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi3\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = Phi3Model(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3ForSequenceClassification",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3ForSequenceClassification(Phi3PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = Phi3Model(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n    def get_input_embeddings(self):\n        return self.model.embed_tokens",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3ForTokenClassification",
        "kind": 6,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3ForTokenClassification(Phi3PreTrainedModel):\n    def __init__(self, config: Phi3Config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = Phi3Model(config)\n        if hasattr(config, \"classifier_dropout\") and config.classifier_dropout is not None:\n            classifier_dropout = config.classifier_dropout\n        elif hasattr(config, \"hidden_dropout\") and config.hidden_dropout is not None:\n            classifier_dropout = config.hidden_dropout\n        else:",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "rotate_half",
        "kind": 2,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n    Args:\n        q (`torch.Tensor`): The query tensor.",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "apply_rotary_pos_emb",
        "kind": 2,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "logger = logging.get_logger(__name__)\n# Transformers scans dependencies in the modeling file, causing issues on conditional loading. The regex only ignores try/catch blocks, but not if statements\n# if is_flash_attn_2_available():\n_flash_supports_window_size = False\ntry:\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\nexcept ImportError as error:\n    logger.warning(",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "_flash_supports_window_size",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "_flash_supports_window_size = False\ntry:\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\nexcept ImportError as error:\n    logger.warning(\n        f\"`flash-attention` package not found, consider installing for better performance: {error}.\"\n    )\n    if not _flash_supports_window_size:",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "_CHECKPOINT_FOR_DOC",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "_CHECKPOINT_FOR_DOC = \"microsoft/Phi-3-mini-4k-instruct\"\n_CONFIG_FOR_DOC = \"Phi3Config\"\nPHI3_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    \"microsoft/Phi-3-mini-128k-instruct\",\n    # See all Phi-3 models at https://huggingface.co/models?filter=Phi-3\n]\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Phi3\nclass Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "_CONFIG_FOR_DOC",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "_CONFIG_FOR_DOC = \"Phi3Config\"\nPHI3_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    \"microsoft/Phi-3-mini-128k-instruct\",\n    # See all Phi-3 models at https://huggingface.co/models?filter=Phi-3\n]\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Phi3\nclass Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_PRETRAINED_MODEL_ARCHIVE_LIST",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    \"microsoft/Phi-3-mini-128k-instruct\",\n    # See all Phi-3 models at https://huggingface.co/models?filter=Phi-3\n]\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Phi3\nclass Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Phi3RMSNorm is equivalent to T5LayerNorm",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_ATTENTION_CLASSES",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_ATTENTION_CLASSES = {\n    \"eager\": Phi3Attention,\n    \"flash_attention_2\": Phi3FlashAttention2,\n    \"sdpa\": Phi3SdpaAttention,\n}\nclass Phi3DecoderLayer(nn.Module):\n    def __init__(self, config: Phi3Config, layer_idx: int):\n        super().__init__()\n        self.config = config\n        self.self_attn = PHI3_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_START_DOCSTRING",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n    Parameters:\n        config ([`Phi3Config`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:",
        "detail": "data.models.huggingface.models--microsoft--Phi-3-mini-128k-instruct.snapshots.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3Config",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "peekOfCode": "class Phi3Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Phi3Model`]. It is used to instantiate a Phi-3\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the\n    [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct).\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32064):",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "peekOfCode": "logger = logging.get_logger(__name__)\nPHI3_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"microsoft/Phi-3-mini-4k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/config.json\",\n    \"microsoft/Phi-3-mini-128k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/config.json\",\n}\nclass Phi3Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Phi3Model`]. It is used to instantiate a Phi-3\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_PRETRAINED_CONFIG_ARCHIVE_MAP",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "peekOfCode": "PHI3_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"microsoft/Phi-3-mini-4k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/config.json\",\n    \"microsoft/Phi-3-mini-128k-instruct\": \"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/config.json\",\n}\nclass Phi3Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Phi3Model`]. It is used to instantiate a Phi-3\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the\n    [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct).",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.configuration_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3RMSNorm",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Phi3RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3RotaryEmbedding",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3RotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        self.register_buffer(\"inv_freq\", None, persistent=False)\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3LongRoPEScaledRotaryEmbedding",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3LongRoPEScaledRotaryEmbedding(Phi3RotaryEmbedding):\n    def __init__(self, dim, config, device=None):\n        super().__init__(dim, config.max_position_embeddings, config.rope_theta, device)\n        self.short_factor = config.rope_scaling[\"short_factor\"]\n        self.long_factor = config.rope_scaling[\"long_factor\"]\n        self.original_max_position_embeddings = config.original_max_position_embeddings\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.original_max_position_embeddings:",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3MLP",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n        self.activation_fn = ACT2FN[config.hidden_act]\n    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n        up_states = self.gate_up_proj(hidden_states)\n        gate, up_states = up_states.chunk(2, dim=-1)",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3Attention",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3Attention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3FlashAttention2",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3FlashAttention2(Phi3Attention):\n    \"\"\"\n    Phi-3 flash attention module. This module inherits from `Phi3Attention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3SdpaAttention",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3SdpaAttention(Phi3Attention):\n    \"\"\"\n    Phi3 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `Phi3Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n    # Adapted from Phi3Attention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3DecoderLayer",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3DecoderLayer(nn.Module):\n    def __init__(self, config: Phi3Config, layer_idx: int):\n        super().__init__()\n        self.config = config\n        self.self_attn = PHI3_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n        self.mlp = Phi3MLP(config)\n        self.input_layernorm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n        self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n        self.post_attention_layernorm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3PreTrainedModel",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3PreTrainedModel(PreTrainedModel):\n    config_class = Phi3Config\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"Phi3DecoderLayer\"]\n    _skip_keys_device_placement = \"past_key_values\"\n    _supports_flash_attn_2 = True\n    _supports_sdpa = False\n    _supports_cache_class = True\n    _version = \"0.0.5\"",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3Model",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3Model(Phi3PreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Phi3DecoderLayer`]\n    Args:\n        config: Phi3Config\n    \"\"\"\n    def __init__(self, config: Phi3Config):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3ForCausalLM",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3ForCausalLM(Phi3PreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi3\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = Phi3Model(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3ForSequenceClassification",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3ForSequenceClassification(Phi3PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = Phi3Model(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n    def get_input_embeddings(self):\n        return self.model.embed_tokens",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "Phi3ForTokenClassification",
        "kind": 6,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "class Phi3ForTokenClassification(Phi3PreTrainedModel):\n    def __init__(self, config: Phi3Config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = Phi3Model(config)\n        if hasattr(config, \"classifier_dropout\") and config.classifier_dropout is not None:\n            classifier_dropout = config.classifier_dropout\n        elif hasattr(config, \"hidden_dropout\") and config.hidden_dropout is not None:\n            classifier_dropout = config.hidden_dropout\n        else:",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "rotate_half",
        "kind": 2,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n    Args:\n        q (`torch.Tensor`): The query tensor.",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "apply_rotary_pos_emb",
        "kind": 2,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "logger = logging.get_logger(__name__)\n# Transformers scans dependencies in the modeling file, causing issues on conditional loading. The regex only ignores try/catch blocks, but not if statements\n# if is_flash_attn_2_available():\n_flash_supports_window_size = False\ntry:\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\nexcept ImportError as error:\n    logger.warning(",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "_flash_supports_window_size",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "_flash_supports_window_size = False\ntry:\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\nexcept ImportError as error:\n    logger.warning(\n        f\"`flash-attention` package not found, consider installing for better performance: {error}.\"\n    )\n    if not _flash_supports_window_size:",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "_CHECKPOINT_FOR_DOC",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "_CHECKPOINT_FOR_DOC = \"microsoft/Phi-3-mini-4k-instruct\"\n_CONFIG_FOR_DOC = \"Phi3Config\"\nPHI3_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    \"microsoft/Phi-3-mini-128k-instruct\",\n    # See all Phi-3 models at https://huggingface.co/models?filter=Phi-3\n]\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Phi3\nclass Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "_CONFIG_FOR_DOC",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "_CONFIG_FOR_DOC = \"Phi3Config\"\nPHI3_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    \"microsoft/Phi-3-mini-128k-instruct\",\n    # See all Phi-3 models at https://huggingface.co/models?filter=Phi-3\n]\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Phi3\nclass Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_PRETRAINED_MODEL_ARCHIVE_LIST",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    \"microsoft/Phi-3-mini-128k-instruct\",\n    # See all Phi-3 models at https://huggingface.co/models?filter=Phi-3\n]\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Phi3\nclass Phi3RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Phi3RMSNorm is equivalent to T5LayerNorm",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_ATTENTION_CLASSES",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_ATTENTION_CLASSES = {\n    \"eager\": Phi3Attention,\n    \"flash_attention_2\": Phi3FlashAttention2,\n    \"sdpa\": Phi3SdpaAttention,\n}\nclass Phi3DecoderLayer(nn.Module):\n    def __init__(self, config: Phi3Config, layer_idx: int):\n        super().__init__()\n        self.config = config\n        self.self_attn = PHI3_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_START_DOCSTRING",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n    Parameters:\n        config ([`Phi3Config`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "PHI3_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "description": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "peekOfCode": "PHI3_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:",
        "detail": "data.models.huggingface.modules.transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "jetson_containers.build",
        "description": "jetson_containers.build",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('packages', type=str, nargs='*', default=[], help='packages or containers to build (filterable by wildcards)')\nparser.add_argument('--name', type=str, default='', help=\"the name of the output container to build\")\nparser.add_argument('--base', type=str, default='', help=\"the base container to use at the beginning of the build chain (default: l4t-jetpack)\")\nparser.add_argument('--multiple', action='store_true', help=\"the specified packages should be built independently as opposed to chained together\")\nparser.add_argument('--build-flags', type=str, default='', help=\"extra flags to pass to 'docker build' commands\")\nparser.add_argument('--package-dirs', type=str, default='', help=\"additional package search directories (comma or colon-separated)\")\nparser.add_argument('--list-packages', action='store_true', help=\"show the list of packages that were found under the search directories\")\nparser.add_argument('--show-packages', action='store_true', help=\"show info about one or more packages (if none are specified, all will be listed\")\nparser.add_argument('--skip-packages', type=str, default='', help=\"disable certain packages/containers (filterable by wildcards, comma/colon-separated)\")",
        "detail": "jetson_containers.build",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "jetson_containers.build",
        "description": "jetson_containers.build",
        "peekOfCode": "args = parser.parse_args()\n# validate args\nif args.skip_errors and not args.multiple:\n    raise ValueError(\"--skip-errors can only be used with --multiple flag\")\nif args.verbose:\n    os.environ['VERBOSE'] = 'ON'\n# split multi-value keyword arguments\nargs.package_dirs = re.split(',|;|:', args.package_dirs)\nargs.skip_packages = re.split(',|;|:', args.skip_packages)\nargs.skip_tests = re.split(',|;|:', args.skip_tests)",
        "detail": "jetson_containers.build",
        "documentation": {}
    },
    {
        "label": "args.package_dirs",
        "kind": 5,
        "importPath": "jetson_containers.build",
        "description": "jetson_containers.build",
        "peekOfCode": "args.package_dirs = re.split(',|;|:', args.package_dirs)\nargs.skip_packages = re.split(',|;|:', args.skip_packages)\nargs.skip_tests = re.split(',|;|:', args.skip_tests)\nargs.test_only = re.split(',|;|:', args.test_only)\nprint(args)\nprint(f\"-- L4T_VERSION={L4T_VERSION}\")\nprint(f\"-- JETPACK_VERSION={JETPACK_VERSION}\")\nprint(f\"-- CUDA_VERSION={CUDA_VERSION}\")\nprint(f\"-- PYTHON_VERSION={PYTHON_VERSION}\")\nprint(f\"-- LSB_RELEASE={LSB_RELEASE} ({LSB_CODENAME})\")",
        "detail": "jetson_containers.build",
        "documentation": {}
    },
    {
        "label": "args.skip_packages",
        "kind": 5,
        "importPath": "jetson_containers.build",
        "description": "jetson_containers.build",
        "peekOfCode": "args.skip_packages = re.split(',|;|:', args.skip_packages)\nargs.skip_tests = re.split(',|;|:', args.skip_tests)\nargs.test_only = re.split(',|;|:', args.test_only)\nprint(args)\nprint(f\"-- L4T_VERSION={L4T_VERSION}\")\nprint(f\"-- JETPACK_VERSION={JETPACK_VERSION}\")\nprint(f\"-- CUDA_VERSION={CUDA_VERSION}\")\nprint(f\"-- PYTHON_VERSION={PYTHON_VERSION}\")\nprint(f\"-- LSB_RELEASE={LSB_RELEASE} ({LSB_CODENAME})\")\n# add package directories",
        "detail": "jetson_containers.build",
        "documentation": {}
    },
    {
        "label": "args.skip_tests",
        "kind": 5,
        "importPath": "jetson_containers.build",
        "description": "jetson_containers.build",
        "peekOfCode": "args.skip_tests = re.split(',|;|:', args.skip_tests)\nargs.test_only = re.split(',|;|:', args.test_only)\nprint(args)\nprint(f\"-- L4T_VERSION={L4T_VERSION}\")\nprint(f\"-- JETPACK_VERSION={JETPACK_VERSION}\")\nprint(f\"-- CUDA_VERSION={CUDA_VERSION}\")\nprint(f\"-- PYTHON_VERSION={PYTHON_VERSION}\")\nprint(f\"-- LSB_RELEASE={LSB_RELEASE} ({LSB_CODENAME})\")\n# add package directories\nif args.package_dirs:",
        "detail": "jetson_containers.build",
        "documentation": {}
    },
    {
        "label": "args.test_only",
        "kind": 5,
        "importPath": "jetson_containers.build",
        "description": "jetson_containers.build",
        "peekOfCode": "args.test_only = re.split(',|;|:', args.test_only)\nprint(args)\nprint(f\"-- L4T_VERSION={L4T_VERSION}\")\nprint(f\"-- JETPACK_VERSION={JETPACK_VERSION}\")\nprint(f\"-- CUDA_VERSION={CUDA_VERSION}\")\nprint(f\"-- PYTHON_VERSION={PYTHON_VERSION}\")\nprint(f\"-- LSB_RELEASE={LSB_RELEASE} ({LSB_CODENAME})\")\n# add package directories\nif args.package_dirs:\n    package_search_dirs(args.package_dirs)",
        "detail": "jetson_containers.build",
        "documentation": {}
    },
    {
        "label": "find_package_workflows",
        "kind": 2,
        "importPath": "jetson_containers.ci",
        "description": "jetson_containers.ci",
        "peekOfCode": "def find_package_workflows(package, root):\n    \"\"\"\n    Find all the GitHub Workflows for building a specific package.\n    \"\"\"\n    workflow_root = os.path.join(root, '.github/workflows')\n    workflows = []\n    entries = os.listdir(workflow_root)\n    for entry in entries:\n        entry_path = os.path.join(workflow_root, entry)\n        if not os.path.isfile(entry_path):",
        "detail": "jetson_containers.ci",
        "documentation": {}
    },
    {
        "label": "generate_workflow",
        "kind": 2,
        "importPath": "jetson_containers.ci",
        "description": "jetson_containers.ci",
        "peekOfCode": "def generate_workflow(package, root, simulate=False):\n    \"\"\"\n    Generate the YAML workflow definition for automated container builds for that package\n    \"\"\"\n    if not root:\n        root = os.path.dirname(os.path.dirname(__file__))\n    name = package['name']\n    workflow_name = f\"{name}_jp{JETPACK_VERSION.major}{JETPACK_VERSION.minor}\".replace(':','-').replace('.','')\n    filename = os.path.join(root, '.github/workflows', f\"{workflow_name}.yml\")\n    on_paths = [",
        "detail": "jetson_containers.ci",
        "documentation": {}
    },
    {
        "label": "generate_workflow_build_all",
        "kind": 2,
        "importPath": "jetson_containers.ci",
        "description": "jetson_containers.ci",
        "peekOfCode": "def generate_workflow_build_all(packages, root, simulate=False):\n    \"\"\"\n    Generate the BUILD ALL workflow which builds all containers for that L4T version.\n    \"\"\"\n    if not root:\n        root = os.path.dirname(os.path.dirname(__file__))\n    workflow_name = f\"build-all_r{L4T_VERSION}\"\n    filename = os.path.join(root, '.github/workflows', f\"{workflow_name}.yml\")\n    txt = f\"name: \\\"{workflow_name}\\\"\\n\"\n    txt += f\"run-name: \\\"Build All (JetPack {JETPACK_VERSION})\\\"\\n\"",
        "detail": "jetson_containers.ci",
        "documentation": {}
    },
    {
        "label": "generate_workflow_badge",
        "kind": 2,
        "importPath": "jetson_containers.ci",
        "description": "jetson_containers.ci",
        "peekOfCode": "def generate_workflow_badge(workflow, repo):\n    \"\"\"\n    Generate the markdown for a workflow status badge.\n    \"\"\"\n    def remove_prefix(str, prefix):\n        return str[len(prefix):] if str.startswith(prefix) else str\n    def remove_domain(url):\n        url = url.split('/')\n        return url[-2] + '/' + url[-1]\n    def restore_tag(name):",
        "detail": "jetson_containers.ci",
        "documentation": {}
    },
    {
        "label": "register_runner",
        "kind": 2,
        "importPath": "jetson_containers.ci",
        "description": "jetson_containers.ci",
        "peekOfCode": "def register_runner(token, root, repo, labels=[], simulate=False):\n    \"\"\"\n    Setup and register this machine as a self-hosted runner with GitHub\n    \"\"\"\n    if not args.token:\n        raise ValueError(f\"--token must be provided from GitHub when registering self-hosted runners\")\n    if not root:\n        root = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'runner')\n    labels.extend([\n        'jetson',",
        "detail": "jetson_containers.ci",
        "documentation": {}
    },
    {
        "label": "build_container",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def build_container(name, packages, base=get_l4t_base(), build_flags='', simulate=False, skip_tests=[], test_only=[], push='', no_github_api=False):\n    \"\"\"\n    Multi-stage container build that chains together selected packages into one container image.\n    For example, `['pytorch', 'tensorflow']` would build a container that had both pytorch and tensorflow in it.\n    Parameters:\n      name (str) -- name of container image to build (or a namespace to build under, ending in /)\n                    if empty, a default name will be assigned based on the package(s) selected.           \n      packages (list[str]) -- list of package names to build (into one container)\n      base (str) -- base container image to use (defaults to l4t-base or l4t-jetpack)\n      build_flags (str) -- arguments to add to the 'docker build' command",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "build_containers",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def build_containers(name, packages, base=get_l4t_base(), build_flags='', simulate=False, skip_errors=False, skip_packages=[], skip_tests=[], test_only=[], push=''):\n    \"\"\"\n    Build separate container images for each of the requested packages (this is typically used in batch building jobs)\n    For example, `['pytorch', 'tensorflow']` would build a pytorch container and a tensorflow container.\n    TODO add multiprocessing parallel build support for jobs=-1 (use all CPU cores)\n    Parameters:\n      name (str) -- name of container to build (or a namespace to build under, ending in /)\n                    if empty, a default name will be assigned based on the package(s) selected.\n                    wildcards can be used to select packages (i.e. 'ros*' would build all ROS packages)             \n      packages (list[str]) -- list of package names to build (in separated containers)",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "tag_container",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def tag_container(source, target, simulate=False):\n    \"\"\"\n    Tag a container image (source -> target)\n    \"\"\"\n    cmd = f\"{sudo_prefix()}docker tag {source} {target}\"\n    print(f\"-- Tagging container {source} -> {target}\")\n    print(f\"{cmd}\\n\")\n    if not simulate:\n        subprocess.run(cmd, shell=True, check=True)\ndef push_container(name, repository='', simulate=False):",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "push_container",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def push_container(name, repository='', simulate=False):\n    \"\"\"\n    Push container to a repository or user with 'docker push'  \n    If repository is specified (for example, a DockerHub username) the container will be re-tagged\n    under that repository first. Otherwise, it's assumed the image is tagged under the correct name already.\n    It's also assumed that this machine has already been logged into the repository with 'docker login'\n    Returns the container name/tag that was pushed.\n    \"\"\"\n    cmd = \"\"\n    if repository:",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "test_container",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def test_container(name, package, simulate=False):\n    \"\"\"\n    Run tests on a container\n    \"\"\"\n    package = find_package(package)\n    if 'test' not in package:\n        return True\n    for test in package['test']:\n        test_cmd = test.split(' ')  # test could be a command with arguments\n        test_exe = test_cmd[0]      # just get just the script/executable name",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "get_local_containers",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def get_local_containers():\n    \"\"\"\n    Get the locally-available container images from the 'docker images' command\n    Returns a list of dicts with entries like the following:\n        {\"Containers\":\"N/A\",\"CreatedAt\":\"2023-07-23 15:24:28 -0400 EDT\",\"CreatedSince\":\"42 hours ago\",\n         \"Digest\":\"\\u003cnone\\u003e\",\"ID\":\"6acd9e526f50\",\"Repository\":\"runner/l4t-pytorch\",\n         \"SharedSize\":\"N/A\",\"Size\":\"11.4GB\",\"Tag\":\"r35.2.1\",\"UniqueSize\":\"N/A\",\"VirtualSize\":\"11.37GB\"}   \n    These containers are sorted by most recent created to the oldest.\n    \"\"\"\n    global _LOCAL_CACHE",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "get_registry_containers",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def get_registry_containers(user='dustynv', **kwargs):\n    \"\"\"\n    Fetch a DockerHub user's public container images/tags.\n    Returns a list of dicts with keys like 'namespace', 'name', and 'tags'.\n    To view the number of requests remaining within the rate-limit:\n      curl -i https://hub.docker.com/v2/namespaces/dustynv/repositories/l4t-pytorch/tags\n    All the caching is to prevent going over the DockerHub API rate limits.\n    \"\"\"\n    global _REGISTRY_CACHE\n    if len(_REGISTRY_CACHE) > 0:",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "find_local_containers",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def find_local_containers(package, return_dicts=False, **kwargs):\n    \"\"\"\n    Search for local containers on the machine containing this package.\n    Returns a list of strings, unless return_dicts=True in which case\n    a list of dicts is returned with the full metadata from the Docker engine.\n    \"\"\"\n    if isinstance(package, dict):\n        package = package['name']\n    namespace, repo, tag = split_container_name(package)\n    local_images = get_local_containers()",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "find_registry_containers",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def find_registry_containers(package, check_l4t_version=True, return_dicts=False, **kwargs):\n    \"\"\"\n    Search DockerHub for container images compatible with the package or container name.\n    The returned list of images will also be compatible with the version of L4T \n    running on the device, unless check_l4t_version is set to false\n    Normally a list of strings is returned, unless return_dicts=True in which case\n    a list of dicts is returned with the full metadata from DockerHub.\n    \"\"\"\n    if isinstance(package, dict):\n        package = package['name']",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "find_container",
        "kind": 2,
        "importPath": "jetson_containers.container",
        "description": "jetson_containers.container",
        "peekOfCode": "def find_container(package, prefer_sources=['local', 'registry', 'build'], disable_sources=[], quiet=True, **kwargs):\n    \"\"\"\n    Finds a local or remote container image to run for the given package (returns a string)\n    TODO search for other packages that depend on this package if an image isn't available.\n    TODO check if the dockerhub image has updated vs local copy, and if so ask user if they want to pull it.\n    \"\"\"\n    if isinstance(package, dict):\n        package = package['name']\n    namespace, repo, tag = split_container_name(package)\n    log_debug(f\"-- Finding compatible container image for namespace={namespace} repo={repo} tag={tag}\")",
        "detail": "jetson_containers.container",
        "documentation": {}
    },
    {
        "label": "generate_package_list",
        "kind": 2,
        "importPath": "jetson_containers.docs",
        "description": "jetson_containers.docs",
        "peekOfCode": "def generate_package_list(packages, root, repo, filename='packages/README.md', simulate=False):\n    \"\"\"\n    Generate a markdown table of all the packages\n    \"\"\"\n    filename = os.path.join(root, filename)\n    # group packages by category for navigability\n    groups = group_packages(packages, key='group', default='other')\n    txt = \"# Packages\\n\"\n    txt += \"> \"\n    # build list of groups",
        "detail": "jetson_containers.docs",
        "documentation": {}
    },
    {
        "label": "generate_package_docs",
        "kind": 2,
        "importPath": "jetson_containers.docs",
        "description": "jetson_containers.docs",
        "peekOfCode": "def generate_package_docs(packages, root, repo, simulate=False):\n    \"\"\"\n    Generate README.md files for the supplied packages.\n    Group them by path so there's just one page per directory.\n    \"\"\"\n    groups = group_packages(packages, 'path')\n    for pkg_path, pkgs in groups.items():\n        pkg_name = os.path.basename(pkg_path)\n        filename = os.path.join(pkg_path, 'README.md')\n        txt = f\"# {pkg_name}\\n\\n\"",
        "detail": "jetson_containers.docs",
        "documentation": {}
    },
    {
        "label": "generate_registry_docs",
        "kind": 2,
        "importPath": "jetson_containers.docs",
        "description": "jetson_containers.docs",
        "peekOfCode": "def generate_registry_docs(packages, root, repo, user, password, simulate=False):\n    \"\"\"\n    Apply descriptions to the container repos on DockerHub\n    \"\"\"\n    groups = group_packages(packages, 'path')\n    hub = dockerhub_api.DockerHub(username=user, password=password, return_lists=True)\n    request_cache = []\n    for name, package in packages.items():\n        namespace, repository, tag = split_container_name(name)\n        repo_path = package['path'][package['path'].find('/packages/')+1:]",
        "detail": "jetson_containers.docs",
        "documentation": {}
    },
    {
        "label": "get_l4t_version",
        "kind": 2,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "def get_l4t_version(version_file='/etc/nv_tegra_release'):\n    \"\"\"\n    Returns the L4T_VERSION in a packaging.version.Version object\n    Which can be compared against other version objects:  https://packaging.pypa.io/en/latest/version.html\n    You can also access the version components directly.  For example, on L4T R35.3.1:\n        version.major == 35\n        version.minor == 3\n        version.micro == 1\n    The L4T_VERSION will either be parsed from /etc/nv_tegra_release or the $L4T_VERSION environment variable.\n    \"\"\"",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "get_jetpack_version",
        "kind": 2,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "def get_jetpack_version(l4t_version=get_l4t_version(), default='5.1'):\n    \"\"\"\n    Returns the version of JetPack (based on the L4T version)\n    https://github.com/rbonghi/jetson_stats/blob/master/jtop/core/jetson_variables.py\n    \"\"\"\n    if not isinstance(l4t_version, Version):\n        l4t_version = Version(l4t_version)\n    NVIDIA_JETPACK = {\n        # -------- JP6 --------\n        \"36.3.0\": \"6.0 GA\",",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "get_cuda_version",
        "kind": 2,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "def get_cuda_version(version_file='/usr/local/cuda/version.json'):\n    \"\"\"\n    Returns the installed version of the CUDA Toolkit in a packaging.version.Version object\n    The CUDA_VERSION will either be parsed from /usr/local/cuda/version.json or the $CUDA_VERSION environment variable.\n    \"\"\"\n    def to_version(version):\n        version = Version(version)\n        return Version(f\"{version.major}.{version.minor}\")\n    if 'CUDA_VERSION' in os.environ and len(os.environ['CUDA_VERSION']) > 0:\n        return to_version(os.environ['CUDA_VERSION'])",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "get_l4t_base",
        "kind": 2,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "def get_l4t_base(l4t_version=get_l4t_version()):\n    \"\"\"\n    Returns the l4t-base or l4t-jetpack container to use\n    \"\"\"\n    if l4t_version.major >= 36:   # JetPack 6\n        return \"ubuntu:22.04\" #\"nvcr.io/ea-linux4tegra/l4t-jetpack:r36.0.0\"\n    elif l4t_version.major >= 34: # JetPack 5\n        if l4t_version >= Version('35.4.1'):\n            return \"nvcr.io/nvidia/l4t-jetpack:r35.4.1\"\n        else:",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "l4t_version_from_tag",
        "kind": 2,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "def l4t_version_from_tag(tag):\n    \"\"\"\n    Extract the L4T_VERSION from a container tag by searching it for patterns like 'r35.2.1' / ect.\n    Returns a packaging.version.Version object, or None if a valid L4T_VERSION couldn't be found in the tag.\n    \"\"\"\n    tag = tag.split(':')[-1]\n    tag = re.split('-|_', tag)\n    for t in tag:\n        if t[0] != 'r' and t[0] != 'R':\n            continue",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "l4t_version_compatible",
        "kind": 2,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "def l4t_version_compatible(l4t_version, l4t_version_host=get_l4t_version(), **kwargs):\n    \"\"\"\n    Returns true if the host OS can run containers built for the provided L4T version.\n    \"\"\"\n    if not l4t_version:\n        return False\n    if not isinstance(l4t_version, Version):\n        l4t_version = Version(l4t_version)\n    if l4t_version_host.major == 36: # JetPack 6 runs containers for JetPack 6\n        if l4t_version.major == 36:",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "get_lsb_release",
        "kind": 2,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "def get_lsb_release():\n    \"\"\"\n    Returns a tuple of (LSB_RELEASE, LSB_CODENAME)\n       (\"18.04\", \"bionic\")\n       (\"20.04\", \"focal\")\n    \"\"\"\n    return (subprocess.run([\"lsb_release\", \"-rs\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, check=True).stdout.strip(),\n           subprocess.run([\"lsb_release\", \"-cs\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, check=True).stdout.strip())\n# set L4T_VERSION and CUDA_VERSION globals        \nL4T_VERSION = get_l4t_version()",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "L4T_VERSION",
        "kind": 5,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "L4T_VERSION = get_l4t_version()\nJETPACK_VERSION = get_jetpack_version()\nCUDA_VERSION = get_cuda_version()\n# Nano/TX1 = 5.3, TX2 = 6.2, Xavier = 7.2, Orin = 8.7\nif L4T_VERSION.major >= 36:    # JetPack 6\n    CUDA_ARCHITECTURES = [87]\nelif L4T_VERSION.major >= 34:  # JetPack 5\n    CUDA_ARCHITECTURES = [72, 87]\nelif L4T_VERSION.major == 32:  # JetPack 4\n    CUDA_ARCHITECTURES = [53, 62, 72]",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "JETPACK_VERSION",
        "kind": 5,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "JETPACK_VERSION = get_jetpack_version()\nCUDA_VERSION = get_cuda_version()\n# Nano/TX1 = 5.3, TX2 = 6.2, Xavier = 7.2, Orin = 8.7\nif L4T_VERSION.major >= 36:    # JetPack 6\n    CUDA_ARCHITECTURES = [87]\nelif L4T_VERSION.major >= 34:  # JetPack 5\n    CUDA_ARCHITECTURES = [72, 87]\nelif L4T_VERSION.major == 32:  # JetPack 4\n    CUDA_ARCHITECTURES = [53, 62, 72]\n# x86_64, aarch64",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "CUDA_VERSION",
        "kind": 5,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "CUDA_VERSION = get_cuda_version()\n# Nano/TX1 = 5.3, TX2 = 6.2, Xavier = 7.2, Orin = 8.7\nif L4T_VERSION.major >= 36:    # JetPack 6\n    CUDA_ARCHITECTURES = [87]\nelif L4T_VERSION.major >= 34:  # JetPack 5\n    CUDA_ARCHITECTURES = [72, 87]\nelif L4T_VERSION.major == 32:  # JetPack 4\n    CUDA_ARCHITECTURES = [53, 62, 72]\n# x86_64, aarch64\nSYSTEM_ARCH = platform.machine()",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "SYSTEM_ARCH",
        "kind": 5,
        "importPath": "jetson_containers.l4t_version",
        "description": "jetson_containers.l4t_version",
        "peekOfCode": "SYSTEM_ARCH = platform.machine()\n# Python version (3.6, 3.8, 3.10, ect)\nif 'PYTHON_VERSION' in os.environ and len(os.environ['PYTHON_VERSION']) > 0:\n    PYTHON_VERSION = Version(os.environ['PYTHON_VERSION'])\nelse:\n    PYTHON_VERSION = Version(f'{sys.version_info.major}.{sys.version_info.minor}')\n# LSB release and codename (\"20.04\", \"focal\")\nLSB_RELEASE, LSB_CODENAME = get_lsb_release()",
        "detail": "jetson_containers.l4t_version",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 2,
        "importPath": "jetson_containers.logging",
        "description": "jetson_containers.logging",
        "peekOfCode": "def log_dir(type='root'):\n    \"\"\"\n    Return the path to the logging directory.\n    type can be:  root, build, test, run\n    \"\"\"\n    return _LOG_DIRS[type]\ndef set_log_dir(path, type='root', create=True):\n    \"\"\"\n    Set the path to the logging directory, and create it if needed.\n    type can be:  root, build, test, run",
        "detail": "jetson_containers.logging",
        "documentation": {}
    },
    {
        "label": "set_log_dir",
        "kind": 2,
        "importPath": "jetson_containers.logging",
        "description": "jetson_containers.logging",
        "peekOfCode": "def set_log_dir(path, type='root', create=True):\n    \"\"\"\n    Set the path to the logging directory, and create it if needed.\n    type can be:  root, build, test, run\n    \"\"\"\n    _LOG_DIRS[type] = path\n    if create:\n        os.makedirs(path, exist_ok=True)\n    if type == 'root':\n        set_log_dir(os.path.join(path, 'build'), 'build', create)",
        "detail": "jetson_containers.logging",
        "documentation": {}
    },
    {
        "label": "_LOG_DIRS",
        "kind": 5,
        "importPath": "jetson_containers.logging",
        "description": "jetson_containers.logging",
        "peekOfCode": "_LOG_DIRS = {}\ndef log_dir(type='root'):\n    \"\"\"\n    Return the path to the logging directory.\n    type can be:  root, build, test, run\n    \"\"\"\n    return _LOG_DIRS[type]\ndef set_log_dir(path, type='root', create=True):\n    \"\"\"\n    Set the path to the logging directory, and create it if needed.",
        "detail": "jetson_containers.logging",
        "documentation": {}
    },
    {
        "label": "package_search_dirs",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def package_search_dirs(package_dirs, scan=False):\n    \"\"\"\n    Add a list of directories to search for packages under.\n    If scan is true, these directories will be scanned for packages.\n    \"\"\"\n    global _PACKAGE_DIRS\n    if isinstance(package_dirs, str) and len(package_dirs) > 0:\n        package_dirs = [package_dirs]\n    for package_dir in package_dirs:\n        if len(package_dir) > 0:",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "package_scan_options",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def package_scan_options(dict):\n    \"\"\"\n    Set global package scanning options\n      -- check_l4t_version:  if true (default), packages that don't meet the required L4T_VERSION of the host will be disabled\n    \"\"\"\n    global _PACKAGE_OPTS\n    _PACKAGE_OPTS.update(dict)\ndef scan_packages(package_dirs=_PACKAGE_DIRS, rescan=False):\n    \"\"\"\n    Find packages from the list of provided search paths.",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "scan_packages",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def scan_packages(package_dirs=_PACKAGE_DIRS, rescan=False):\n    \"\"\"\n    Find packages from the list of provided search paths.\n    If a path ends in * wildcard, it will be searched recursively.\n    This looks for Dockerfiles and config scripts in these directories.\n    Returns a dict of package info from this path and sub-paths.\n    \"\"\"\n    global _PACKAGES\n    global _PACKAGE_SCAN\n    # skip scanning if it's already done",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "find_package",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def find_package(package, required=True, scan=True):\n    \"\"\"\n    Find a package by name or alias, and return it's configuration dict.\n    This filters the names with pattern matching using shell-style wildcards.\n    If required is true, a KeyError exception will be raised if any of the packages can't be found.\n    \"\"\"\n    if validate_dict(package):\n        return package\n    if scan:\n        scan_packages()",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def find_packages(packages, required=True, scan=True, skip=[]):\n    \"\"\"\n    Find a set of packages by name or alias, and return them in a dict.\n    This filters the names with pattern matching using shell-style wildcards.\n    If required is true, a KeyError exception will be raised if any of the packages can't be found.\n    \"\"\"\n    if scan:\n        scan_packages()\n    if isinstance(packages, str):\n        if packages == '*' or packages == 'all' or len(packages) == 0:",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "skip_packages",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def skip_packages(packages, skip):\n    \"\"\"\n    Filter a dict of packages by a list of names to skip (can use wildcards)\n    \"\"\"\n    if isinstance(skip, str):\n        skip = [skip]\n    if len(skip) == 0:\n        return packages\n    filtered = {}\n    for key, value in packages.items():",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "group_packages",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def group_packages(packages, key, default=''):\n    \"\"\"\n    Group packages by one of their keys, for example 'group' will return a dict\n    of all the groups where each group contains the packages belonging to it.\n    Or you can group by path, package name, depends, ect. or any other key.\n    If a package doesn't have the key, it won't be added unless a default is specified.\n    \"\"\"\n    grouped = {}\n    for name, package in packages.items():\n        if key in package:",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "resolve_dependencies",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def resolve_dependencies(packages, check=True):\n    \"\"\"\n    Recursively expand the list of dependencies to include all sub-dependencies.\n    Returns a new list of containers to build which contains all the dependencies.\n    If check is true, then each dependency will be confirmed to exist, otherwise\n    a KeyError exception will be raised with the name of the missing dependency.\n    \"\"\"\n    if isinstance(packages, str):\n        packages = [packages]\n    def add_depends(packages):",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "update_dependencies",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def update_dependencies(old, new):\n    \"\"\"\n    Merge two lists of dependencies, with the new list overriding the old one::\n       update_dependencies(['pytorch', 'transformers'], ['pytorch:2.0']) -> ['pytorch:2.0', 'transformers']\n    The dependencies will be matched by comparing just their name and ignoring any tag.\n    \"\"\"\n    if not new:\n        return old\n    if isinstance(new, str):\n        new = [new]",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "dependant_packages",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def dependant_packages(package):\n    \"\"\"\n    Find the list of all packages that depend on this package.\n    \"\"\"\n    if isinstance(package, str):\n        package = find_package(package)\n    dependants = []\n    for key, pkg in _PACKAGES.items():\n        if package == pkg:\n            continue",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "apply_config",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def apply_config(package, config):\n    \"\"\"\n    Apply a config dict to an existing package configuration\n    \"\"\"\n    if config is None or not isinstance(config, dict):\n        return\n    if validate_dict(config):  # the package config entries are in the top-level dict\n        package.update(validate_lists(config))\n        if 'dockerfile' in config:\n            apply_config(package, parse_yaml_header(os.path.join(config['path'], config['dockerfile'])))  ",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "config_package",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def config_package(package):\n    \"\"\"\n    Run a package's config.py or JSON if it has one\n    \"\"\"\n    if isinstance(package, str):\n        package = find_package(package)\n    elif not isinstance(package, dict):\n        raise ValueError(\"package should either be a string or dict\")\n    if 'dockerfile' in package:\n        config = parse_yaml_header(os.path.join(package['path'], package['dockerfile']))",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def check_requirements(package):\n    \"\"\"\n    Check if the L4T/CUDA versions meet the requirements needed by the package\n    \"\"\"\n    for r in package['requires']:\n        if not isinstance(r, str):\n            r = str(r)\n        r = r.lower()\n        if 'cu' in r:\n            if Version(f\"{CUDA_VERSION.major}{CUDA_VERSION.minor}\") not in SpecifierSet(r.replace('cu','')):",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "validate_package",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def validate_package(package):\n    \"\"\"\n    Validate/check a package's configuration, returning a list (i.e. of subpackages)\n    \"\"\"\n    packages = []\n    if isinstance(package, tuple):\n        package = list(package)\n    if isinstance(package, dict):\n        for key, value in package.items():  # check for sub-packages\n            if validate_dict(value):",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "validate_config",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def validate_config(path):\n    \"\"\"\n    Return a well-formed package configuration JSON or YAML file, or None on error.\n    \"\"\"\n    ext = os.path.splitext(path)[1]\n    if ext != '.json' and ext != '.yml' and ext != '.yaml':\n        return None\n    try:\n        with open(path, 'r') as file:\n            if ext == '.json':",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "validate_dict",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def validate_dict(package):\n    \"\"\"\n    Return true if this is a package configuration dict.\n    \"\"\"\n    if not isinstance(package, dict):\n        return False\n    for key, value in package.items():\n        if key not in _PACKAGE_KEYS:\n            #print(f\"-- Unknown key '{key}' in package config:\", value)\n            return False",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "validate_lists",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def validate_lists(package):\n    \"\"\"\n    Make sure that certain config entries are lists.\n    \"\"\"\n    def str2list(pkg, key):\n        if key in pkg and isinstance(pkg[key], str):\n            pkg[key] = [pkg[key]]\n    str2list(package, 'alias')\n    str2list(package, 'config')\n    str2list(package, 'depends')",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "parse_yaml_header",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def parse_yaml_header(dockerfile):\n    \"\"\"\n    Parse YAML configuration from the Dockerfile header\n    \"\"\"\n    try:\n        txt = \"\"\n        with open(dockerfile, 'r') as file:\n            in_yaml = False\n            while True:\n                line = file.readline()",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "package_alias",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def package_alias(alias):\n    \"\"\"\n    Add an alternate name (or list of names) to the current package.\n    It will then be able to be found using these other aliases.\n    \"\"\"\n    if isinstance(alias, str):\n        alias = [alias]\n    elif not isinstance(alias, list):\n        raise ValueError(\"alias should be a string or list of strings\")\n    package_config({'alias': alias})",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "package_build_args",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def package_build_args(env):\n    \"\"\"\n    Add docker --build-arg options to the current package\n    env should be a dict where the keys are the ARG name\n    \"\"\"\n    build_args = \"\"\n    for key, value in env.items():\n        build_args += f\"--build-arg {key}={value} \"\n    package_config({'build_args': build_args})\ndef package_config(config):",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "package_config",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def package_config(config):\n    \"\"\"\n    Apply a dict to the current package's configuration\n    \"\"\"\n    global _CURRENT_PACKAGE\n    print('package_config dir()', dir())\n    print('CURRENT_PACKAGE', _CURRENT_PACKAGE)\n    if _CURRENT_PACKAGE is None:\n        raise ValueError(\"a package isn't currently being configured\")\n    _CURRENT_PACKAGE.update(config)",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "package_depends",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def package_depends(package):\n    \"\"\"\n    Apply a build dependency to the current package.\n    \"\"\"\n    if isinstance(package, str):\n        package = [package]\n    elif not isinstance(package, list):\n        raise ValueError(\"package should be a string or list of strings\")\n    package_config({'depends': package})\ndef package_name(name):",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "package_name",
        "kind": 2,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "def package_name(name):\n    \"\"\"\n    Set the current package's name.\n    \"\"\"\n    package_config({'name': name})\n'''",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "_PACKAGES",
        "kind": 5,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "_PACKAGES = {}\n_PACKAGE_SCAN = False\n_PACKAGE_ROOT = os.path.dirname(os.path.dirname(__file__))\n_PACKAGE_DIRS = [os.path.join(_PACKAGE_ROOT, 'packages/*')]\n_PACKAGE_OPTS = {'check_l4t_version': True}\n_PACKAGE_KEYS = ['alias', 'build_args', 'build_flags', 'config', 'depends', 'disabled',  \n                 'dockerfile', 'docs', 'group', 'name', 'notes', 'path', \n                 'prefix', 'postfix', 'requires', 'test']\ndef package_search_dirs(package_dirs, scan=False):\n    \"\"\"",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "_PACKAGE_SCAN",
        "kind": 5,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "_PACKAGE_SCAN = False\n_PACKAGE_ROOT = os.path.dirname(os.path.dirname(__file__))\n_PACKAGE_DIRS = [os.path.join(_PACKAGE_ROOT, 'packages/*')]\n_PACKAGE_OPTS = {'check_l4t_version': True}\n_PACKAGE_KEYS = ['alias', 'build_args', 'build_flags', 'config', 'depends', 'disabled',  \n                 'dockerfile', 'docs', 'group', 'name', 'notes', 'path', \n                 'prefix', 'postfix', 'requires', 'test']\ndef package_search_dirs(package_dirs, scan=False):\n    \"\"\"\n    Add a list of directories to search for packages under.",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "_PACKAGE_ROOT",
        "kind": 5,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "_PACKAGE_ROOT = os.path.dirname(os.path.dirname(__file__))\n_PACKAGE_DIRS = [os.path.join(_PACKAGE_ROOT, 'packages/*')]\n_PACKAGE_OPTS = {'check_l4t_version': True}\n_PACKAGE_KEYS = ['alias', 'build_args', 'build_flags', 'config', 'depends', 'disabled',  \n                 'dockerfile', 'docs', 'group', 'name', 'notes', 'path', \n                 'prefix', 'postfix', 'requires', 'test']\ndef package_search_dirs(package_dirs, scan=False):\n    \"\"\"\n    Add a list of directories to search for packages under.\n    If scan is true, these directories will be scanned for packages.",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "_PACKAGE_DIRS",
        "kind": 5,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "_PACKAGE_DIRS = [os.path.join(_PACKAGE_ROOT, 'packages/*')]\n_PACKAGE_OPTS = {'check_l4t_version': True}\n_PACKAGE_KEYS = ['alias', 'build_args', 'build_flags', 'config', 'depends', 'disabled',  \n                 'dockerfile', 'docs', 'group', 'name', 'notes', 'path', \n                 'prefix', 'postfix', 'requires', 'test']\ndef package_search_dirs(package_dirs, scan=False):\n    \"\"\"\n    Add a list of directories to search for packages under.\n    If scan is true, these directories will be scanned for packages.\n    \"\"\"",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "_PACKAGE_OPTS",
        "kind": 5,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "_PACKAGE_OPTS = {'check_l4t_version': True}\n_PACKAGE_KEYS = ['alias', 'build_args', 'build_flags', 'config', 'depends', 'disabled',  \n                 'dockerfile', 'docs', 'group', 'name', 'notes', 'path', \n                 'prefix', 'postfix', 'requires', 'test']\ndef package_search_dirs(package_dirs, scan=False):\n    \"\"\"\n    Add a list of directories to search for packages under.\n    If scan is true, these directories will be scanned for packages.\n    \"\"\"\n    global _PACKAGE_DIRS",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "_PACKAGE_KEYS",
        "kind": 5,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "_PACKAGE_KEYS = ['alias', 'build_args', 'build_flags', 'config', 'depends', 'disabled',  \n                 'dockerfile', 'docs', 'group', 'name', 'notes', 'path', \n                 'prefix', 'postfix', 'requires', 'test']\ndef package_search_dirs(package_dirs, scan=False):\n    \"\"\"\n    Add a list of directories to search for packages under.\n    If scan is true, these directories will be scanned for packages.\n    \"\"\"\n    global _PACKAGE_DIRS\n    if isinstance(package_dirs, str) and len(package_dirs) > 0:",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "#_CURRENT_PACKAGE",
        "kind": 5,
        "importPath": "jetson_containers.packages",
        "description": "jetson_containers.packages",
        "peekOfCode": "#_CURRENT_PACKAGE = None\ndef package_alias(alias):\n    \"\"\"\n    Add an alternate name (or list of names) to the current package.\n    It will then be able to be found using these other aliases.\n    \"\"\"\n    if isinstance(alias, str):\n        alias = [alias]\n    elif not isinstance(alias, list):\n        raise ValueError(\"alias should be a string or list of strings\")",
        "detail": "jetson_containers.packages",
        "documentation": {}
    },
    {
        "label": "check_dependencies",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def check_dependencies(install=True):\n    \"\"\"\n    Check if the required pip packages are available, and install them if needed.\n    \"\"\"\n    try:\n        import yaml\n        import wget\n        import dockerhub_api\n        from packaging.version import Version\n        x = Version('1.2.3') # check that .major, .minor, .micro are available",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "query_yes_no",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def query_yes_no(question, default=\"no\"):\n    \"\"\"\n    Ask a yes/no question via raw_input() and return their answer.\n    \"question\" is a string that is presented to the user.\n    \"default\" is the presumed answer if the user just hits <Enter>.\n            It must be \"yes\" (the default), \"no\" or None (meaning\n            an answer is required of the user).\n    The \"answer\" return value is True for \"yes\" or False for \"no\".\n    \"\"\"\n    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "split_container_name",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def split_container_name(name):\n    \"\"\"\n    Splits a container name like `dustynv/ros:tag` or `nvcr.io/nvidia/l4t-pytorch:tag`\n    into a (namespace, repository, tag) tuple (where namespace would be `dustynv` or\n    `nvcr.io/nvidia`, and repository would be `ros` or `l4t-pytorch`)\n    \"\"\"\n    parts = name.split(':')\n    repo = parts[0]\n    namespace = ''\n    tag = ''",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "user_in_group",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def user_in_group(group):\n    \"\"\"\n    Returns true if the user running the current process is in the specified user group.\n    Equivalent to this bash command:   id -nGz \"$USER\" | grep -qzxF \"$GROUP\"\n    \"\"\"\n    try:\n        group = grp.getgrnam(group)\n    except KeyError:\n        return False\n    return (group.gr_gid in os.getgroups())",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "is_root_user",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def is_root_user():\n    \"\"\"\n    Returns true if this is the root user running\n    \"\"\"\n    return os.geteuid() == 0\ndef needs_sudo(group='docker'):\n    \"\"\"\n    Returns true if sudo is needed to use the docker engine (if user isn't in the docker group)\n    \"\"\"\n    if is_root_user():",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "needs_sudo",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def needs_sudo(group='docker'):\n    \"\"\"\n    Returns true if sudo is needed to use the docker engine (if user isn't in the docker group)\n    \"\"\"\n    if is_root_user():\n        return False\n    else:\n        return not user_in_group(group)\ndef sudo_prefix(group='docker'):\n    \"\"\"",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "sudo_prefix",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def sudo_prefix(group='docker'):\n    \"\"\"\n    Returns a sudo prefix for command strings if the user needs sudo for accessing docker\n    \"\"\"\n    if needs_sudo(group):\n        #print('USER NEEDS SUDO')\n        return \"sudo \"\n    else:\n        return \"\"\ndef github_latest_commit(repo, branch='main'):",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "github_latest_commit",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def github_latest_commit(repo, branch='main'):\n    \"\"\"\n    Returns the SHA of the latest commit to the given github user/repo/branch.\n    \"\"\"\n    url = f\"https://api.github.com/repos/{repo}/commits/{branch}\"\n    github_token = os.environ.get('GITHUB_TOKEN')\n    if github_token:\n        log_debug(f\"-- GITHUB_TOKEN={github_token}\")\n        headers = {'Authorization': 'token %s' % github_token}\n        request = Request(url, headers=headers)",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "log_debug",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def log_debug(*args, **kwargs):\n    \"\"\"\n    Debug print function that only prints when VERBOSE or DEBUG environment variable is set\n    TODO change this to use python logging APIs or move to logging.py\n    \"\"\"\n    if os.environ.get('VERBOSE', False) or os.environ.get('DEBUG', False):\n        print(*args, **kwargs)\ndef pprint_debug(*args, **kwargs):\n    \"\"\"\n    Debug print function that only prints when VERBOSE or DEBUG environment variable is set",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "pprint_debug",
        "kind": 2,
        "importPath": "jetson_containers.utils",
        "description": "jetson_containers.utils",
        "peekOfCode": "def pprint_debug(*args, **kwargs):\n    \"\"\"\n    Debug print function that only prints when VERBOSE or DEBUG environment variable is set\n    TODO change this to use python logging APIs or move to logging.py\n    \"\"\"\n    if os.environ.get('VERBOSE', False) or os.environ.get('DEBUG', False):\n        pprint.pprint(*args, **kwargs)",
        "detail": "jetson_containers.utils",
        "documentation": {}
    },
    {
        "label": "build_arrow",
        "kind": 2,
        "importPath": "packages.arrow.config",
        "description": "packages.arrow.config",
        "peekOfCode": "def build_arrow(version, branch, default=False):\n    arrow = package.copy()\n    arrow['name'] = f'arrow:{version}'\n    arrow['build_args'] = {'ARROW_BRANCH': branch}\n    if default:\n        arrow['alias'] = 'arrow'\n    return arrow\npackage = [\n    build_arrow('14.0.1', 'apache-arrow-14.0.1', default=True),\n    build_arrow('12.0.1', 'apache-arrow-12.0.1'),",
        "detail": "packages.arrow.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.arrow.config",
        "description": "packages.arrow.config",
        "peekOfCode": "package = [\n    build_arrow('14.0.1', 'apache-arrow-14.0.1', default=True),\n    build_arrow('12.0.1', 'apache-arrow-12.0.1'),\n    build_arrow('5.0.0', 'apache-arrow-5.0.0'),\n]",
        "detail": "packages.arrow.config",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "packages.audio.piper-tts.test",
        "description": "packages.audio.piper-tts.test",
        "peekOfCode": "def main(model='en_US-lessac-high', config=None, cache=os.environ.get('PIPER_CACHE'),\n         speaker=0, length_scale=1.0, noise_scale=0.667, noise_w=0.8, sentence_silence=0.2,\n         prompt=DEFAULT_PROMPT, output='/dev/null', use_cuda=True, runs=5, dump=False, **kwargs):\n    # Download voice info\n    try:\n        voices_info = get_voices(cache, update_voices=True)\n    except Exception as error:\n        print(f\"Failed to download Piper voice list  ({error})\")\n        voices_info = get_voices(cache)\n    # Resolve aliases for backwards compatibility with old voice names",
        "detail": "packages.audio.piper-tts.test",
        "documentation": {}
    },
    {
        "label": "p",
        "kind": 5,
        "importPath": "packages.audio.riva-client.list_audio_devices",
        "description": "packages.audio.riva-client.list_audio_devices",
        "peekOfCode": "p = pyaudio.PyAudio()\nprint(\"\\nAUDIO DEVICES:\\n\")\nfor i in range(p.get_device_count()):\n    dev = p.get_device_info_by_index(i)\n    print(f\"{dev['index']:2d}: {dev['name']:50s} (inputs={dev['maxInputChannels']:<3d} outputs={dev['maxOutputChannels']:<3d} sampleRate={int(dev['defaultSampleRate'])})\")\n    #pprint.pprint(dev)",
        "detail": "packages.audio.riva-client.list_audio_devices",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "packages.audio.riva-client.loopback",
        "description": "packages.audio.riva-client.loopback",
        "peekOfCode": "def parse_args():\n    default_device_info = riva.client.audio_io.get_default_input_device_info()\n    default_device_index = None if default_device_info is None else default_device_info['index']\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"--list-devices\", action=\"store_true\", help=\"List output audio devices indices.\")\n    parser.add_argument(\"--input-device\", type=int, default=default_device_index, help=\"An input audio device to use.\")\n    parser.add_argument(\"--output-device\", type=int, help=\"Output device to use.\")\n    parser.add_argument(\"-o\", \"--output\", type=str, help=\"Output file .wav file to write synthesized audio.\")\n    parser.add_argument(\"--no-punctuation\", action='store_true', help=\"Disable ASR automatic punctuation\")\n    parser.add_argument(\"--voice\", help=\"A voice name to use for TTS. If this parameter is missing, then the server will try a first available model based on parameter `--language-code`.\")",
        "detail": "packages.audio.riva-client.loopback",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "packages.audio.riva-client.loopback",
        "description": "packages.audio.riva-client.loopback",
        "peekOfCode": "def main():\n    args = parse_args()\n    if args.list_devices:\n        riva.client.audio_io.list_output_devices()\n        return\n    auth = riva.client.Auth(args.ssl_cert, args.use_ssl, args.server)\n    nchannels = 1\n    sampwidth = 2\n    asr_service = riva.client.ASRService(auth)\n    tts_service = riva.client.SpeechSynthesisService(auth)",
        "detail": "packages.audio.riva-client.loopback",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.audio.whisper_trt.test",
        "description": "packages.audio.whisper_trt.test",
        "peekOfCode": "model = load_trt_model(\"tiny.en\", verbose=True)  # base.en small.en\nprint(f\"Transcribing {AUDIO}\")\nresult = model.transcribe(\"/data/audio/dusty.wav\") # or pass numpy array\nprint(result['text'])",
        "detail": "packages.audio.whisper_trt.test",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "packages.audio.whisper_trt.test",
        "description": "packages.audio.whisper_trt.test",
        "peekOfCode": "result = model.transcribe(\"/data/audio/dusty.wav\") # or pass numpy array\nprint(result['text'])",
        "detail": "packages.audio.whisper_trt.test",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "packages.audio.xtts.test",
        "description": "packages.audio.xtts.test",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#print(TTS().list_models())\nmodel=\"tts_models/multilingual/multi-dataset/xtts_v2\"  # \"tts_models/multilingual/multi-dataset/xtts_v1.1\"\nspeaker='Sofia Hellen'\nlanguage='en'\nprint(f\"Loading TTS model {model}\")\ntts = TTS(model).to(device)\nprint(dir(tts.synthesizer.tts_model.speaker_manager))\nprint(tts.synthesizer.tts_model.speaker_manager)\nprint(f\"\\nMulti-speaker:  {tts.is_multi_speaker}\")",
        "detail": "packages.audio.xtts.test",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 5,
        "importPath": "packages.audio.xtts.test",
        "description": "packages.audio.xtts.test",
        "peekOfCode": "tts = TTS(model).to(device)\nprint(dir(tts.synthesizer.tts_model.speaker_manager))\nprint(tts.synthesizer.tts_model.speaker_manager)\nprint(f\"\\nMulti-speaker:  {tts.is_multi_speaker}\")\nif tts.is_multi_speaker:\n    print(f\"\\nSpeakers:  {tts.synthesizer.tts_model.speaker_manager.name_to_id}\")\nprint(f\"\\nLanguages:  {tts.synthesizer.tts_model.language_manager.name_to_id}\")\n# Text to speech to a file\nprompts = [\n    \"Hello there, how are you today?\", ",
        "detail": "packages.audio.xtts.test",
        "documentation": {}
    },
    {
        "label": "prompts",
        "kind": 5,
        "importPath": "packages.audio.xtts.test",
        "description": "packages.audio.xtts.test",
        "peekOfCode": "prompts = [\n    \"Hello there, how are you today?\", \n    \"The weather is 76 degrees out and sunny.\", \n    \"Your first meeting is in an hour downtown, with normal traffic.\",\n    \"Can I interest you in anything quick for breakfast?\",\n]\nif tts.is_multi_speaker:\n    prompts = [' '.join(prompts)] #+ prompts\nfor prompt_idx, prompt in enumerate(prompts):\n    wav = f\"/data/audio/tts/{os.path.basename(model)}_offline_{speaker.lower().replace(' ', '_')}.wav\" #_{prompt_idx}.wav\"",
        "detail": "packages.audio.xtts.test",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlogging.info(f\"loading TTS model {model_dir}\")\nconfig = XttsConfig()\nconfig.load_json(f\"{model_dir}/config.json\")\nlogging.info(f\"TTS model config:\\n{pprint.pformat(config, indent=1)}\")\nmodel = Xtts.init_from_config(config)\nmodel.load_checkpoint(\n    config, \n    checkpoint_dir=model_dir, \n    speaker_file_path=f\"{model_dir}/speakers_xtts.pth\",",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "config = XttsConfig()\nconfig.load_json(f\"{model_dir}/config.json\")\nlogging.info(f\"TTS model config:\\n{pprint.pformat(config, indent=1)}\")\nmodel = Xtts.init_from_config(config)\nmodel.load_checkpoint(\n    config, \n    checkpoint_dir=model_dir, \n    speaker_file_path=f\"{model_dir}/speakers_xtts.pth\",\n    use_tensorrt=True\n)",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "model = Xtts.init_from_config(config)\nmodel.load_checkpoint(\n    config, \n    checkpoint_dir=model_dir, \n    speaker_file_path=f\"{model_dir}/speakers_xtts.pth\",\n    use_tensorrt=True\n)\nmodel.cuda()\nspeaker_manager = model.speaker_manager\ngpt_cond_latent, speaker_embedding = speaker_manager.speakers[speaker].values()",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "speaker_manager = model.speaker_manager\ngpt_cond_latent, speaker_embedding = speaker_manager.speakers[speaker].values()\ngpt_cond_latent.to(device)\nspeaker_embedding.to(device)\nprompts = [\n    \"Hello there, how are you today?\", \n    \"The weather is 76 degrees out and sunny.\", \n    \"Your first meeting is in an hour downtown, with normal traffic.\",\n    \"Can I interest you in anything quick for breakfast?\",\n]",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "prompts",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "prompts = [\n    \"Hello there, how are you today?\", \n    \"The weather is 76 degrees out and sunny.\", \n    \"Your first meeting is in an hour downtown, with normal traffic.\",\n    \"Can I interest you in anything quick for breakfast?\",\n]\nprompts = [' '.join(prompts)] + prompts\nlong_prompt = \"\"\"French onion soup is a classic and delicious dish that is easy to make at home. Here's a simple recipe for French onion soup that you can try:\nIngredients:\n* 1 onion, 1/4 cup, chopped",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "prompts",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "prompts = [' '.join(prompts)] + prompts\nlong_prompt = \"\"\"French onion soup is a classic and delicious dish that is easy to make at home. Here's a simple recipe for French onion soup that you can try:\nIngredients:\n* 1 onion, 1/4 cup, chopped\n* 2 tablespoons butter\n* 1/4 cup white wine (optional)\n* 4 cups beef broth\n* 2 tablespoons tomato paste\n* 1 teaspoon dried thyme\n* 1/2 teaspoon dried oregano",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "long_prompt",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "long_prompt = \"\"\"French onion soup is a classic and delicious dish that is easy to make at home. Here's a simple recipe for French onion soup that you can try:\nIngredients:\n* 1 onion, 1/4 cup, chopped\n* 2 tablespoons butter\n* 1/4 cup white wine (optional)\n* 4 cups beef broth\n* 2 tablespoons tomato paste\n* 1 teaspoon dried thyme\n* 1/2 teaspoon dried oregano\n* 1/2 teaspoon salt",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "#prompts",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_stream",
        "description": "packages.audio.xtts.test_stream",
        "peekOfCode": "#prompts = [long_prompt[:500]]\nfor prompt_idx, prompt in enumerate(prompts):\n    wav_path = f\"/data/audio/tts/{model_name}_streaming_{speaker.lower().replace(' ', '_')}_{prompt_idx}.wav\"\n    logging.info(f'\\nstreaming \"{prompt}\"  speaker=\"{speaker}\"  lang=\"{language}\"  wav=\"{wav_path}\"\\n')\n    time_begin = time.perf_counter()\n    time_last = time_begin\n    chunks = model.inference_stream(\n        prompt,\n        language,\n        gpt_cond_latent,",
        "detail": "packages.audio.xtts.test_stream",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_voices",
        "description": "packages.audio.xtts.test_voices",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#print(TTS().list_models())\nmodel=\"tts_models/multilingual/multi-dataset/xtts_v2\"  # \"tts_models/multilingual/multi-dataset/xtts_v1.1\"\nlanguage='en'\nprint(f\"Loading TTS model {model}\")\ntts = TTS(model).to(device)\nprint(dir(tts.synthesizer.tts_model.speaker_manager))\nprint(tts.synthesizer.tts_model.speaker_manager)\nprint(f\"\\nMulti-speaker:  {tts.is_multi_speaker}\")\nif tts.is_multi_speaker:",
        "detail": "packages.audio.xtts.test_voices",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_voices",
        "description": "packages.audio.xtts.test_voices",
        "peekOfCode": "tts = TTS(model).to(device)\nprint(dir(tts.synthesizer.tts_model.speaker_manager))\nprint(tts.synthesizer.tts_model.speaker_manager)\nprint(f\"\\nMulti-speaker:  {tts.is_multi_speaker}\")\nif tts.is_multi_speaker:\n    print(f\"\\nSpeakers:  {tts.synthesizer.tts_model.speaker_manager.name_to_id}\")\nprint(f\"\\nLanguages:  {tts.synthesizer.tts_model.language_manager.name_to_id}\")\n# Text to speech to a file\nprompts = [\n    \"Hello there, how are you today?\", ",
        "detail": "packages.audio.xtts.test_voices",
        "documentation": {}
    },
    {
        "label": "prompts",
        "kind": 5,
        "importPath": "packages.audio.xtts.test_voices",
        "description": "packages.audio.xtts.test_voices",
        "peekOfCode": "prompts = [\n    \"Hello there, how are you today?\", \n    \"The weather is 76 degrees out and sunny.\", \n    \"Your first meeting is in an hour downtown, with normal traffic.\",\n    \"Can I interest you in anything quick for breakfast?\",\n]\nif tts.is_multi_speaker:\n    prompts = [' '.join(prompts)] #+ prompts\nfor speaker in tts.synthesizer.tts_model.speaker_manager.speakers:\n    for prompt_idx, prompt in enumerate(prompts):",
        "detail": "packages.audio.xtts.test_voices",
        "documentation": {}
    },
    {
        "label": "protobuf_implementation",
        "kind": 5,
        "importPath": "packages.build.protobuf.protobuf_apt.test",
        "description": "packages.build.protobuf.protobuf_apt.test",
        "peekOfCode": "protobuf_implementation = str(api_implementation.Type())\nif Version(google.protobuf.__version__).major < 4:\n    print(f'protobuf default API implementation: {str(api_implementation._default_implementation_type)}')\nprint(f'protobuf active API implementation:  {protobuf_implementation}')\nprint('protobuf (apt) OK\\n')",
        "detail": "packages.build.protobuf.protobuf_apt.test",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.build.protobuf.protobuf_cpp.config",
        "description": "packages.build.protobuf.protobuf_cpp.config",
        "peekOfCode": "package['build_args'] = {\n    'PROTOBUF_VERSION': PROTOBUF_VERSION,\n}",
        "detail": "packages.build.protobuf.protobuf_cpp.config",
        "documentation": {}
    },
    {
        "label": "protobuf_implementation",
        "kind": 5,
        "importPath": "packages.build.protobuf.protobuf_cpp.test",
        "description": "packages.build.protobuf.protobuf_cpp.test",
        "peekOfCode": "protobuf_implementation = str(api_implementation.Type())\nprint(f'protobuf default API implementation: {str(api_implementation._default_implementation_type)}')\nprint(f'protobuf active API implementation:  {protobuf_implementation}')\nif protobuf_implementation != \"cpp\":\n    raise ValueError(f'expected protobuf to have cpp implementation, but instead it has {protobuf_implementation} implementation')\nprint('protobuf (cpp) OK\\n')",
        "detail": "packages.build.protobuf.protobuf_cpp.test",
        "documentation": {}
    },
    {
        "label": "python",
        "kind": 2,
        "importPath": "packages.build.python.config",
        "description": "packages.build.python.config",
        "peekOfCode": "def python(version, requires=None) -> list:\n    pkg = package.copy()\n    pkg['name'] = f'python:{version}'\n    pkg['build_args'] = {'PYTHON_VERSION_ARG': version}\n    if Version(version) == PYTHON_VERSION:\n        pkg['alias'] = 'python'\n    if requires:\n        pkg['requires'] = requires\n    return pkg\npackage = [",
        "detail": "packages.build.python.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.build.python.config",
        "description": "packages.build.python.config",
        "peekOfCode": "package = [\n    python('3.6', '==32.*'),  # JetPack 4\n    python('3.8', '<36'),     # JetPack 4 + 5\n    python('3.10', '>=34'),   # JetPack 5 + 6\n    python('3.11', '>=34'),   # JetPack 6\n    python('3.12', '>=34'),   # JetPack 6\n]",
        "detail": "packages.build.python.config",
        "documentation": {}
    },
    {
        "label": "ctranslate2",
        "kind": 2,
        "importPath": "packages.ctranslate2.config",
        "description": "packages.ctranslate2.config",
        "peekOfCode": "def ctranslate2(version, requires=None, default=False):\n    ct = package.copy()\n    ct['name'] = f'ctranslate2:{version}'\n    ct['build_args'] = {\n        'CTRANSLATE_VERSION': version,\n        'CTRANSLATE_BRANCH': 'v' + version if version.replace('.','').isnumeric() else version\n    }\n    if requires:\n        ct['requires'] = requires\n    builder = ct.copy()",
        "detail": "packages.ctranslate2.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.ctranslate2.config",
        "description": "packages.ctranslate2.config",
        "peekOfCode": "package = [\n    ctranslate2('master', default=False),\n    ctranslate2('4.2.0', default=True)\n]",
        "detail": "packages.ctranslate2.config",
        "documentation": {}
    },
    {
        "label": "cuda_build_args",
        "kind": 2,
        "importPath": "packages.cuda.cuda.config",
        "description": "packages.cuda.cuda.config",
        "peekOfCode": "def cuda_build_args(version):\n    short_version = f\"cu{version.replace('.', '')}\"\n    repo_path = f\"jp{JETPACK_VERSION.major}/{short_version}\"\n    index_host = \"jetson.webredirect.org\"\n    return {\n        'CUDA_ARCH_LIST': ';'.join([str(x) for x in CUDA_ARCHITECTURES]),\n        'DISTRO': f\"ubuntu{LSB_RELEASE.replace('.','')}\",\n        'TAR_INDEX_URL': f\"http://{index_host}:8000/{repo_path}\",\n        'PIP_INDEX_REPO': f\"http://{index_host}/{repo_path}\",\n        'PIP_TRUSTED_HOSTS': index_host,",
        "detail": "packages.cuda.cuda.config",
        "documentation": {}
    },
    {
        "label": "cuda_package",
        "kind": 2,
        "importPath": "packages.cuda.cuda.config",
        "description": "packages.cuda.cuda.config",
        "peekOfCode": "def cuda_package(version, url, deb, packages=None, requires=None) -> list:\n    \"\"\"\n    Generate containers for a particular version of CUDA installed from debian packages\n    This will download & install the specified packages (by default the full CUDA Toolkit) \n    from a .deb URL from developer.nvidia.com/cuda-downloads (the `aarch64-jetson` versions)\n    \"\"\"\n    if not packages:\n        packages = os.environ.get('CUDA_PACKAGES', 'cuda-toolkit*')\n    cuda = package.copy()\n    cuda['name'] = f'cuda:{version}'",
        "detail": "packages.cuda.cuda.config",
        "documentation": {}
    },
    {
        "label": "cuda_builtin",
        "kind": 2,
        "importPath": "packages.cuda.cuda.config",
        "description": "packages.cuda.cuda.config",
        "peekOfCode": "def cuda_builtin(version, requires=None) -> list:\n    \"\"\"\n    Backwards-compatability for when CUDA already installed in base container (like l4t-jetpack)\n    This will just retag the base, marking CUDA dependency as satisfied in any downstream containers.\n    \"\"\"\n    passthrough = package.copy()\n    if not isinstance(version, str):\n        version = f'{version.major}.{version.minor}'\n    passthrough['name'] = f'cuda:{version}'\n    passthrough['dockerfile'] = 'Dockerfile.builtin'",
        "detail": "packages.cuda.cuda.config",
        "documentation": {}
    },
    {
        "label": "cuda_samples",
        "kind": 2,
        "importPath": "packages.cuda.cuda.config",
        "description": "packages.cuda.cuda.config",
        "peekOfCode": "def cuda_samples(version, requires) -> list:\n    \"\"\"\n    Generates container that installs/builds the CUDA samples\n    \"\"\"\n    samples = package.copy()\n    if not isinstance(version, str):\n        version = f'{version.major}.{version.minor}'\n    samples['name'] = f'cuda:{version}-samples'\n    samples['dockerfile'] = 'Dockerfile.samples'\n    samples['notes'] = \"CUDA samples from https://github.com/NVIDIA/cuda-samples installed under /opt/cuda-samples\"",
        "detail": "packages.cuda.cuda.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.cuda.cuda.config",
        "description": "packages.cuda.cuda.config",
        "peekOfCode": "package = [\n    # JetPack 6\n    cuda_package('12.2', 'https://nvidia.box.com/shared/static/uvqtun1sc0bq76egarc8wwuh6c23e76e.deb', 'cuda-tegra-repo-ubuntu2204-12-2-local', requires='==36.*'), \n    cuda_package('12.4', 'https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda-tegra-repo-ubuntu2204-12-4-local_12.4.1-1_arm64.deb', 'cuda-tegra-repo-ubuntu2204-12-4-local', requires='==36.*'), \n    cuda_samples('12.2', requires='==36.*'),\n    cuda_samples('12.4', requires='==36.*'),\n    # JetPack 5\n    cuda_package('12.2', 'https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda-tegra-repo-ubuntu2004-12-2-local_12.2.2-1_arm64.deb', 'cuda-tegra-repo-ubuntu2004-12-2-local', requires='==35.*'),\n    cuda_package('11.8', 'https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-tegra-repo-ubuntu2004-11-8-local_11.8.0-1_arm64.deb', 'cuda-tegra-repo-ubuntu2004-11-8-local', requires='==35.*'),\n    cuda_samples('12.2', requires='==35.*'),",
        "detail": "packages.cuda.cuda.config",
        "documentation": {}
    },
    {
        "label": "cuda_python",
        "kind": 2,
        "importPath": "packages.cuda.cuda-python.config",
        "description": "packages.cuda.cuda-python.config",
        "peekOfCode": "def cuda_python(version, cuda=None):\n    pkg = package.copy()\n    pkg['name'] = f\"cuda-python:{version}\"\n    if Version(version) == CUDA_VERSION:\n        pkg['alias'] = 'cuda-python'\n    if not cuda:\n        cuda = version\n    if len(cuda.split('.')) > 2:\n        cuda = cuda[:-2]\n    pkg['depends'] = update_dependencies(pkg['depends'], f\"cuda:{cuda}\")",
        "detail": "packages.cuda.cuda-python.config",
        "documentation": {}
    },
    {
        "label": "cuDevice",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "cuDevice = cuda.CUdevice(0)\ncuContext = checkCudaErrors(cuda.cuCtxCreate(0, cuDevice))\nprint('cuda device name:', checkCudaErrors(cuda.cuDeviceGetName(512, cuDevice)))\nuvaSupported = checkCudaErrors(cuda.cuDeviceGetAttribute(cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING, cuDevice))\nif not uvaSupported:\n    raise RuntimeError(\"Accessing pageable memory directly requires UVA\")\nvectorAddDrv = '''\\\n/* Vector addition: C = A + B.\n *\n * This sample is a very basic sample that implements element by element",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "cuContext",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "cuContext = checkCudaErrors(cuda.cuCtxCreate(0, cuDevice))\nprint('cuda device name:', checkCudaErrors(cuda.cuDeviceGetName(512, cuDevice)))\nuvaSupported = checkCudaErrors(cuda.cuDeviceGetAttribute(cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING, cuDevice))\nif not uvaSupported:\n    raise RuntimeError(\"Accessing pageable memory directly requires UVA\")\nvectorAddDrv = '''\\\n/* Vector addition: C = A + B.\n *\n * This sample is a very basic sample that implements element by element\n * vector addition. It is the same as the sample illustrating Chapter 3",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "uvaSupported",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "uvaSupported = checkCudaErrors(cuda.cuDeviceGetAttribute(cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING, cuDevice))\nif not uvaSupported:\n    raise RuntimeError(\"Accessing pageable memory directly requires UVA\")\nvectorAddDrv = '''\\\n/* Vector addition: C = A + B.\n *\n * This sample is a very basic sample that implements element by element\n * vector addition. It is the same as the sample illustrating Chapter 3\n * of the programming guide with some additions like error checking.\n *",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "vectorAddDrv",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "vectorAddDrv = '''\\\n/* Vector addition: C = A + B.\n *\n * This sample is a very basic sample that implements element by element\n * vector addition. It is the same as the sample illustrating Chapter 3\n * of the programming guide with some additions like error checking.\n *\n */\n// Device code\nextern \"C\" __global__ void VecAdd_kernel(const float *A, const float *B, float *C, int N)",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "kernelHelper",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "kernelHelper = KernelHelper(vectorAddDrv, int(cuDevice))\n_VecAdd_kernel = kernelHelper.getFunction(b'VecAdd_kernel')\nN = 50000\nsize = N * np.dtype(np.float32).itemsize\n# Allocate input vectors h_A and h_B in host memory\nh_A = np.random.rand(size).astype(dtype=np.float32)\nh_B = np.random.rand(size).astype(dtype=np.float32)\nh_C = np.random.rand(size).astype(dtype=np.float32)\n# Allocate vectors in device memory\nd_A = checkCudaErrors(cuda.cuMemAlloc(size))",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "_VecAdd_kernel",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "_VecAdd_kernel = kernelHelper.getFunction(b'VecAdd_kernel')\nN = 50000\nsize = N * np.dtype(np.float32).itemsize\n# Allocate input vectors h_A and h_B in host memory\nh_A = np.random.rand(size).astype(dtype=np.float32)\nh_B = np.random.rand(size).astype(dtype=np.float32)\nh_C = np.random.rand(size).astype(dtype=np.float32)\n# Allocate vectors in device memory\nd_A = checkCudaErrors(cuda.cuMemAlloc(size))\nd_B = checkCudaErrors(cuda.cuMemAlloc(size))",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "N",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "N = 50000\nsize = N * np.dtype(np.float32).itemsize\n# Allocate input vectors h_A and h_B in host memory\nh_A = np.random.rand(size).astype(dtype=np.float32)\nh_B = np.random.rand(size).astype(dtype=np.float32)\nh_C = np.random.rand(size).astype(dtype=np.float32)\n# Allocate vectors in device memory\nd_A = checkCudaErrors(cuda.cuMemAlloc(size))\nd_B = checkCudaErrors(cuda.cuMemAlloc(size))\nd_C = checkCudaErrors(cuda.cuMemAlloc(size))",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "size",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "size = N * np.dtype(np.float32).itemsize\n# Allocate input vectors h_A and h_B in host memory\nh_A = np.random.rand(size).astype(dtype=np.float32)\nh_B = np.random.rand(size).astype(dtype=np.float32)\nh_C = np.random.rand(size).astype(dtype=np.float32)\n# Allocate vectors in device memory\nd_A = checkCudaErrors(cuda.cuMemAlloc(size))\nd_B = checkCudaErrors(cuda.cuMemAlloc(size))\nd_C = checkCudaErrors(cuda.cuMemAlloc(size))\n# Copy vectors from host memory to device memory",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "h_A",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "h_A = np.random.rand(size).astype(dtype=np.float32)\nh_B = np.random.rand(size).astype(dtype=np.float32)\nh_C = np.random.rand(size).astype(dtype=np.float32)\n# Allocate vectors in device memory\nd_A = checkCudaErrors(cuda.cuMemAlloc(size))\nd_B = checkCudaErrors(cuda.cuMemAlloc(size))\nd_C = checkCudaErrors(cuda.cuMemAlloc(size))\n# Copy vectors from host memory to device memory\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_A, h_A, size))\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_B, h_B, size))",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "h_B",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "h_B = np.random.rand(size).astype(dtype=np.float32)\nh_C = np.random.rand(size).astype(dtype=np.float32)\n# Allocate vectors in device memory\nd_A = checkCudaErrors(cuda.cuMemAlloc(size))\nd_B = checkCudaErrors(cuda.cuMemAlloc(size))\nd_C = checkCudaErrors(cuda.cuMemAlloc(size))\n# Copy vectors from host memory to device memory\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_A, h_A, size))\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_B, h_B, size))\nif True:",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "h_C",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "h_C = np.random.rand(size).astype(dtype=np.float32)\n# Allocate vectors in device memory\nd_A = checkCudaErrors(cuda.cuMemAlloc(size))\nd_B = checkCudaErrors(cuda.cuMemAlloc(size))\nd_C = checkCudaErrors(cuda.cuMemAlloc(size))\n# Copy vectors from host memory to device memory\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_A, h_A, size))\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_B, h_B, size))\nif True:\n    # Grid/Block configuration",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "d_A",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "d_A = checkCudaErrors(cuda.cuMemAlloc(size))\nd_B = checkCudaErrors(cuda.cuMemAlloc(size))\nd_C = checkCudaErrors(cuda.cuMemAlloc(size))\n# Copy vectors from host memory to device memory\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_A, h_A, size))\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_B, h_B, size))\nif True:\n    # Grid/Block configuration\n    threadsPerBlock = 256\n    blocksPerGrid   = (N + threadsPerBlock - 1) / threadsPerBlock",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "d_B",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "d_B = checkCudaErrors(cuda.cuMemAlloc(size))\nd_C = checkCudaErrors(cuda.cuMemAlloc(size))\n# Copy vectors from host memory to device memory\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_A, h_A, size))\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_B, h_B, size))\nif True:\n    # Grid/Block configuration\n    threadsPerBlock = 256\n    blocksPerGrid   = (N + threadsPerBlock - 1) / threadsPerBlock\n    kernelArgs = ((d_A, d_B, d_C, N),",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "d_C",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "d_C = checkCudaErrors(cuda.cuMemAlloc(size))\n# Copy vectors from host memory to device memory\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_A, h_A, size))\ncheckCudaErrors(cuda.cuMemcpyHtoD(d_B, h_B, size))\nif True:\n    # Grid/Block configuration\n    threadsPerBlock = 256\n    blocksPerGrid   = (N + threadsPerBlock - 1) / threadsPerBlock\n    kernelArgs = ((d_A, d_B, d_C, N),\n                  (None, None, None, ctypes.c_int))",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "print(\"{}\".format(\"Result",
        "kind": 5,
        "importPath": "packages.cuda.cuda-python.test_driver",
        "description": "packages.cuda.cuda-python.test_driver",
        "peekOfCode": "print(\"{}\".format(\"Result = PASS\" if i+1 == N else \"Result = FAIL\"))\nif i+1 != N:\n    raise RuntimeError(\"VecAdd_kernel computed invalid results\")\nprint('cuda-python (driver API) OK\\n')",
        "detail": "packages.cuda.cuda-python.test_driver",
        "documentation": {}
    },
    {
        "label": "KernelHelper",
        "kind": 6,
        "importPath": "packages.cuda.cuda-python.utils",
        "description": "packages.cuda.cuda-python.utils",
        "peekOfCode": "class KernelHelper:\n    def __init__(self, code, devID):\n        prog = checkCudaErrors(nvrtc.nvrtcCreateProgram(str.encode(code), b'sourceCode.cu', 0, [], []))\n        CUDA_HOME = os.getenv('CUDA_HOME')\n        if CUDA_HOME == None:\n            raise RuntimeError('Environment variable CUDA_HOME is not defined')\n        include_dirs = os.path.join(CUDA_HOME, 'include')\n        # Initialize CUDA\n        checkCudaErrors(cudart.cudaFree(0))\n        major = checkCudaErrors(cudart.cudaDeviceGetAttribute(cudart.cudaDeviceAttr.cudaDevAttrComputeCapabilityMajor, devID))",
        "detail": "packages.cuda.cuda-python.utils",
        "documentation": {}
    },
    {
        "label": "checkCudaErrors",
        "kind": 2,
        "importPath": "packages.cuda.cuda-python.utils",
        "description": "packages.cuda.cuda-python.utils",
        "peekOfCode": "def checkCudaErrors(result):\n    if result[0].value:\n        raise RuntimeError(\"CUDA error code={}({})\".format(result[0].value, _cudaGetErrorEnum(result[0])))\n    if len(result) == 1:\n        return None\n    elif len(result) == 2:\n        return result[1]\n    else:\n        return result[1:]\nclass KernelHelper:",
        "detail": "packages.cuda.cuda-python.utils",
        "documentation": {}
    },
    {
        "label": "cudnn_package",
        "kind": 2,
        "importPath": "packages.cuda.cudnn.config",
        "description": "packages.cuda.cudnn.config",
        "peekOfCode": "def cudnn_package(version, url, deb, packages=None, cuda=None, requires=None):\n    \"\"\"\n    Generate containers for a particular version of cuDNN installed from debian packages\n    \"\"\"\n    if not packages:\n        packages = os.environ.get('CUDNN_PACKAGES', 'libcudnn*-dev libcudnn*-samples')\n    cudnn = package.copy()\n    cudnn['name'] = f'cudnn:{version}'\n    cudnn['build_args'] = {\n        'CUDNN_URL': url,",
        "detail": "packages.cuda.cudnn.config",
        "documentation": {}
    },
    {
        "label": "cudnn_builtin",
        "kind": 2,
        "importPath": "packages.cuda.cudnn.config",
        "description": "packages.cuda.cudnn.config",
        "peekOfCode": "def cudnn_builtin(version=None, requires=None, default=False):\n    \"\"\"\n    Backwards-compatability for when cuDNN already installed in base container (like l4t-jetpack)\n    \"\"\"\n    passthrough = package.copy()\n    if version is not None:\n        if not isinstance(version, str):\n            version = f'{version.major}.{version.minor}'\n        if default:\n            passthrough['alias'] = 'cudnn'  ",
        "detail": "packages.cuda.cudnn.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.cuda.cudnn.config",
        "description": "packages.cuda.cudnn.config",
        "peekOfCode": "package = [\n    # JetPack 6\n    cudnn_package('8.9', 'https://nvidia.box.com/shared/static/ht4li6b0j365ta7b76a6gw29rk5xh8cy.deb', 'cudnn-local-tegra-repo-ubuntu2204-8.9.4.25', cuda='12.2', requires='==36.*'), \n    cudnn_package('9.0', 'https://developer.download.nvidia.com/compute/cudnn/9.0.0/local_installers/cudnn-local-tegra-repo-ubuntu2204-9.0.0_1.0-1_arm64.deb', 'cudnn-local-tegra-repo-ubuntu2204-9.0.0', cuda='12.4', requires='==36.*'),\n    # JetPack 4-5 (cuDNN installed in base container)\n    cudnn_builtin(requires='<36', default=True),\n]",
        "detail": "packages.cuda.cudnn.config",
        "documentation": {}
    },
    {
        "label": "CUPY_NVCC_GENERATE_CODE",
        "kind": 5,
        "importPath": "packages.cuda.cupy.config",
        "description": "packages.cuda.cupy.config",
        "peekOfCode": "CUPY_NVCC_GENERATE_CODE = [f\"arch=compute_{x},code=sm_{x}\" for x in CUDA_ARCHITECTURES]\nCUPY_NVCC_GENERATE_CODE = ';'.join(CUPY_NVCC_GENERATE_CODE)\npackage['build_args'] = {\n    'CUPY_VERSION': CUPY_VERSION,\n    'CUPY_NVCC_GENERATE_CODE': CUPY_NVCC_GENERATE_CODE,\n}",
        "detail": "packages.cuda.cupy.config",
        "documentation": {}
    },
    {
        "label": "CUPY_NVCC_GENERATE_CODE",
        "kind": 5,
        "importPath": "packages.cuda.cupy.config",
        "description": "packages.cuda.cupy.config",
        "peekOfCode": "CUPY_NVCC_GENERATE_CODE = ';'.join(CUPY_NVCC_GENERATE_CODE)\npackage['build_args'] = {\n    'CUPY_VERSION': CUPY_VERSION,\n    'CUPY_NVCC_GENERATE_CODE': CUPY_NVCC_GENERATE_CODE,\n}",
        "detail": "packages.cuda.cupy.config",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.cuda.cupy.config",
        "description": "packages.cuda.cupy.config",
        "peekOfCode": "package['build_args'] = {\n    'CUPY_VERSION': CUPY_VERSION,\n    'CUPY_NVCC_GENERATE_CODE': CUPY_NVCC_GENERATE_CODE,\n}",
        "detail": "packages.cuda.cupy.config",
        "documentation": {}
    },
    {
        "label": "x_gpu",
        "kind": 5,
        "importPath": "packages.cuda.cupy.test",
        "description": "packages.cuda.cupy.test",
        "peekOfCode": "x_gpu = cp.array([1, 2, 3])\nprint(x_gpu)\nl2_gpu = cp.linalg.norm(x_gpu)\nprint(l2_gpu)\nprint('done CuPy GPU array test')\nprint('CuPy OK\\n')",
        "detail": "packages.cuda.cupy.test",
        "documentation": {}
    },
    {
        "label": "l2_gpu",
        "kind": 5,
        "importPath": "packages.cuda.cupy.test",
        "description": "packages.cuda.cupy.test",
        "peekOfCode": "l2_gpu = cp.linalg.norm(x_gpu)\nprint(l2_gpu)\nprint('done CuPy GPU array test')\nprint('CuPy OK\\n')",
        "detail": "packages.cuda.cupy.test",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.config",
        "description": "packages.cuda.pycuda.config",
        "peekOfCode": "package['build_args'] = {\n    # v2022.1 is the last version to support Python 3.6\n    'PYCUDA_VERSION': 'v2022.1' if PYTHON_VERSION == Version('3.6') else 'main',\n}",
        "detail": "packages.cuda.pycuda.config",
        "documentation": {}
    },
    {
        "label": "dev",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "dev = cuda.Device(0)\nprint('CUDA device name:    ' + str(dev.name()))\nprint('CUDA device memory:  ' + str((int)(dev.total_memory()/1048576)) + ' MB')\nprint('CUDA device compute: ' + str(dev.compute_capability()))\n# allocate memory\nprint('allocating device memory...')\na_cpu = np.full((4,8), 1.0, np.float32)\nb_cpu = np.full(a_cpu.shape, 2.0, np.float32)\nc_cpu = np.empty_like(a_cpu)\na_gpu = cuda.mem_alloc(a_cpu.nbytes)",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "a_cpu",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "a_cpu = np.full((4,8), 1.0, np.float32)\nb_cpu = np.full(a_cpu.shape, 2.0, np.float32)\nc_cpu = np.empty_like(a_cpu)\na_gpu = cuda.mem_alloc(a_cpu.nbytes)\nb_gpu = cuda.mem_alloc(b_cpu.nbytes)\nc_gpu = cuda.mem_alloc(b_cpu.nbytes)\ncuda.memcpy_htod(a_gpu, a_cpu)\ncuda.memcpy_htod(b_gpu, b_cpu)\n# test cuda kernel\nprint('building CUDA kernel...')",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "b_cpu",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "b_cpu = np.full(a_cpu.shape, 2.0, np.float32)\nc_cpu = np.empty_like(a_cpu)\na_gpu = cuda.mem_alloc(a_cpu.nbytes)\nb_gpu = cuda.mem_alloc(b_cpu.nbytes)\nc_gpu = cuda.mem_alloc(b_cpu.nbytes)\ncuda.memcpy_htod(a_gpu, a_cpu)\ncuda.memcpy_htod(b_gpu, b_cpu)\n# test cuda kernel\nprint('building CUDA kernel...')\nmodule = SourceModule(\"\"\"",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "c_cpu",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "c_cpu = np.empty_like(a_cpu)\na_gpu = cuda.mem_alloc(a_cpu.nbytes)\nb_gpu = cuda.mem_alloc(b_cpu.nbytes)\nc_gpu = cuda.mem_alloc(b_cpu.nbytes)\ncuda.memcpy_htod(a_gpu, a_cpu)\ncuda.memcpy_htod(b_gpu, b_cpu)\n# test cuda kernel\nprint('building CUDA kernel...')\nmodule = SourceModule(\"\"\"\n    __global__ void cuda_add( float* a, float* b, float* c )",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "a_gpu",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "a_gpu = cuda.mem_alloc(a_cpu.nbytes)\nb_gpu = cuda.mem_alloc(b_cpu.nbytes)\nc_gpu = cuda.mem_alloc(b_cpu.nbytes)\ncuda.memcpy_htod(a_gpu, a_cpu)\ncuda.memcpy_htod(b_gpu, b_cpu)\n# test cuda kernel\nprint('building CUDA kernel...')\nmodule = SourceModule(\"\"\"\n    __global__ void cuda_add( float* a, float* b, float* c )\n    {",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "b_gpu",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "b_gpu = cuda.mem_alloc(b_cpu.nbytes)\nc_gpu = cuda.mem_alloc(b_cpu.nbytes)\ncuda.memcpy_htod(a_gpu, a_cpu)\ncuda.memcpy_htod(b_gpu, b_cpu)\n# test cuda kernel\nprint('building CUDA kernel...')\nmodule = SourceModule(\"\"\"\n    __global__ void cuda_add( float* a, float* b, float* c )\n    {\n        int idx = threadIdx.x + threadIdx.y * blockDim.x;",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "c_gpu",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "c_gpu = cuda.mem_alloc(b_cpu.nbytes)\ncuda.memcpy_htod(a_gpu, a_cpu)\ncuda.memcpy_htod(b_gpu, b_cpu)\n# test cuda kernel\nprint('building CUDA kernel...')\nmodule = SourceModule(\"\"\"\n    __global__ void cuda_add( float* a, float* b, float* c )\n    {\n        int idx = threadIdx.x + threadIdx.y * blockDim.x;\n        c[idx] = a[idx] + b[idx];",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "module",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "module = SourceModule(\"\"\"\n    __global__ void cuda_add( float* a, float* b, float* c )\n    {\n        int idx = threadIdx.x + threadIdx.y * blockDim.x;\n        c[idx] = a[idx] + b[idx];\n    }\n    \"\"\")\nfunc = module.get_function('cuda_add')\nprint('running CUDA kernel...')\nfunc(a_gpu, b_gpu, c_gpu, block=(a_cpu.shape[0], b_cpu.shape[1], 1))",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "func",
        "kind": 5,
        "importPath": "packages.cuda.pycuda.test",
        "description": "packages.cuda.pycuda.test",
        "peekOfCode": "func = module.get_function('cuda_add')\nprint('running CUDA kernel...')\nfunc(a_gpu, b_gpu, c_gpu, block=(a_cpu.shape[0], b_cpu.shape[1], 1))\ncuda.memcpy_dtoh(c_cpu, c_gpu)\nprint('CUDA kernel results:')\nprint(c_cpu)  # should be '3.0'\nprint('PyCUDA OK\\n')",
        "detail": "packages.cuda.pycuda.test",
        "documentation": {}
    },
    {
        "label": "DisplayServer",
        "kind": 6,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "class DisplayServer(object):\n    def __init__(self, *args, **kwargs):\n        part_number, jetson_part_number = get_part_number()\n        i2c_bus_number = MODULE_I2CBUS_TABLE.get(jetson_part_number)\n        logger.info(f\"part_number: {part_number}, jetson_part_number: {jetson_part_number}\")\n        logger.info(f\"i2c_bus_number = {i2c_bus_number}\")\n        if not i2c_bus_number:\n            i2c_bus_number = 7  # Default: I2C bus 7 for Jetson AGX Orin\n        self.display = Adafruit_SSD1306.SSD1306_128_32(rst=None, i2c_bus=i2c_bus_number, gpio=1)\n        self.display.begin()",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "get_part_number",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "def get_part_number():\n    part_number = ''\n    jetson_part_number = ''\n    # Find 699-level part number from EEPROM and extract P-number\n    for bus_number in I2C_EEPROM_BUS:\n        try:\n            bus = SMBus(bus_number)\n            part_number = bus.read_i2c_block_data(0x50, 20, 29)\n            part_number = ''.join(chr(i) for i in part_number).rstrip('\\x00')\n            print(part_number)",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "enable_stats",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "def enable_stats():\n    global server\n    server.enable_stats()\n    return \"stats enabled\"\n@app.route('/stats/off')\ndef disable_stats():\n    global server\n    server.disable_stats()\n    return \"stats disabled\"\n@app.route('/text/<text>')",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "disable_stats",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "def disable_stats():\n    global server\n    server.disable_stats()\n    return \"stats disabled\"\n@app.route('/text/<text>')\ndef set_text(text):\n    global server\n    server.set_text(text)\n    return 'set text: \\n\\n%s' % text\nif __name__ == '__main__':",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "set_text",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "def set_text(text):\n    global server\n    server.set_text(text)\n    return 'set text: \\n\\n%s' % text\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port='5005', debug=True)",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "logger = logging.getLogger(__name__)\nI2C_EEPROM_BUS = [0, 1, 2, 7]\nMODULE_I2CBUS_TABLE = {\n    'p3767-0005': 7, #'NVIDIA Jetson Orin Nano (Developer kit)',\n    'p3767-0004': 7, #'NVIDIA Jetson Orin Nano (4GB ram)',\n    'p3767-0003': 7, #'NVIDIA Jetson Orin Nano (8GB ram)',\n    'p3767-0001': 7, #'NVIDIA Jetson Orin NX (8GB ram)',\n    'p3767-0000': 7, #'NVIDIA Jetson Orin NX (16GB ram)',\n    'p3701-0005': 7, #'NVIDIA Jetson AGX Orin (64GB ram)',\n    'p3701-0004': 7, #'NVIDIA Jetson AGX Orin (32GB ram)',",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "I2C_EEPROM_BUS",
        "kind": 5,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "I2C_EEPROM_BUS = [0, 1, 2, 7]\nMODULE_I2CBUS_TABLE = {\n    'p3767-0005': 7, #'NVIDIA Jetson Orin Nano (Developer kit)',\n    'p3767-0004': 7, #'NVIDIA Jetson Orin Nano (4GB ram)',\n    'p3767-0003': 7, #'NVIDIA Jetson Orin Nano (8GB ram)',\n    'p3767-0001': 7, #'NVIDIA Jetson Orin NX (8GB ram)',\n    'p3767-0000': 7, #'NVIDIA Jetson Orin NX (16GB ram)',\n    'p3701-0005': 7, #'NVIDIA Jetson AGX Orin (64GB ram)',\n    'p3701-0004': 7, #'NVIDIA Jetson AGX Orin (32GB ram)',\n    'p3701-0002': 7, #'NVIDIA Jetson IGX Orin (Developer kit)',",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "MODULE_I2CBUS_TABLE",
        "kind": 5,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "MODULE_I2CBUS_TABLE = {\n    'p3767-0005': 7, #'NVIDIA Jetson Orin Nano (Developer kit)',\n    'p3767-0004': 7, #'NVIDIA Jetson Orin Nano (4GB ram)',\n    'p3767-0003': 7, #'NVIDIA Jetson Orin Nano (8GB ram)',\n    'p3767-0001': 7, #'NVIDIA Jetson Orin NX (8GB ram)',\n    'p3767-0000': 7, #'NVIDIA Jetson Orin NX (16GB ram)',\n    'p3701-0005': 7, #'NVIDIA Jetson AGX Orin (64GB ram)',\n    'p3701-0004': 7, #'NVIDIA Jetson AGX Orin (32GB ram)',\n    'p3701-0002': 7, #'NVIDIA Jetson IGX Orin (Developer kit)',\n    'p3701-0000': 7, #'NVIDIA Jetson AGX Orin',",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "server = DisplayServer()\napp = Flask(__name__)\n@app.route('/stats/on')\ndef enable_stats():\n    global server\n    server.enable_stats()\n    return \"stats enabled\"\n@app.route('/stats/off')\ndef disable_stats():\n    global server",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "packages.hardware.oled.test.oled_server",
        "description": "packages.hardware.oled.test.oled_server",
        "peekOfCode": "app = Flask(__name__)\n@app.route('/stats/on')\ndef enable_stats():\n    global server\n    server.enable_stats()\n    return \"stats enabled\"\n@app.route('/stats/off')\ndef disable_stats():\n    global server\n    server.disable_stats()",
        "detail": "packages.hardware.oled.test.oled_server",
        "documentation": {}
    },
    {
        "label": "notebooks_dir",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def notebooks_dir():\n    return pkg_resources.resource_filename('jetbot', 'notebooks')\ndef platform_notebooks_dir():\n    if 'aarch64' in platform.machine():\n        return os.path.join(notebooks_dir(), 'robot')\n    else:\n        return os.path.join(notebooks_dir(), 'host')\ndef platform_model_str():\n    with open('/proc/device-tree/model', 'r') as f:\n        return str(f.read()[:-1])",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "platform_notebooks_dir",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def platform_notebooks_dir():\n    if 'aarch64' in platform.machine():\n        return os.path.join(notebooks_dir(), 'robot')\n    else:\n        return os.path.join(notebooks_dir(), 'host')\ndef platform_model_str():\n    with open('/proc/device-tree/model', 'r') as f:\n        return str(f.read()[:-1])\ndef platform_is_nano():\n    return 'jetson-nano' in platform_model_str()",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "platform_model_str",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def platform_model_str():\n    with open('/proc/device-tree/model', 'r') as f:\n        return str(f.read()[:-1])\ndef platform_is_nano():\n    return 'jetson-nano' in platform_model_str()\ndef ip_address(interface):\n    try:\n        if network_interface_state(interface) == 'down':\n            return None\n        cmd = \"hostname -I | cut -d' ' -f1\"",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "platform_is_nano",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def platform_is_nano():\n    return 'jetson-nano' in platform_model_str()\ndef ip_address(interface):\n    try:\n        if network_interface_state(interface) == 'down':\n            return None\n        cmd = \"hostname -I | cut -d' ' -f1\"\n        return subprocess.check_output(cmd, shell=True).decode('ascii')[:-1]\n    except:\n        return None",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "ip_address",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def ip_address(interface):\n    try:\n        if network_interface_state(interface) == 'down':\n            return None\n        cmd = \"hostname -I | cut -d' ' -f1\"\n        return subprocess.check_output(cmd, shell=True).decode('ascii')[:-1]\n    except:\n        return None\ndef network_interface_state(interface):\n    try:",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "network_interface_state",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def network_interface_state(interface):\n    try:\n        with open('/sys/class/net/%s/operstate' % interface, 'r') as f:\n            return f.read()\n    except:\n        return 'down' # default to down\ndef power_mode():\n    \"\"\"Gets the Jetson's current power mode\n    Gets the current power mode as set by the tool ``nvpmodel``.\n    Returns:",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "power_mode",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def power_mode():\n    \"\"\"Gets the Jetson's current power mode\n    Gets the current power mode as set by the tool ``nvpmodel``.\n    Returns:\n        str: The current power mode.  Either 'MAXN' or '5W'.\n    \"\"\"\n    #return subprocess.check_output(\"nvpmodel -q | grep -o '5W\\|MAXN'\", shell = True ).decode('utf-8').strip('\\n')\n    return \"Max-N\"\ndef power_usage():\n    \"\"\"Gets the Jetson's current power usage in Watts",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "power_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def power_usage():\n    \"\"\"Gets the Jetson's current power usage in Watts\n    Returns:\n        float: The current power usage in Watts.\n    \"\"\"\n    #with open(\"/sys/devices/50000000.host1x/546c0000.i2c/i2c-6/6-0040/iio:device0/in_power0_input\", 'r') as f:\n        #return float(f.read()) / 1000.0\n    return 0.0\ndef cpu_usage():\n    \"\"\"Gets the Jetson's current CPU usage fraction",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "cpu_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def cpu_usage():\n    \"\"\"Gets the Jetson's current CPU usage fraction\n    Returns:\n        float: The current CPU usage fraction.\n    \"\"\"\n    return float(subprocess.check_output(\"top -bn1 | grep load | awk '{printf \\\"%.2f\\\", $(NF-2)}'\", shell = True ).decode('utf-8'))\ndef gpu_usage():\n    \"\"\"Gets the Jetson's current GPU usage fraction\n    Returns:\n        float: The current GPU usage fraction.",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "gpu_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def gpu_usage():\n    \"\"\"Gets the Jetson's current GPU usage fraction\n    Returns:\n        float: The current GPU usage fraction.\n    \"\"\"\n    load = 1\n    gpu_path = \"/sys/class/devfreq/\"\n    extensionsToCheck = ('.gv11b', '.gp10b', '.ga10b', '.gpu')\n    for item in os.listdir(gpu_path):\n        if item.endswith(extensionsToCheck):",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def memory_usage():\n    \"\"\"Gets the Jetson's current RAM memory usage fraction\n    Returns:\n        float: The current RAM usage fraction.\n    \"\"\"\n    return float(subprocess.check_output(\"free -m | awk 'NR==2{printf \\\"%.2f\\\", $3*100/$2 }'\", shell = True ).decode('utf-8')) / 100.0\ndef disk_usage():\n    \"\"\"Gets the Jetson's current disk memory usage fraction\n    Returns:\n        float: The current disk usage fraction.",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "disk_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def disk_usage():\n    \"\"\"Gets the Jetson's current disk memory usage fraction\n    Returns:\n        float: The current disk usage fraction.\n    \"\"\"\n    return float(subprocess.check_output(\"df -h | awk '$NF==\\\"/\\\"{printf \\\"%s\\\", $5}'\", shell = True ).decode('utf-8').strip('%')) / 100.0\ndef cat(path):\n    with open(path, 'r') as f:\n        return f.readline().rstrip('\\x00')\ndef find_igpu(igpu_path):",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "cat",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def cat(path):\n    with open(path, 'r') as f:\n        return f.readline().rstrip('\\x00')\ndef find_igpu(igpu_path):\n    # Check if exist a integrated gpu\n    # if not os.path.exists(\"/dev/nvhost-gpu\") and not os.path.exists(\"/dev/nvhost-power-gpu\"):\n    #     return []\n    igpu = {}\n    if not os.path.isdir(igpu_path):\n        return igpu",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "find_igpu",
        "kind": 2,
        "importPath": "packages.hardware.oled.test.utils",
        "description": "packages.hardware.oled.test.utils",
        "peekOfCode": "def find_igpu(igpu_path):\n    # Check if exist a integrated gpu\n    # if not os.path.exists(\"/dev/nvhost-gpu\") and not os.path.exists(\"/dev/nvhost-power-gpu\"):\n    #     return []\n    igpu = {}\n    if not os.path.isdir(igpu_path):\n        return igpu\n    for item in os.listdir(igpu_path):\n        item_path = os.path.join(igpu_path, item)\n        if os.path.isfile(item_path) or os.path.islink(item_path):",
        "detail": "packages.hardware.oled.test.utils",
        "documentation": {}
    },
    {
        "label": "DisplayServer",
        "kind": 6,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "class DisplayServer(object):\n    def __init__(self, *args, **kwargs):\n        part_number, jetson_part_number = get_part_number()\n        i2c_bus_number = MODULE_I2CBUS_TABLE.get(jetson_part_number)\n        logger.info(f\"part_number: {part_number}, jetson_part_number: {jetson_part_number}\")\n        logger.info(f\"i2c_bus_number = {i2c_bus_number}\")\n        if not i2c_bus_number:\n            i2c_bus_number = 7  # Default: I2C bus 7 for Jetson AGX Orin\n        self.display = Adafruit_SSD1306.SSD1306_128_32(rst=None, i2c_bus=i2c_bus_number, gpio=1)\n        self.display.begin()",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "get_part_number",
        "kind": 2,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "def get_part_number():\n    part_number = ''\n    jetson_part_number = ''\n    # Find 699-level part number from EEPROM and extract P-number\n    for bus_number in I2C_EEPROM_BUS:\n        try:\n            bus = SMBus(bus_number)\n            part_number = bus.read_i2c_block_data(0x50, 20, 29)\n            part_number = ''.join(chr(i) for i in part_number).rstrip('\\x00')\n            # print(part_number)",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "enable_stats",
        "kind": 2,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "def enable_stats():\n    global server\n    server.enable_stats()\n    return \"stats enabled\"\n@app.route('/stats/off')\ndef disable_stats():\n    global server\n    server.disable_stats()\n    return \"stats disabled\"\n@app.route('/text/<text>')",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "disable_stats",
        "kind": 2,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "def disable_stats():\n    global server\n    server.disable_stats()\n    return \"stats disabled\"\n@app.route('/text/<text>')\ndef set_text(text):\n    global server\n    server.set_text(text)\n    return 'set text: \\n\\n%s' % text\nif __name__ == '__main__':",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "set_text",
        "kind": 2,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "def set_text(text):\n    global server\n    server.set_text(text)\n    return 'set text: \\n\\n%s' % text\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port='5005', debug=False)",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "logger = logging.getLogger(__name__)\nI2C_EEPROM_BUS = [0, 1, 2, 7]\nMODULE_I2CBUS_TABLE = {\n    'p3767-0005': 7, #'NVIDIA Jetson Orin Nano (Developer kit)',\n    'p3767-0004': 7, #'NVIDIA Jetson Orin Nano (4GB ram)',\n    'p3767-0003': 7, #'NVIDIA Jetson Orin Nano (8GB ram)',\n    'p3767-0001': 7, #'NVIDIA Jetson Orin NX (8GB ram)',\n    'p3767-0000': 7, #'NVIDIA Jetson Orin NX (16GB ram)',\n    'p3701-0005': 7, #'NVIDIA Jetson AGX Orin (64GB ram)',\n    'p3701-0004': 7, #'NVIDIA Jetson AGX Orin (32GB ram)',",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "I2C_EEPROM_BUS",
        "kind": 5,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "I2C_EEPROM_BUS = [0, 1, 2, 7]\nMODULE_I2CBUS_TABLE = {\n    'p3767-0005': 7, #'NVIDIA Jetson Orin Nano (Developer kit)',\n    'p3767-0004': 7, #'NVIDIA Jetson Orin Nano (4GB ram)',\n    'p3767-0003': 7, #'NVIDIA Jetson Orin Nano (8GB ram)',\n    'p3767-0001': 7, #'NVIDIA Jetson Orin NX (8GB ram)',\n    'p3767-0000': 7, #'NVIDIA Jetson Orin NX (16GB ram)',\n    'p3701-0005': 7, #'NVIDIA Jetson AGX Orin (64GB ram)',\n    'p3701-0004': 7, #'NVIDIA Jetson AGX Orin (32GB ram)',\n    'p3701-0002': 7, #'NVIDIA Jetson IGX Orin (Developer kit)',",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "MODULE_I2CBUS_TABLE",
        "kind": 5,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "MODULE_I2CBUS_TABLE = {\n    'p3767-0005': 7, #'NVIDIA Jetson Orin Nano (Developer kit)',\n    'p3767-0004': 7, #'NVIDIA Jetson Orin Nano (4GB ram)',\n    'p3767-0003': 7, #'NVIDIA Jetson Orin Nano (8GB ram)',\n    'p3767-0001': 7, #'NVIDIA Jetson Orin NX (8GB ram)',\n    'p3767-0000': 7, #'NVIDIA Jetson Orin NX (16GB ram)',\n    'p3701-0005': 7, #'NVIDIA Jetson AGX Orin (64GB ram)',\n    'p3701-0004': 7, #'NVIDIA Jetson AGX Orin (32GB ram)',\n    'p3701-0002': 7, #'NVIDIA Jetson IGX Orin (Developer kit)',\n    'p3701-0000': 7, #'NVIDIA Jetson AGX Orin',",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "server = DisplayServer()\napp = Flask(__name__)\n@app.route('/stats/on')\ndef enable_stats():\n    global server\n    server.enable_stats()\n    return \"stats enabled\"\n@app.route('/stats/off')\ndef disable_stats():\n    global server",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "packages.hardware.oled.oled_server",
        "description": "packages.hardware.oled.oled_server",
        "peekOfCode": "app = Flask(__name__)\n@app.route('/stats/on')\ndef enable_stats():\n    global server\n    server.enable_stats()\n    return \"stats enabled\"\n@app.route('/stats/off')\ndef disable_stats():\n    global server\n    server.disable_stats()",
        "detail": "packages.hardware.oled.oled_server",
        "documentation": {}
    },
    {
        "label": "notebooks_dir",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def notebooks_dir():\n    return pkg_resources.resource_filename('jetbot', 'notebooks')\ndef platform_notebooks_dir():\n    if 'aarch64' in platform.machine():\n        return os.path.join(notebooks_dir(), 'robot')\n    else:\n        return os.path.join(notebooks_dir(), 'host')\ndef platform_model_str():\n    with open('/proc/device-tree/model', 'r') as f:\n        return str(f.read()[:-1])",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "platform_notebooks_dir",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def platform_notebooks_dir():\n    if 'aarch64' in platform.machine():\n        return os.path.join(notebooks_dir(), 'robot')\n    else:\n        return os.path.join(notebooks_dir(), 'host')\ndef platform_model_str():\n    with open('/proc/device-tree/model', 'r') as f:\n        return str(f.read()[:-1])\ndef platform_is_nano():\n    return 'jetson-nano' in platform_model_str()",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "platform_model_str",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def platform_model_str():\n    with open('/proc/device-tree/model', 'r') as f:\n        return str(f.read()[:-1])\ndef platform_is_nano():\n    return 'jetson-nano' in platform_model_str()\ndef ip_address(interface):\n    try:\n        if network_interface_state(interface) == 'down':\n            return None\n        cmd = \"hostname -I | cut -d' ' -f1\"",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "platform_is_nano",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def platform_is_nano():\n    return 'jetson-nano' in platform_model_str()\ndef ip_address(interface):\n    try:\n        if network_interface_state(interface) == 'down':\n            return None\n        cmd = \"hostname -I | cut -d' ' -f1\"\n        return subprocess.check_output(cmd, shell=True).decode('ascii')[:-1]\n    except:\n        return None",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "ip_address",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def ip_address(interface):\n    try:\n        if network_interface_state(interface) == 'down':\n            return None\n        cmd = \"hostname -I | cut -d' ' -f1\"\n        return subprocess.check_output(cmd, shell=True).decode('ascii')[:-1]\n    except:\n        return None\ndef network_interface_state(interface):\n    try:",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "network_interface_state",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def network_interface_state(interface):\n    try:\n        with open('/sys/class/net/%s/operstate' % interface, 'r') as f:\n            return f.read()\n    except:\n        return 'down' # default to down\ndef power_mode():\n    \"\"\"Gets the Jetson's current power mode\n    Gets the current power mode as set by the tool ``nvpmodel``.\n    Returns:",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "power_mode",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def power_mode():\n    \"\"\"Gets the Jetson's current power mode\n    Gets the current power mode as set by the tool ``nvpmodel``.\n    Returns:\n        str: The current power mode.  Either 'MAXN' or '5W'.\n    \"\"\"\n    #return subprocess.check_output(\"nvpmodel -q | grep -o '5W\\|MAXN'\", shell = True ).decode('utf-8').strip('\\n')\n    return \"Max-N\"\ndef power_usage():\n    \"\"\"Gets the Jetson's current power usage in Watts",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "power_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def power_usage():\n    \"\"\"Gets the Jetson's current power usage in Watts\n    Returns:\n        float: The current power usage in Watts.\n    \"\"\"\n    #with open(\"/sys/devices/50000000.host1x/546c0000.i2c/i2c-6/6-0040/iio:device0/in_power0_input\", 'r') as f:\n        #return float(f.read()) / 1000.0\n    return 0.0\ndef cpu_usage():\n    \"\"\"Gets the Jetson's current CPU usage fraction",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "cpu_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def cpu_usage():\n    \"\"\"Gets the Jetson's current CPU usage fraction\n    Returns:\n        float: The current CPU usage fraction.\n    \"\"\"\n    return float(subprocess.check_output(\"top -bn1 | grep load | awk '{printf \\\"%.2f\\\", $(NF-2)}'\", shell = True ).decode('utf-8'))\ndef gpu_usage():\n    \"\"\"Gets the Jetson's current GPU usage fraction\n    Returns:\n        float: The current GPU usage fraction.",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "gpu_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def gpu_usage():\n    \"\"\"Gets the Jetson's current GPU usage fraction\n    Returns:\n        float: The current GPU usage fraction.\n    \"\"\"\n    load = 1\n    gpu_path = \"/sys/class/devfreq/\"\n    extensionsToCheck = ('.gv11b', '.gp10b', '.ga10b', '.gpu')\n    for item in os.listdir(gpu_path):\n        if item.endswith(extensionsToCheck):",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def memory_usage():\n    \"\"\"Gets the Jetson's current RAM memory usage fraction\n    Returns:\n        float: The current RAM usage fraction.\n    \"\"\"\n    return float(subprocess.check_output(\"free -m | awk 'NR==2{printf \\\"%.2f\\\", $3*100/$2 }'\", shell = True ).decode('utf-8')) / 100.0\ndef disk_usage():\n    \"\"\"Gets the Jetson's current disk memory usage fraction\n    Returns:\n        float: The current disk usage fraction.",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "disk_usage",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def disk_usage():\n    \"\"\"Gets the Jetson's current disk memory usage fraction\n    Returns:\n        float: The current disk usage fraction.\n    \"\"\"\n    return float(subprocess.check_output(\"df -h | awk '$NF==\\\"/\\\"{printf \\\"%s\\\", $5}'\", shell = True ).decode('utf-8').strip('%')) / 100.0\ndef cat(path):\n    with open(path, 'r') as f:\n        return f.readline().rstrip('\\x00')\ndef find_igpu(igpu_path):",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "cat",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def cat(path):\n    with open(path, 'r') as f:\n        return f.readline().rstrip('\\x00')\ndef find_igpu(igpu_path):\n    # Check if exist a integrated gpu\n    # if not os.path.exists(\"/dev/nvhost-gpu\") and not os.path.exists(\"/dev/nvhost-power-gpu\"):\n    #     return []\n    igpu = {}\n    if not os.path.isdir(igpu_path):\n        return igpu",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "find_igpu",
        "kind": 2,
        "importPath": "packages.hardware.oled.utils",
        "description": "packages.hardware.oled.utils",
        "peekOfCode": "def find_igpu(igpu_path):\n    # Check if exist a integrated gpu\n    # if not os.path.exists(\"/dev/nvhost-gpu\") and not os.path.exists(\"/dev/nvhost-power-gpu\"):\n    #     return []\n    igpu = {}\n    if not os.path.isdir(igpu_path):\n        return igpu\n    for item in os.listdir(igpu_path):\n        item_path = os.path.join(igpu_path, item)\n        if os.path.isfile(item_path) or os.path.islink(item_path):",
        "detail": "packages.hardware.oled.utils",
        "documentation": {}
    },
    {
        "label": "latest_zed_version",
        "kind": 5,
        "importPath": "packages.hardware.zed.config",
        "description": "packages.hardware.zed.config",
        "peekOfCode": "latest_zed_version = Version('35.3.1')\nif L4T_VERSION > latest_zed_version:\n    L4T_VERSION = latest_zed_version\npackage['build_args'] = {\n    'L4T_MAJOR_VERSION': L4T_VERSION.major,\n    'L4T_MINOR_VERSION': L4T_VERSION.minor,\n    'L4T_PATCH_VERSION': L4T_VERSION.micro,\n    'ZED_SDK_MAJOR': 4,\n    'ZED_SDK_MINOR': 0,\n}",
        "detail": "packages.hardware.zed.config",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.hardware.zed.config",
        "description": "packages.hardware.zed.config",
        "peekOfCode": "package['build_args'] = {\n    'L4T_MAJOR_VERSION': L4T_VERSION.major,\n    'L4T_MINOR_VERSION': L4T_VERSION.minor,\n    'L4T_PATCH_VERSION': L4T_VERSION.micro,\n    'ZED_SDK_MAJOR': 4,\n    'ZED_SDK_MINOR': 0,\n}",
        "detail": "packages.hardware.zed.config",
        "documentation": {}
    },
    {
        "label": "package['name']",
        "kind": 5,
        "importPath": "packages.jetson-inference.config",
        "description": "packages.jetson-inference.config",
        "peekOfCode": "package['name'] = 'jetson-inference:main'\npackage['alias'] = 'jetson-inference'\npackage = [package]\nfor distro in ROS2_DISTROS:\n    ros = package[0].copy()\n    ros['name'] = f'jetson-inference:{distro}'\n    ros['depends'] = [f'ros:{distro}-foxglove', 'jetson-inference:main']\n    ros['dockerfile'] = 'Dockerfile.ros'\n    ros['test'] = ros['test'] + ['test_ros.sh']\n    del ros['alias']",
        "detail": "packages.jetson-inference.config",
        "documentation": {}
    },
    {
        "label": "package['alias']",
        "kind": 5,
        "importPath": "packages.jetson-inference.config",
        "description": "packages.jetson-inference.config",
        "peekOfCode": "package['alias'] = 'jetson-inference'\npackage = [package]\nfor distro in ROS2_DISTROS:\n    ros = package[0].copy()\n    ros['name'] = f'jetson-inference:{distro}'\n    ros['depends'] = [f'ros:{distro}-foxglove', 'jetson-inference:main']\n    ros['dockerfile'] = 'Dockerfile.ros'\n    ros['test'] = ros['test'] + ['test_ros.sh']\n    del ros['alias']\n    package.append(ros)",
        "detail": "packages.jetson-inference.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.jetson-inference.config",
        "description": "packages.jetson-inference.config",
        "peekOfCode": "package = [package]\nfor distro in ROS2_DISTROS:\n    ros = package[0].copy()\n    ros['name'] = f'jetson-inference:{distro}'\n    ros['depends'] = [f'ros:{distro}-foxglove', 'jetson-inference:main']\n    ros['dockerfile'] = 'Dockerfile.ros'\n    ros['test'] = ros['test'] + ['test_ros.sh']\n    del ros['alias']\n    package.append(ros)",
        "detail": "packages.jetson-inference.config",
        "documentation": {}
    },
    {
        "label": "myst",
        "kind": 5,
        "importPath": "packages.jupyterlab.config",
        "description": "packages.jupyterlab.config",
        "peekOfCode": "myst = package.copy()\nmyst['name'] = 'jupyterlab:myst'\nmyst['dockerfile'] = 'Dockerfile.myst'\nmyst['depends'] = ['jupyterlab:main']\ndel myst['alias']\npackage = [package, myst]",
        "detail": "packages.jupyterlab.config",
        "documentation": {}
    },
    {
        "label": "myst['name']",
        "kind": 5,
        "importPath": "packages.jupyterlab.config",
        "description": "packages.jupyterlab.config",
        "peekOfCode": "myst['name'] = 'jupyterlab:myst'\nmyst['dockerfile'] = 'Dockerfile.myst'\nmyst['depends'] = ['jupyterlab:main']\ndel myst['alias']\npackage = [package, myst]",
        "detail": "packages.jupyterlab.config",
        "documentation": {}
    },
    {
        "label": "myst['dockerfile']",
        "kind": 5,
        "importPath": "packages.jupyterlab.config",
        "description": "packages.jupyterlab.config",
        "peekOfCode": "myst['dockerfile'] = 'Dockerfile.myst'\nmyst['depends'] = ['jupyterlab:main']\ndel myst['alias']\npackage = [package, myst]",
        "detail": "packages.jupyterlab.config",
        "documentation": {}
    },
    {
        "label": "myst['depends']",
        "kind": 5,
        "importPath": "packages.jupyterlab.config",
        "description": "packages.jupyterlab.config",
        "peekOfCode": "myst['depends'] = ['jupyterlab:main']\ndel myst['alias']\npackage = [package, myst]",
        "detail": "packages.jupyterlab.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.jupyterlab.config",
        "description": "packages.jupyterlab.config",
        "peekOfCode": "package = [package, myst]",
        "detail": "packages.jupyterlab.config",
        "documentation": {}
    },
    {
        "label": "AutoAWQ",
        "kind": 2,
        "importPath": "packages.llm.auto_awq.config",
        "description": "packages.llm.auto_awq.config",
        "peekOfCode": "def AutoAWQ(version, kernels_version, default=False):\n    pkg = package.copy()\n    pkg['name'] = f'auto_awq:{version}'\n    if default:\n        pkg['alias'] = 'auto_awq'\n    pkg['build_args'] = {\n        'AUTOAWQ_VERSION': version,\n        'AUTOAWQ_KERNELS_VERSION': kernels_version,\n        'AUTOAWQ_CUDA_ARCH': ','.join([str(x) for x in CUDA_ARCHITECTURES])\n    }",
        "detail": "packages.llm.auto_awq.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.auto_awq.config",
        "description": "packages.llm.auto_awq.config",
        "peekOfCode": "package = [\n    AutoAWQ('0.2.4', '0.0.6', default=True),\n]",
        "detail": "packages.llm.auto_awq.config",
        "documentation": {}
    },
    {
        "label": "AutoGPTQ",
        "kind": 2,
        "importPath": "packages.llm.auto_gptq.config",
        "description": "packages.llm.auto_gptq.config",
        "peekOfCode": "def AutoGPTQ(version, branch=None, default=False):\n    pkg = package.copy()\n    pkg['name'] = f'auto_gptq:{version}'\n    if default:\n        pkg['alias'] = 'auto_gptq'\n    if not branch:\n        branch = version\n    pkg['build_args'] = {\n        'AUTOGPTQ_VERSION': version,\n        'AUTOGPTQ_BRANCH': branch,",
        "detail": "packages.llm.auto_gptq.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.auto_gptq.config",
        "description": "packages.llm.auto_gptq.config",
        "peekOfCode": "package = [\n    AutoGPTQ('0.7.1', default=True),\n]",
        "detail": "packages.llm.auto_gptq.config",
        "documentation": {}
    },
    {
        "label": "load_prompts",
        "kind": 2,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "def load_prompts(prompts):\n    \"\"\"\n    Load prompts from a list of txt or json files\n    (or if these are strings, just return the strings)\n    \"\"\"\n    prompt_list = []\n    for prompt in prompts:\n        ext = os.path.splitext(prompt)[1]\n        if ext == '.json':\n            with open(prompt) as file:",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_path",
        "kind": 2,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "def get_model_name_from_path(model_path):\n    model_path = model_path.strip(\"/\")\n    model_paths = model_path.split(\"/\")\n    if len(model_paths) > 2:\n        return model_paths[-2] + '/' + model_paths[-1]\n    else:\n        return model_paths[-1]\nmodel_name = get_model_name_from_path(args.quant)\n# get quantization config (apart from w_bit)\nq_config = {",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default='', required=True, help=\"name or path of the huggingface model\")\nparser.add_argument('--quant', type=str, default='', required=True, help=\"path to the real AWQ quantized model checkpoint\")\nparser.add_argument(\"--prompt\", action='append', nargs='*')\n# benchmarking options\nparser.add_argument(\"--max-new-tokens\", type=int, default=128)\nparser.add_argument(\"--max-num-prompts\", type=int, default=None)\nparser.add_argument('--save', type=str, default='', help='CSV file to save benchmarking results to')\n# quantization options\nparser.add_argument('--w_bit', type=int, default=4)",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "args = parser.parse_args()\nif not args.prompt:\n    if args.chat:  # https://modal.com/docs/guide/ex/vllm_inference\n        args.prompt = [\n            \"What is the meaning of life?\",\n            \"How many points did you list out?\",\n            \"What is the weather forecast today?\",\n            \"What is the fable involving a fox and grapes?\",\n            \"What's a good recipe for making tabouli?\",\n            \"What is the product of 9 and 8?\",",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "args.prompt",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "args.prompt = load_prompts(args.prompt)\nif args.max_num_prompts:\n    args.prompt = args.prompt[:args.max_num_prompts]\ndef get_model_name_from_path(model_path):\n    model_path = model_path.strip(\"/\")\n    model_paths = model_path.split(\"/\")\n    if len(model_paths) > 2:\n        return model_paths[-2] + '/' + model_paths[-1]\n    else:\n        return model_paths[-1]",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "model_name = get_model_name_from_path(args.quant)\n# get quantization config (apart from w_bit)\nq_config = {\n    \"zero_point\": not args.no_zero_point,  # by default True\n    \"q_group_size\": args.q_group_size,  # whether to use group quantization\n}\nprint(\"Quantization config:\", q_config)\n# select compute device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Running on device {device}\")",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "q_config",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "q_config = {\n    \"zero_point\": not args.no_zero_point,  # by default True\n    \"q_group_size\": args.q_group_size,  # whether to use group quantization\n}\nprint(\"Quantization config:\", q_config)\n# select compute device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Running on device {device}\")\nif args.no_quant:\n    precision = \"fp16\"",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Running on device {device}\")\nif args.no_quant:\n    precision = \"fp16\"\n    print(f\"Loading model {args.model} without quantization ({precision})\")\n    model = AutoModelForCausalLM.from_pretrained(args.model, torch_dtype=torch.float16).to(device)\nelse:\n    print(f\"Loading model {args.model} with quantized weights from {args.quant}\")\n    with init_empty_weights():\n        model = AutoModelForCausalLM.from_pretrained(args.model, torch_dtype=torch.float16)",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)\n# benchmark inference\navg_prefill_time = 0\navg_prefill_rate = 0\navg_decode_time = 0\navg_decode_rate = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        prompt = prompt['text']\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_prefill_time",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "avg_prefill_time = 0\navg_prefill_rate = 0\navg_decode_time = 0\navg_decode_rate = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        prompt = prompt['text']\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    num_input_tokens = input_ids.shape[-1]\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_prefill_rate",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "avg_prefill_rate = 0\navg_decode_time = 0\navg_decode_rate = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        prompt = prompt['text']\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    num_input_tokens = input_ids.shape[-1]\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n    time_begin = 0",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_decode_time",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "avg_decode_time = 0\navg_decode_rate = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        prompt = prompt['text']\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    num_input_tokens = input_ids.shape[-1]\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n    time_begin = 0\n    def generate():",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_decode_rate",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "avg_decode_rate = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        prompt = prompt['text']\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    num_input_tokens = input_ids.shape[-1]\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n    time_begin = 0\n    def generate():\n        global time_begin",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "kind": 5,
        "importPath": "packages.llm.awq.benchmark",
        "description": "packages.llm.awq.benchmark",
        "peekOfCode": "memory_usage = (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  # https://stackoverflow.com/a/7669482\nprint(f\"Peak memory usage:  {memory_usage:.2f} MB\")\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, api, model, precision, input_tokens, output_tokens, prefill_time, prefill_rate, decode_time, decode_rate, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, {'tinychat' if not args.no_tinychat_kernels else 'awq'}, \")\n        file.write(f\"{model_name}, {precision}, {num_input_tokens}, {args.max_new_tokens}, \")\n        file.write(f\"{avg_prefill_time}, {avg_prefill_rate}, {avg_decode_time}, {avg_decode_rate}, {memory_usage}\\n\")",
        "detail": "packages.llm.awq.benchmark",
        "documentation": {}
    },
    {
        "label": "awq",
        "kind": 2,
        "importPath": "packages.llm.awq.config",
        "description": "packages.llm.awq.config",
        "peekOfCode": "def awq(version, kernels=None, branch=None, default=False):\n    pkg = package.copy()\n    pkg['name'] = f'awq:{version}'\n    if not branch:\n        branch = version\n    pkg['build_args'] = {\n        'AWQ_REPO': 'mit-han-lab/llm-awq',\n        'AWQ_BRANCH': branch,\n        'AWQ_VERSION': version,\n        'AWQ_KERNEL_VERSION': kernels,",
        "detail": "packages.llm.awq.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.awq.config",
        "description": "packages.llm.awq.config",
        "peekOfCode": "package = [\n    awq('0.1.0', kernels='0.0.0', branch='main', default=True),\n]",
        "detail": "packages.llm.awq.config",
        "documentation": {}
    },
    {
        "label": "run_cmd",
        "kind": 2,
        "importPath": "packages.llm.awq.quantize",
        "description": "packages.llm.awq.quantize",
        "peekOfCode": "def run_cmd(cmd):\n    print(\"\\nRunning command:\\n\")\n    print(cmd, \"\\n\")\n    if not args.simulate:\n        subprocess.run(cmd, executable='/bin/bash', shell=True, check=True) \ncmd_prefix = f\"python3 -m awq.entry --model_path {args.model} --w_bit {args.w_bit} --q_group_size {args.q_group_size}\"\n# Perform AWQ search\nif not args.load_awq:\n    run_cmd(f\"{cmd_prefix} --run_awq --dump_awq {model_search}\")\n# Evaluate the AWQ quantized model on WikiText-2 (simulated pseudo quantization)",
        "detail": "packages.llm.awq.quantize",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.awq.quantize",
        "description": "packages.llm.awq.quantize",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, required=True, help=\"model name or path to model directory (should be a .pt model)\")\nparser.add_argument('--output', type=str, default='', help=\"directory to output the quantized model to (default is /data/models/awq/$MODEL)\")\nparser.add_argument('--load_awq', type=str, default='', help=\"load a model that's already had AWQ search performed on it (i.e. from the AWQ Model Zoo), and skip the search step\")\nparser.add_argument('--w_bit', type=int, default=4, choices=[3,4], help=\"the number of bits (3 or 4)\")\nparser.add_argument('--q_group_size', type=int, default=128, help=\"the group size (default 128)\")\nparser.add_argument('--no_cache', action='store_true', help=\"dump the quantized AWQ weights even if the file already exists\")\nparser.add_argument('--skip_eval', action='store_true', help=\"evaluate the real quantized model on wikitext\")\nparser.add_argument('--simulate', action='store_true', help=\"print out the commands without actually running them\")\nargs = parser.parse_args()",
        "detail": "packages.llm.awq.quantize",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.awq.quantize",
        "description": "packages.llm.awq.quantize",
        "peekOfCode": "args = parser.parse_args()\nif not args.output:\n    args.output = f\"/data/models/awq/{os.path.basename(args.model)}\"\nprint(args)\nos.makedirs(args.output, exist_ok=True)\nprefix_awq = f\"w{args.w_bit}-g{args.q_group_size}\"\nmodel_search = os.path.join(args.output, f\"{prefix_awq}.pt\") if not args.load_awq else args.load_awq\nmodel_quant = os.path.join(args.output, f\"{prefix_awq}-awq-v2.pt\")\ndef run_cmd(cmd):\n    print(\"\\nRunning command:\\n\")",
        "detail": "packages.llm.awq.quantize",
        "documentation": {}
    },
    {
        "label": "prefix_awq",
        "kind": 5,
        "importPath": "packages.llm.awq.quantize",
        "description": "packages.llm.awq.quantize",
        "peekOfCode": "prefix_awq = f\"w{args.w_bit}-g{args.q_group_size}\"\nmodel_search = os.path.join(args.output, f\"{prefix_awq}.pt\") if not args.load_awq else args.load_awq\nmodel_quant = os.path.join(args.output, f\"{prefix_awq}-awq-v2.pt\")\ndef run_cmd(cmd):\n    print(\"\\nRunning command:\\n\")\n    print(cmd, \"\\n\")\n    if not args.simulate:\n        subprocess.run(cmd, executable='/bin/bash', shell=True, check=True) \ncmd_prefix = f\"python3 -m awq.entry --model_path {args.model} --w_bit {args.w_bit} --q_group_size {args.q_group_size}\"\n# Perform AWQ search",
        "detail": "packages.llm.awq.quantize",
        "documentation": {}
    },
    {
        "label": "model_search",
        "kind": 5,
        "importPath": "packages.llm.awq.quantize",
        "description": "packages.llm.awq.quantize",
        "peekOfCode": "model_search = os.path.join(args.output, f\"{prefix_awq}.pt\") if not args.load_awq else args.load_awq\nmodel_quant = os.path.join(args.output, f\"{prefix_awq}-awq-v2.pt\")\ndef run_cmd(cmd):\n    print(\"\\nRunning command:\\n\")\n    print(cmd, \"\\n\")\n    if not args.simulate:\n        subprocess.run(cmd, executable='/bin/bash', shell=True, check=True) \ncmd_prefix = f\"python3 -m awq.entry --model_path {args.model} --w_bit {args.w_bit} --q_group_size {args.q_group_size}\"\n# Perform AWQ search\nif not args.load_awq:",
        "detail": "packages.llm.awq.quantize",
        "documentation": {}
    },
    {
        "label": "model_quant",
        "kind": 5,
        "importPath": "packages.llm.awq.quantize",
        "description": "packages.llm.awq.quantize",
        "peekOfCode": "model_quant = os.path.join(args.output, f\"{prefix_awq}-awq-v2.pt\")\ndef run_cmd(cmd):\n    print(\"\\nRunning command:\\n\")\n    print(cmd, \"\\n\")\n    if not args.simulate:\n        subprocess.run(cmd, executable='/bin/bash', shell=True, check=True) \ncmd_prefix = f\"python3 -m awq.entry --model_path {args.model} --w_bit {args.w_bit} --q_group_size {args.q_group_size}\"\n# Perform AWQ search\nif not args.load_awq:\n    run_cmd(f\"{cmd_prefix} --run_awq --dump_awq {model_search}\")",
        "detail": "packages.llm.awq.quantize",
        "documentation": {}
    },
    {
        "label": "cmd_prefix",
        "kind": 5,
        "importPath": "packages.llm.awq.quantize",
        "description": "packages.llm.awq.quantize",
        "peekOfCode": "cmd_prefix = f\"python3 -m awq.entry --model_path {args.model} --w_bit {args.w_bit} --q_group_size {args.q_group_size}\"\n# Perform AWQ search\nif not args.load_awq:\n    run_cmd(f\"{cmd_prefix} --run_awq --dump_awq {model_search}\")\n# Evaluate the AWQ quantized model on WikiText-2 (simulated pseudo quantization)\nif not args.load_awq and not args.skip_eval:\n    run_cmd(f\"{cmd_prefix} --tasks wikitext --load_awq {model_search} --q_backend fake\")\n# Generate real quantized weights (INT4)\nif args.no_cache or not os.path.isfile(model_quant):\n    run_cmd(f\"{cmd_prefix} --load_awq {model_search} --q_backend real --dump_quant {model_quant}\")",
        "detail": "packages.llm.awq.quantize",
        "documentation": {}
    },
    {
        "label": "bitsandbytes",
        "kind": 2,
        "importPath": "packages.llm.bitsandbytes.config",
        "description": "packages.llm.bitsandbytes.config",
        "peekOfCode": "def bitsandbytes(version, requires=None, default=False):\n    pkg = package.copy()\n    if requires:\n        pkg['requires'] = requires   \n    pkg['name'] = f'bitsandbytes:{version}'\n    pkg['build_args'] = {\n        'BITSANDBYTES_VERSION': version,\n        'BITSANDBYTES_REPO': \"dusty-nv/bitsandbytes\",\n        'BITSANDBYTES_BRANCH': \"main\",\n        'CUDA_INSTALLED_VERSION': int(str(CUDA_VERSION.major) + str(CUDA_VERSION.minor)),",
        "detail": "packages.llm.bitsandbytes.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.bitsandbytes.config",
        "description": "packages.llm.bitsandbytes.config",
        "peekOfCode": "package = [\n    bitsandbytes('0.39.1', default=True),\n]\n# from jetson_containers import CUDA_VERSION, find_container\n# builder = package.copy()\n# runtime = package.copy()\n# builder['name'] = 'bitsandbytes:builder'\n# builder['dockerfile'] = 'Dockerfile.builder'\n# builder['build_args'] = {\n#     'BITSANDBYTES_REPO': \"dusty-nv/bitsandbytes\",",
        "detail": "packages.llm.bitsandbytes.config",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.bitsandbytes.test",
        "description": "packages.llm.bitsandbytes.test",
        "peekOfCode": "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='cuda', load_in_8bit=True, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nstreamer = TextIteratorStreamer(tokenizer)\nprompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=64, streamer=streamer)).start()",
        "detail": "packages.llm.bitsandbytes.test",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "packages.llm.bitsandbytes.test",
        "description": "packages.llm.bitsandbytes.test",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(model_name)\nstreamer = TextIteratorStreamer(tokenizer)\nprompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=64, streamer=streamer)).start()\nfor text in streamer:",
        "detail": "packages.llm.bitsandbytes.test",
        "documentation": {}
    },
    {
        "label": "streamer",
        "kind": 5,
        "importPath": "packages.llm.bitsandbytes.test",
        "description": "packages.llm.bitsandbytes.test",
        "peekOfCode": "streamer = TextIteratorStreamer(tokenizer)\nprompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=64, streamer=streamer)).start()\nfor text in streamer:\n    print(text, end='', flush=True)",
        "detail": "packages.llm.bitsandbytes.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "packages.llm.bitsandbytes.test",
        "description": "packages.llm.bitsandbytes.test",
        "peekOfCode": "prompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=64, streamer=streamer)).start()\nfor text in streamer:\n    print(text, end='', flush=True)\nprint(f'\\n\\ndone testing bitsandbytes with {model_name}')",
        "detail": "packages.llm.bitsandbytes.test",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "packages.llm.bitsandbytes.test",
        "description": "packages.llm.bitsandbytes.test",
        "peekOfCode": "inputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=64, streamer=streamer)).start()\nfor text in streamer:\n    print(text, end='', flush=True)\nprint(f'\\n\\ndone testing bitsandbytes with {model_name}')",
        "detail": "packages.llm.bitsandbytes.test",
        "documentation": {}
    },
    {
        "label": "exllama",
        "kind": 2,
        "importPath": "packages.llm.exllama.config",
        "description": "packages.llm.exllama.config",
        "peekOfCode": "def exllama(version, branch=None, requires=None, default=False):\n    pkg = package.copy()\n    pkg['name'] = f'exllama:{version}'\n    if requires:\n        pkg['requires'] = requires\n    if default:\n        pkg['alias'] = 'exllama'\n    if not branch:\n        branch = version\n    pkg['build_args'] = {",
        "detail": "packages.llm.exllama.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.exllama.config",
        "description": "packages.llm.exllama.config",
        "peekOfCode": "package = [\n    exllama('0.0.14', requires='==35.*', default=True),\n    exllama('0.0.15', requires='>=36', default=True),\n    #exllama('0.0.16', requires=['>=36', '>=cu124']),\n]",
        "detail": "packages.llm.exllama.config",
        "documentation": {}
    },
    {
        "label": "flash_attn",
        "kind": 2,
        "importPath": "packages.llm.flash-attention.config",
        "description": "packages.llm.flash-attention.config",
        "peekOfCode": "def flash_attn(version, requires=None, default=False):\n    pkg = package.copy()\n    if requires:\n        pkg['requires'] = requires   \n    pkg['name'] = f'flash-attention:{version}'\n    pkg['build_args'] = {\n        'FLASH_ATTENTION_VERSION': version,\n    }\n    builder = pkg.copy()\n    builder['name'] = f'flash-attention:{version}-builder'",
        "detail": "packages.llm.flash-attention.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.flash-attention.config",
        "description": "packages.llm.flash-attention.config",
        "peekOfCode": "package = [\n    flash_attn('2.5.6', default=False),\n    flash_attn('2.5.7', default=True),\n]",
        "detail": "packages.llm.flash-attention.config",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.huggingface_hub.huggingface-downloader",
        "description": "packages.llm.huggingface_hub.huggingface-downloader",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('repos', type=str, nargs='+', default=[], help=\"HuggingFace model, datasets, or file names to download\")\nparser.add_argument('--type', type=str, default='model', choices=['model', 'dataset'], help=\"'model' or 'dataset'\")\nparser.add_argument('--token', type=str, default=os.environ.get('HUGGINGFACE_TOKEN', ''), help=\"HuggingFace account login token from https://huggingface.co/docs/hub/security-tokens (defaults to $HUGGINGFACE_TOKEN)\")\nparser.add_argument('--cache-dir', type=str, default=os.environ.get('TRANSFORMERS_CACHE', '/root/.cache/huggingface'), help=\"Location to download the repo to (defaults to $TRANSFORMERS_CACHE)\")\nparser.add_argument('--location-file', type=str, default='/tmp/hf_download', help=\"file to write the local location/path of the downloaded repo(s) to\")\nparser.add_argument('--allow-patterns', type=str, default='', help=\"comma-separated list of file patterns to download (enclose in single quotes if using wildcards)\")\nparser.add_argument('--ignore-patterns', type=str, default='', help=\"comma-separated list of file patterns to exclude from downloading (enclose in single quotes if using wildcards)\")\nparser.add_argument('--skip-safetensors', action='store_true', help=\"filter out the downloading of .safetensor files\")\nargs = parser.parse_args()",
        "detail": "packages.llm.huggingface_hub.huggingface-downloader",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.huggingface_hub.huggingface-downloader",
        "description": "packages.llm.huggingface_hub.huggingface-downloader",
        "peekOfCode": "args = parser.parse_args()\nargs.allow_patterns = [x for x in args.allow_patterns.split(',') if x]\nargs.ignore_patterns = [x for x in args.ignore_patterns.split(',') if x]\nif args.skip_safetensors:\n    args.ignore_patterns.append('*.safetensors')\nif len(args.allow_patterns) == 0:\n    args.allow_patterns = None\nif len(args.ignore_patterns) == 0:\n    args.ignore_patterns = None\nif args.token:",
        "detail": "packages.llm.huggingface_hub.huggingface-downloader",
        "documentation": {}
    },
    {
        "label": "args.allow_patterns",
        "kind": 5,
        "importPath": "packages.llm.huggingface_hub.huggingface-downloader",
        "description": "packages.llm.huggingface_hub.huggingface-downloader",
        "peekOfCode": "args.allow_patterns = [x for x in args.allow_patterns.split(',') if x]\nargs.ignore_patterns = [x for x in args.ignore_patterns.split(',') if x]\nif args.skip_safetensors:\n    args.ignore_patterns.append('*.safetensors')\nif len(args.allow_patterns) == 0:\n    args.allow_patterns = None\nif len(args.ignore_patterns) == 0:\n    args.ignore_patterns = None\nif args.token:\n    print(\"Logging into HuggingFace Hub...\")",
        "detail": "packages.llm.huggingface_hub.huggingface-downloader",
        "documentation": {}
    },
    {
        "label": "args.ignore_patterns",
        "kind": 5,
        "importPath": "packages.llm.huggingface_hub.huggingface-downloader",
        "description": "packages.llm.huggingface_hub.huggingface-downloader",
        "peekOfCode": "args.ignore_patterns = [x for x in args.ignore_patterns.split(',') if x]\nif args.skip_safetensors:\n    args.ignore_patterns.append('*.safetensors')\nif len(args.allow_patterns) == 0:\n    args.allow_patterns = None\nif len(args.ignore_patterns) == 0:\n    args.ignore_patterns = None\nif args.token:\n    print(\"Logging into HuggingFace Hub...\")\n    login(token=args.token)",
        "detail": "packages.llm.huggingface_hub.huggingface-downloader",
        "documentation": {}
    },
    {
        "label": "locations",
        "kind": 5,
        "importPath": "packages.llm.huggingface_hub.huggingface-downloader",
        "description": "packages.llm.huggingface_hub.huggingface-downloader",
        "peekOfCode": "locations = []\nfor repo in args.repos:\n    if os.path.isdir(repo) or os.path.isfile(repo):\n        print(f\"\\nPath to local directory or file given: {repo}\")\n        locations.append(repo)\n        continue\n    print(f\"\\nDownloading {repo} to {args.cache_dir}\\n\")\n    # handle either \"org/repo\" or individual \"org/repo/file\"\n    # the former has 0-1 slashes, while the later has 2.\n    num_slashes = 0",
        "detail": "packages.llm.huggingface_hub.huggingface-downloader",
        "documentation": {}
    },
    {
        "label": "execute_command",
        "kind": 2,
        "importPath": "packages.llm.llama-factory.test",
        "description": "packages.llm.llama-factory.test",
        "peekOfCode": "def execute_command(command):\n    try:\n        stream = os.popen(command)\n        output = stream.read()\n        print(\"Output:\\n\", output)\n    except Exception as e:\n        print(\"Error:\\n\", str(e))\ncommand = \"llamafactory-cli version\"\nexecute_command(command)",
        "detail": "packages.llm.llama-factory.test",
        "documentation": {}
    },
    {
        "label": "command",
        "kind": 5,
        "importPath": "packages.llm.llama-factory.test",
        "description": "packages.llm.llama-factory.test",
        "peekOfCode": "command = \"llamafactory-cli version\"\nexecute_command(command)",
        "detail": "packages.llm.llama-factory.test",
        "documentation": {}
    },
    {
        "label": "get_max_rss",
        "kind": 2,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "def get_max_rss():\n    \"\"\"\n    Return the peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    \"\"\"\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \nmodel = Llama(model_path=args.model, \n              n_ctx=args.ctx_size, \n              n_batch=args.batch_size, \n              n_gpu_layers=args.n_gpu_layers,\n              n_gqa=args.gqa,",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('-m', '--model', type=str, default='', required=True, help=\"path to the GGML .bin model\")\nparser.add_argument('-p', '--prompt', type=str, default='Once upon a time,')\nparser.add_argument('-n', '--n-predict', type=int, default=128, help='number of output tokens to generate, including the input prompt')\nparser.add_argument('-c', '--ctx-size', type=int, default=512, help='size of the prompt context (default: 512)')\nparser.add_argument('-b', '--batch-size', type=int, default=512, help='batch size for prompt processing (default: 512)')\nparser.add_argument('-t', '--threads', type=int, default=6, help='number of threads to use during computation (default: 6)')\nparser.add_argument('-ngl', '--n-gpu-layers', type=int, default=999, help='number of layers to store in VRAM (default: 999)')\nparser.add_argument('-gqa', '--gqa', type=int, default=1, help='grouped-query attention factor (TEMP!!! use 8 for LLaMAv2 70B) (default: 1)')\nparser.add_argument('--top-k', type=int, default=40, help='top-k sampling (default: 40, 0 = disabled)')",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\ndef get_max_rss():\n    \"\"\"\n    Return the peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    \"\"\"\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \nmodel = Llama(model_path=args.model, \n              n_ctx=args.ctx_size, \n              n_batch=args.batch_size, ",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "model = Llama(model_path=args.model, \n              n_ctx=args.ctx_size, \n              n_batch=args.batch_size, \n              n_gpu_layers=args.n_gpu_layers,\n              n_gqa=args.gqa,\n              n_threads=args.threads)\ninput_tokens = model.tokenize(args.prompt.encode('utf-8'))\nprint(f\"input_tokens ({len(input_tokens)})\", input_tokens)\nprint(f\"system RAM used: {get_max_rss():.2f} MB\")\ntime_avg = 0.0",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "input_tokens",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "input_tokens = model.tokenize(args.prompt.encode('utf-8'))\nprint(f\"input_tokens ({len(input_tokens)})\", input_tokens)\nprint(f\"system RAM used: {get_max_rss():.2f} MB\")\ntime_avg = 0.0\nfor run in range(args.runs + args.warmup):\n    if not args.use_prompt_cache:\n        model.reset()\n    output_tokens = []\n    time_begin = time.perf_counter()\n    if args.profile_tokenization:",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "time_avg",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "time_avg = 0.0\nfor run in range(args.runs + args.warmup):\n    if not args.use_prompt_cache:\n        model.reset()\n    output_tokens = []\n    time_begin = time.perf_counter()\n    if args.profile_tokenization:\n        output = model(args.prompt, max_tokens=args.n_predict, top_k=args.top_k, top_p=args.top_p, echo=True)\n    else:   \n        for token in model.generate(input_tokens, top_k=args.top_k, top_p=args.top_p):",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "tokens_sec",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "tokens_sec = args.n_predict / time_avg \nmemory_usage = get_max_rss()\nprint(f\"\\nAVG = {time_avg:.4f} seconds, {tokens_sec:.1f} tokens/sec  memory={memory_usage:.2f} MB\\n\")  \nprint(args)\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, model, tokens, tokens/sec, latency, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, \")",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "memory_usage = get_max_rss()\nprint(f\"\\nAVG = {time_avg:.4f} seconds, {tokens_sec:.1f} tokens/sec  memory={memory_usage:.2f} MB\\n\")  \nprint(args)\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, model, tokens, tokens/sec, latency, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, \")\n        file.write(f\"{os.path.basename(args.model)}, {args.n_predict}, {tokens_sec}, {time_avg}, {memory_usage}\\n\")",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "print(f\"\\nAVG",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.benchmark",
        "description": "packages.llm.llama_cpp.benchmark",
        "peekOfCode": "print(f\"\\nAVG = {time_avg:.4f} seconds, {tokens_sec:.1f} tokens/sec  memory={memory_usage:.2f} MB\\n\")  \nprint(args)\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, model, tokens, tokens/sec, latency, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, \")\n        file.write(f\"{os.path.basename(args.model)}, {args.n_predict}, {tokens_sec}, {time_avg}, {memory_usage}\\n\")",
        "detail": "packages.llm.llama_cpp.benchmark",
        "documentation": {}
    },
    {
        "label": "llama_cpp",
        "kind": 2,
        "importPath": "packages.llm.llama_cpp.config",
        "description": "packages.llm.llama_cpp.config",
        "peekOfCode": "def llama_cpp(version, branch=None, test=None, default=False, flags=None):\n    pkg = package.copy()\n    pkg['name'] = f'llama_cpp:{version}'\n    if default:\n        pkg['alias'] = 'llama_cpp'\n    if not test:\n        test = \"test_model.py --model $(huggingface-downloader TheBloke/Llama-2-7B-GGUF/llama-2-7b.Q4_K_S.gguf)\"\n    pkg['test'] = pkg['test'] + [test]\n    if not branch:\n        branch = version",
        "detail": "packages.llm.llama_cpp.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.config",
        "description": "packages.llm.llama_cpp.config",
        "peekOfCode": "package = [\n    llama_cpp('0.2.57'),\n    llama_cpp('0.2.70', default=True),\n    llama_cpp('0.2.83', flags=\"-DGGML_CUDA=on\"),\n]",
        "detail": "packages.llm.llama_cpp.config",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_model",
        "description": "packages.llm.llama_cpp.test_model",
        "peekOfCode": "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-m', '--model', type=str, default='', required=True, help=\"path to the GGML .bin model\")\nparser.add_argument('-p', '--prompt', type=str, default='Once upon a time,')\nparser.add_argument('-n', '--n-predict', type=int, default=128, help='number of output tokens to generate, including the input prompt')\nparser.add_argument('-c', '--ctx-size', type=int, default=512, help='size of the prompt context (default: 512)')\nparser.add_argument('-b', '--batch-size', type=int, default=512, help='batch size for prompt processing (default: 512)')\nparser.add_argument('-t', '--threads', type=int, default=6, help='number of threads to use during computation (default: 6)')\nparser.add_argument('-ngl', '--n-gpu-layers', type=int, default=999, help='number of layers to store in VRAM (default: 999)')\nparser.add_argument('-gqa', '--gqa', type=int, default=1, help='grouped-query attention factor (TEMP!!! use 8 for LLaMAv2 70B) (default: 1)')\nargs = parser.parse_args()",
        "detail": "packages.llm.llama_cpp.test_model",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_model",
        "description": "packages.llm.llama_cpp.test_model",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\nmodel = Llama(model_path=args.model, \n              n_ctx=args.ctx_size, \n              n_batch=args.batch_size, \n              n_gpu_layers=args.n_gpu_layers,\n              n_gqa=args.gqa,\n              n_threads=args.threads)\nprint(f\"\\nPROMPT: {args.prompt}\\n\")\npprint.pprint(model(args.prompt, max_tokens=args.n_predict, echo=False))",
        "detail": "packages.llm.llama_cpp.test_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_model",
        "description": "packages.llm.llama_cpp.test_model",
        "peekOfCode": "model = Llama(model_path=args.model, \n              n_ctx=args.ctx_size, \n              n_batch=args.batch_size, \n              n_gpu_layers=args.n_gpu_layers,\n              n_gqa=args.gqa,\n              n_threads=args.threads)\nprint(f\"\\nPROMPT: {args.prompt}\\n\")\npprint.pprint(model(args.prompt, max_tokens=args.n_predict, echo=False))\nprint(\"\\nllama.cpp OK\")",
        "detail": "packages.llm.llama_cpp.test_model",
        "documentation": {}
    },
    {
        "label": "test_tokenizer",
        "kind": 2,
        "importPath": "packages.llm.llama_cpp.test_tokenizer",
        "description": "packages.llm.llama_cpp.test_tokenizer",
        "peekOfCode": "def test_tokenizer(text, add_bos=False, checks=[]):\n    print(f\"\\nPrompt:\\n{text}\")\n    tokens = llm.tokenize(text.encode('utf-8'), add_bos=add_bos)\n    print(f\"\\nTokens:\\n{tokens}\")\n    detokenized = llm.detokenize(tokens).decode('utf-8', errors='ignore')\n    print(f\"\\nDetokenized:\\n{detokenized}\")\n    for check in checks:\n        if tokens[check[0]] != check[1]:\n            raise RuntimeError(f\"token {check[0]} has unexpected value (actual={tokens[check[0]]} expected={check[1]})\")\n    if not add_bos and detokenized != text:",
        "detail": "packages.llm.llama_cpp.test_tokenizer",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_tokenizer",
        "description": "packages.llm.llama_cpp.test_tokenizer",
        "peekOfCode": "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-m', '--model', type=str, default='', required=True, help=\"path to the GGML .bin model\")\nparser.add_argument('-p', '--prompt', action='append', nargs='*')\n#parser.add_argument('--add-bos', action='store_true')\nargs = parser.parse_args()\nif args.prompt:\n    args.prompt = [x[0] for x in args.prompt]\nelse:\n    args.prompt = []\nprint(args)",
        "detail": "packages.llm.llama_cpp.test_tokenizer",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_tokenizer",
        "description": "packages.llm.llama_cpp.test_tokenizer",
        "peekOfCode": "args = parser.parse_args()\nif args.prompt:\n    args.prompt = [x[0] for x in args.prompt]\nelse:\n    args.prompt = []\nprint(args)\nllm = Llama(model_path=args.model, \n        n_gpu_layers=999,\n        vocab_only=True,\n        verbose=True)",
        "detail": "packages.llm.llama_cpp.test_tokenizer",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_tokenizer",
        "description": "packages.llm.llama_cpp.test_tokenizer",
        "peekOfCode": "llm = Llama(model_path=args.model, \n        n_gpu_layers=999,\n        vocab_only=True,\n        verbose=True)\ndef test_tokenizer(text, add_bos=False, checks=[]):\n    print(f\"\\nPrompt:\\n{text}\")\n    tokens = llm.tokenize(text.encode('utf-8'), add_bos=add_bos)\n    print(f\"\\nTokens:\\n{tokens}\")\n    detokenized = llm.detokenize(tokens).decode('utf-8', errors='ignore')\n    print(f\"\\nDetokenized:\\n{detokenized}\")",
        "detail": "packages.llm.llama_cpp.test_tokenizer",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_tokenizer",
        "description": "packages.llm.llama_cpp.test_tokenizer",
        "peekOfCode": "prompt = \"\"\"<s>[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\nThere's a llama in my garden! What should I do? [/INST]\"\"\"\ntest_tokenizer(prompt, checks=[[0,1]])\nprompt += \" I'm not sure, that's quite the conundrum! </s>\"\ntest_tokenizer(prompt, checks=[[0,1], [-1,2]])\nprompt += \"<s>[INST] It's eating all the plant's in my garden! [/INST]\"\ntest_tokenizer(prompt, checks=[[0,1]])",
        "detail": "packages.llm.llama_cpp.test_tokenizer",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "packages.llm.llama_cpp.test_tokenizer",
        "description": "packages.llm.llama_cpp.test_tokenizer",
        "peekOfCode": "prompt = \"\"\"<s>[INST] <<SYS>>\nAnswer the questions.\n<</SYS>>\nHello, how are you today? [/INST] Hello! I'm doing well, thank you for asking! How about you? Is there anything on your mind that you would like to talk about or ask me? I'm here to help with any questions you may have. </s><s>[INST] Wanna play 20 questions? [/INST]\n\"\"\"\ntest_tokenizer(prompt, checks=[[0,1]])\nprint(\"\\nllama.cpp tokenizer OK\")",
        "detail": "packages.llm.llama_cpp.test_tokenizer",
        "documentation": {}
    },
    {
        "label": "ASR",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.asr",
        "description": "packages.llm.llamaspeak.asr",
        "peekOfCode": "class ASR(threading.Thread):\n    \"\"\"\n    Streaming ASR service, either from microphone or web audio (or other samples from process_audio())\n    \"\"\"\n    def __init__(self, auth, input_device=None, sample_rate_hz=44100, audio_chunk=1600, audio_channels=1, \n                 automatic_punctuation=True, verbatim_transcripts=True, profanity_filter=False, \n                 language_code='en-US', boosted_lm_words=None, boosted_lm_score=4.0, callback=None, **kwargs):\n        super(ASR, self).__init__()\n        self.queue = AudioQueue()\n        self.callback = callback",
        "detail": "packages.llm.llamaspeak.asr",
        "documentation": {}
    },
    {
        "label": "AudioQueue",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.asr",
        "description": "packages.llm.llamaspeak.asr",
        "peekOfCode": "class AudioQueue:\n    \"\"\"\n    Implement same context manager/iterator interfaces as MicrophoneStream (for ASR.process_audio())\n    \"\"\"\n    def __init__(self, audio_chunk=1600):\n        self.queue = queue.Queue()\n        self.audio_chunk = audio_chunk\n    def put(self, samples):\n        self.queue.put(samples)\n    def __enter__(self):",
        "detail": "packages.llm.llamaspeak.asr",
        "documentation": {}
    },
    {
        "label": "AudioMixer",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.audio",
        "description": "packages.llm.llamaspeak.audio",
        "peekOfCode": "class AudioMixer(threading.Thread):\n    \"\"\"\n    Multi-track audio output mixer / sound generator\n    \"\"\"\n    def __init__(self, output_device=None, output_file=None, callback=None, sample_rate_hz=44100, audio_channels=1, **kwargs):\n        super(AudioMixer, self).__init__(daemon=True)\n        self.pa = pyaudio.PyAudio()\n        self.tracks = []\n        self.channels = audio_channels\n        self.callback = callback",
        "detail": "packages.llm.llamaspeak.audio",
        "documentation": {}
    },
    {
        "label": "Chatbot",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.chat",
        "description": "packages.llm.llamaspeak.chat",
        "peekOfCode": "class Chatbot(threading.Thread):\n    \"\"\"\n    LLM-based chatbot with streaming ASR/TTS.\n    This class essentially routes different requests/responses between the services.\n    \"\"\"\n    def __init__(self, args, **kwargs):\n        super(Chatbot, self).__init__()\n        self.args = args\n        self.auth = riva.client.Auth(uri=args.server) # args.ssl_cert, args.use_ssl\n        self.asr = ASR(self.auth, callback=self.on_asr_transcript, **vars(args)) #if args.input_device is not None else None",
        "detail": "packages.llm.llamaspeak.chat",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "packages.llm.llamaspeak.chat",
        "description": "packages.llm.llamaspeak.chat",
        "peekOfCode": "def parse_args():\n    \"\"\"\n    Parse command-line arguments for configuring the chatbot.\n    \"\"\"\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # audio I/O\n    parser.add_argument(\"--list-devices\", action=\"store_true\", help=\"List output audio devices indices.\")\n    parser.add_argument(\"--input-device\", type=int, default=None, help=\"An input audio device to use.\")\n    parser.add_argument(\"--output-device\", type=int, default=None, help=\"Output device to use.\")\n    parser.add_argument(\"--sample-rate-hz\", type=int, default=48000, help=\"Number of audio frames per second in synthesized audio.\")",
        "detail": "packages.llm.llamaspeak.chat",
        "documentation": {}
    },
    {
        "label": "LLM",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.llm",
        "description": "packages.llm.llamaspeak.llm",
        "peekOfCode": "class LLM(threading.Thread):\n    \"\"\"\n    Streaming LLM service using text-generation-webui API\n    \"\"\"\n    def __init__(self, llm_server='0.0.0.0', llm_api_port=5000, llm_streaming_port=5005, log_level=0, **kwargs):\n        super(LLM, self).__init__(daemon=True)  # stop thread on main() exit\n        self.queue = queue.Queue()\n        self.log_level = log_level\n        self.server = llm_server\n        self.blocking_port = llm_api_port",
        "detail": "packages.llm.llamaspeak.llm",
        "documentation": {}
    },
    {
        "label": "Tegrastats",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.tegrastats",
        "description": "packages.llm.llamaspeak.tegrastats",
        "peekOfCode": "class Tegrastats(threading.Thread):\n    \"\"\"\n    Reads system stats like CPU/GPU utilization, memory usage, ect.\n    \"\"\"\n    def __init__(self, tegrastats_poll_rate=0.5, callback=None, **kwargs):\n        super(Tegrastats, self).__init__(daemon=True)\n        self.gpu_path, gpu_name = self.find_gpu()\n        self.poll_rate = tegrastats_poll_rate\n        self.running = False\n        self.callback = callback",
        "detail": "packages.llm.llamaspeak.tegrastats",
        "documentation": {}
    },
    {
        "label": "TTS",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.tts",
        "description": "packages.llm.llamaspeak.tts",
        "peekOfCode": "class TTS(threading.Thread):\n    \"\"\"\n    Streaming TTS service\n    \"\"\"\n    def __init__(self, auth, language_code='en-US', voice='English-US.Female-1', sample_rate_hz=44100, **kwargs): \n        super(TTS, self).__init__()\n        self.queue = queue.Queue()\n        self.voice = voice\n        self.muted = False\n        self.language_code = language_code",
        "detail": "packages.llm.llamaspeak.tts",
        "documentation": {}
    },
    {
        "label": "Webserver",
        "kind": 6,
        "importPath": "packages.llm.llamaspeak.webserver",
        "description": "packages.llm.llamaspeak.webserver",
        "peekOfCode": "class Webserver(threading.Thread):\n    \"\"\"\n    Flask + websockets server for the chat interface\n    \"\"\"\n    def __init__(self, web_server='0.0.0.0', web_port=8050, ws_port=49000, \n                 ssl_cert=None, ssl_key=None, msg_callback=None, log_level=0, **kwargs):\n        super(Webserver, self).__init__(daemon=True)  # stop thread on main() exit\n        self.host = web_server\n        self.port = web_port\n        self.log_level = log_level",
        "detail": "packages.llm.llamaspeak.webserver",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "packages.llm.llava.benchmark",
        "description": "packages.llm.llava.benchmark",
        "peekOfCode": "def load_image(image_file):\n    if image_file.startswith('http') or image_file.startswith('https'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    return image\ndef get_max_rss():  # peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \ndisable_torch_init()",
        "detail": "packages.llm.llava.benchmark",
        "documentation": {}
    },
    {
        "label": "get_max_rss",
        "kind": 2,
        "importPath": "packages.llm.llava.benchmark",
        "description": "packages.llm.llava.benchmark",
        "peekOfCode": "def get_max_rss():  # peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \ndisable_torch_init()\nmodel_name = get_model_name_from_path(args.model_path)\ntokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit)\nif 'llama-2' in model_name.lower():\n    conv_mode = \"llava_llama_2\"\nelif \"v1\" in model_name.lower():\n    conv_mode = \"llava_v1\"\nelif \"mpt\" in model_name.lower():",
        "detail": "packages.llm.llava.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.llava.benchmark",
        "description": "packages.llm.llava.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"--model-path\", type=str, default=\"liuhaotian/llava-llama-2-13b-chat-lightning-preview\")\nparser.add_argument(\"--model-base\", type=str, default=None)\nparser.add_argument(\"--model-name\", type=str, default=None)\nparser.add_argument(\"--image-file\", type=str, default=\"/data/images/hoover.jpg\")\nparser.add_argument(\"--prompt\", action='append', nargs='*')\nparser.add_argument(\"--num-gpus\", type=int, default=1)\nparser.add_argument(\"--conv-mode\", type=str, default=None)\nparser.add_argument(\"--temperature\", type=float, default=0.2)\nparser.add_argument(\"--max-new-tokens\", type=int, default=64)",
        "detail": "packages.llm.llava.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.llava.benchmark",
        "description": "packages.llm.llava.benchmark",
        "peekOfCode": "args = parser.parse_args()\nif not args.prompt:\n    args.prompt = [\n        \"What does the sign in the image say?\",\n        \"How far is the exit?\",\n        \"What kind of environment is it in?\",\n        \"Does it look like it's going to rain?\",\n    ]\nprint(args)    \ndef load_image(image_file):",
        "detail": "packages.llm.llava.benchmark",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "packages.llm.llava.benchmark",
        "description": "packages.llm.llava.benchmark",
        "peekOfCode": "model_name = get_model_name_from_path(args.model_path)\ntokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit)\nif 'llama-2' in model_name.lower():\n    conv_mode = \"llava_llama_2\"\nelif \"v1\" in model_name.lower():\n    conv_mode = \"llava_v1\"\nelif \"mpt\" in model_name.lower():\n    conv_mode = \"mpt\"\nelse:\n    conv_mode = \"llava_v0\"",
        "detail": "packages.llm.llava.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.llava.test",
        "description": "packages.llm.llava.test",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--model-path', type=str, default='SaffalPoosh/llava-llama-2-7B-merged')\nparser.add_argument('--model-base', type=str, default=None)\nargs = parser.parse_args()\nprint(args)\nmodel = load_pretrained_model(\n    model_path=args.model_path,\n    model_base=args.model_base,\n    model_name=get_model_name_from_path(args.model_path)\n)",
        "detail": "packages.llm.llava.test",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.llava.test",
        "description": "packages.llm.llava.test",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\nmodel = load_pretrained_model(\n    model_path=args.model_path,\n    model_base=args.model_base,\n    model_name=get_model_name_from_path(args.model_path)\n)\nprint(model)",
        "detail": "packages.llm.llava.test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.llava.test",
        "description": "packages.llm.llava.test",
        "peekOfCode": "model = load_pretrained_model(\n    model_path=args.model_path,\n    model_base=args.model_base,\n    model_name=get_model_name_from_path(args.model_path)\n)\nprint(model)",
        "detail": "packages.llm.llava.test",
        "documentation": {}
    },
    {
        "label": "ChatAgent",
        "kind": 6,
        "importPath": "packages.llm.local_llm.agents.chat",
        "description": "packages.llm.local_llm.agents.chat",
        "peekOfCode": "class ChatAgent(Agent):\n    \"\"\"\n    Agent for two-turn multimodal chat.\n    \"\"\"\n    def __init__(self, model=\"meta-llama/Llama-2-7b-chat-hf\", interactive=True, **kwargs):\n        super().__init__()\n        \"\"\"\n        # Equivalent to:\n        self.pipeline = UserPrompt(interactive=interactive, **kwargs).add(\n            LLMQuery(model, **kwargs).add(",
        "detail": "packages.llm.local_llm.agents.chat",
        "documentation": {}
    },
    {
        "label": "VideoQuery",
        "kind": 6,
        "importPath": "packages.llm.local_llm.agents.video_query",
        "description": "packages.llm.local_llm.agents.video_query",
        "peekOfCode": "class VideoQuery(Agent):\n    \"\"\"\n    Perpetual always-on closed-loop visual agent that applies prompts to a video stream.\n    \"\"\"\n    def __init__(self, model=\"liuhaotian/llava-v1.5-13b\", nanodb=None, vision_scaling='resize', **kwargs):\n        super().__init__()\n        if not vision_scaling:\n            vision_scaling = 'resize'\n        # load model in another process for smooth streaming\n        self.llm = ProcessProxy('ChatQuery', model=model, drop_inputs=True, vision_scaling=vision_scaling, **kwargs) #ProcessProxy((lambda **kwargs: ChatQuery(model, drop_inputs=True, **kwargs)), **kwargs)",
        "detail": "packages.llm.local_llm.agents.video_query",
        "documentation": {}
    },
    {
        "label": "VideoStream",
        "kind": 6,
        "importPath": "packages.llm.local_llm.agents.video_stream",
        "description": "packages.llm.local_llm.agents.video_stream",
        "peekOfCode": "class VideoStream(Agent):\n    \"\"\"\n    Relay, view, or test a video stream.  Use the --video-input and --video-output arguments\n    to set the video source and output protocols used from:\n      https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-streaming.md\n    For example, this will capture a V4L2 camera and serve it via WebRTC with H.264 encoding:\n      python3 -m local_llm.agents.video_stream \\\n        --video-input /dev/video0 \\\n        --video-output webrtc://@:8554/output\n    It's also used as a basic test of video streaming before using more complex agents that rely on it.",
        "detail": "packages.llm.local_llm.agents.video_stream",
        "documentation": {}
    },
    {
        "label": "VoiceChat",
        "kind": 6,
        "importPath": "packages.llm.local_llm.agents.voice_chat",
        "description": "packages.llm.local_llm.agents.voice_chat",
        "peekOfCode": "class VoiceChat(Agent):\n    \"\"\"\n    Uses ASR + TTS to chat with LLM\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # LLM\n        self.llm = ProcessProxy('ChatQuery', **kwargs)  #ChatQuery(**kwargs) # # \n        self.llm.add(PrintStream(color='green'))\n        # ASR",
        "detail": "packages.llm.local_llm.agents.voice_chat",
        "documentation": {}
    },
    {
        "label": "WebChat",
        "kind": 6,
        "importPath": "packages.llm.local_llm.agents.web_chat",
        "description": "packages.llm.local_llm.agents.web_chat",
        "peekOfCode": "class WebChat(VoiceChat):\n    \"\"\"\n    Adds webserver to ASR/TTS voice chat agent.\n    \"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Parameters:\n          upload_dir (str) -- the path to save files uploaded from the client\n        See VoiceChat and WebServer for inherited arguments.\n        \"\"\"",
        "detail": "packages.llm.local_llm.agents.web_chat",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.local_llm.chat.example",
        "description": "packages.llm.local_llm.chat.example",
        "peekOfCode": "model = LocalLM.from_pretrained(\n    model='meta-llama/Llama-2-7b-chat-hf', \n    quant='q4f16_ft', \n    api='mlc'\n)\n# create the chat history\nchat_history = ChatHistory(model, system_prompt=\"You are a helpful and friendly AI assistant.\")\nwhile True:\n    # enter the user query from terminal\n    print('>> ', end='', flush=True)",
        "detail": "packages.llm.local_llm.chat.example",
        "documentation": {}
    },
    {
        "label": "chat_history",
        "kind": 5,
        "importPath": "packages.llm.local_llm.chat.example",
        "description": "packages.llm.local_llm.chat.example",
        "peekOfCode": "chat_history = ChatHistory(model, system_prompt=\"You are a helpful and friendly AI assistant.\")\nwhile True:\n    # enter the user query from terminal\n    print('>> ', end='', flush=True)\n    prompt = input().strip()\n    # add user prompt and generate chat tokens/embeddings\n    chat_history.append(role='user', msg=prompt)\n    embedding, position = chat_history.embed_chat()\n    # generate bot reply\n    reply = model.generate(",
        "detail": "packages.llm.local_llm.chat.example",
        "documentation": {}
    },
    {
        "label": "ChatHistory",
        "kind": 6,
        "importPath": "packages.llm.local_llm.chat.history",
        "description": "packages.llm.local_llm.chat.history",
        "peekOfCode": "class ChatHistory():\n    \"\"\"\n    Multimodal chat history that can contain a mix of media including text/images.\n    ChatHistory objects can be indexed like a list of chat entry dicts,\n    where each entry dict may have keys for 'text', 'image', 'role', ect.\n       `chat_history[n]` will return the n-th chat entry\n    Each type of media has a different embedding function (e.g. LLM's typically \n    do text token embedding internally, and images use CLIP + projection layers). \n    From these, it assembles the embedding for the entire chat as input to the LLM.\n    It uses templating to add the required special tokens as defined by different",
        "detail": "packages.llm.local_llm.chat.history",
        "documentation": {}
    },
    {
        "label": "ChatEntry",
        "kind": 2,
        "importPath": "packages.llm.local_llm.chat.history",
        "description": "packages.llm.local_llm.chat.history",
        "peekOfCode": "def ChatEntry(role='user', msg=None, **kwargs):\n    \"\"\"\n    Create a chat entry consisting of a text message, image, ect as input.  \n    Parameters:\n      role (str) -- The chat's turn template to apply, typically 'user' or 'bot'.\n                    The role should have a corresponding entry in the active ChatTemplate.\n      msg (str|image) -- If a string, it should either contain text or a path\n                         to a txt, json, or image file that will be loaded.\n                         If an image, can be a np.ndarray, torch.Tensor, or PIL.Image.\n                         If a dict, it will be passed through as the ChatEntry.",
        "detail": "packages.llm.local_llm.chat.history",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "kind": 6,
        "importPath": "packages.llm.local_llm.chat.stream",
        "description": "packages.llm.local_llm.chat.stream",
        "peekOfCode": "class StreamingResponse():\n    \"\"\"\n    Asynchronous output token iterator returned from a model's generate() function.\n    Use it to stream the reply from the LLM as they are decoded token-by-token:\n        ```\n        response = model.generate(\"Once upon a time,\")\n        for token in response:\n            print(token, end='', flush=True)\n        ```\n    To terminate processing prematurely, call the .stop() function, which will stop the model",
        "detail": "packages.llm.local_llm.chat.stream",
        "documentation": {}
    },
    {
        "label": "ChatTemplate",
        "kind": 2,
        "importPath": "packages.llm.local_llm.chat.templates",
        "description": "packages.llm.local_llm.chat.templates",
        "peekOfCode": "def ChatTemplate(model):\n    \"\"\"\n    Attempt to automatically determine the chat template from the model name/type.\n    Either returns one of the ChatTemplate dicts from above, or None if undetermined.\n    \"\"\"\n    if not isinstance(model, str):\n        model = model.config.name.lower()\n    if 'stablelm' in model and 'zephyr' in model:\n        chat_template = 'stablelm-zephyr'\n    elif 'obsidian-3b' in model:",
        "detail": "packages.llm.local_llm.chat.templates",
        "documentation": {}
    },
    {
        "label": "ChatTemplates",
        "kind": 5,
        "importPath": "packages.llm.local_llm.chat.templates",
        "description": "packages.llm.local_llm.chat.templates",
        "peekOfCode": "ChatTemplates = {\n    # https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n    'llama-2': {\n        'system_prompt': \"Answer the questions.\",\n        'system': '<s>[INST] <<SYS>>\\n${MESSAGE}\\n<</SYS>>\\n\\n',\n        'first': '${MESSAGE} [/INST]',\n        'user': '<s>[INST] ${MESSAGE} [/INST]',\n        'bot': ' ${MESSAGE}'  # llama-2 output already ends in </s>\n    },\n    # https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "detail": "packages.llm.local_llm.chat.templates",
        "documentation": {}
    },
    {
        "label": "ChatTemplates['llava-v0']",
        "kind": 5,
        "importPath": "packages.llm.local_llm.chat.templates",
        "description": "packages.llm.local_llm.chat.templates",
        "peekOfCode": "ChatTemplates['llava-v0'] = ChatTemplates['vicuna-v0']\nChatTemplates['llava-v1'] = ChatTemplates['vicuna-v1']\nChatTemplates['llava-llama-2'] = ChatTemplates['llama-2'].copy()\nChatTemplates['llava-llama-2'].update({\n    'system_prompt': \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n})\nStopTokens = ['</s>', '<|endoftext|>', '<|im_end|>', '<eos>']\nfor key in ChatTemplates:\n    ChatTemplates[key] = AttributeDict(name=key, **ChatTemplates[key])\ndef ChatTemplate(model):",
        "detail": "packages.llm.local_llm.chat.templates",
        "documentation": {}
    },
    {
        "label": "ChatTemplates['llava-v1']",
        "kind": 5,
        "importPath": "packages.llm.local_llm.chat.templates",
        "description": "packages.llm.local_llm.chat.templates",
        "peekOfCode": "ChatTemplates['llava-v1'] = ChatTemplates['vicuna-v1']\nChatTemplates['llava-llama-2'] = ChatTemplates['llama-2'].copy()\nChatTemplates['llava-llama-2'].update({\n    'system_prompt': \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n})\nStopTokens = ['</s>', '<|endoftext|>', '<|im_end|>', '<eos>']\nfor key in ChatTemplates:\n    ChatTemplates[key] = AttributeDict(name=key, **ChatTemplates[key])\ndef ChatTemplate(model):\n    \"\"\"",
        "detail": "packages.llm.local_llm.chat.templates",
        "documentation": {}
    },
    {
        "label": "ChatTemplates['llava-llama-2']",
        "kind": 5,
        "importPath": "packages.llm.local_llm.chat.templates",
        "description": "packages.llm.local_llm.chat.templates",
        "peekOfCode": "ChatTemplates['llava-llama-2'] = ChatTemplates['llama-2'].copy()\nChatTemplates['llava-llama-2'].update({\n    'system_prompt': \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n})\nStopTokens = ['</s>', '<|endoftext|>', '<|im_end|>', '<eos>']\nfor key in ChatTemplates:\n    ChatTemplates[key] = AttributeDict(name=key, **ChatTemplates[key])\ndef ChatTemplate(model):\n    \"\"\"\n    Attempt to automatically determine the chat template from the model name/type.",
        "detail": "packages.llm.local_llm.chat.templates",
        "documentation": {}
    },
    {
        "label": "StopTokens",
        "kind": 5,
        "importPath": "packages.llm.local_llm.chat.templates",
        "description": "packages.llm.local_llm.chat.templates",
        "peekOfCode": "StopTokens = ['</s>', '<|endoftext|>', '<|im_end|>', '<eos>']\nfor key in ChatTemplates:\n    ChatTemplates[key] = AttributeDict(name=key, **ChatTemplates[key])\ndef ChatTemplate(model):\n    \"\"\"\n    Attempt to automatically determine the chat template from the model name/type.\n    Either returns one of the ChatTemplate dicts from above, or None if undetermined.\n    \"\"\"\n    if not isinstance(model, str):\n        model = model.config.name.lower()",
        "detail": "packages.llm.local_llm.chat.templates",
        "documentation": {}
    },
    {
        "label": "AutoGPTQModel",
        "kind": 6,
        "importPath": "packages.llm.local_llm.models.auto_gptq",
        "description": "packages.llm.local_llm.models.auto_gptq",
        "peekOfCode": "class AutoGPTQModel(HFModel):\n    \"\"\"\n    AutoGPTQ (https://github.com/PanQiWei/AutoGPTQ)\n    \"\"\"\n    def __init__(self, model_path, **kwargs):\n        super(AutoGPTQModel, self).__init__(model_path, load=False, **kwargs)\n        self.model = AutoGPTQForCausalLM.from_quantized(\n            model_path, \n            device=self.device, \n            use_safetensors=True, ",
        "detail": "packages.llm.local_llm.models.auto_gptq",
        "documentation": {}
    },
    {
        "label": "AWQModel",
        "kind": 6,
        "importPath": "packages.llm.local_llm.models.awq",
        "description": "packages.llm.local_llm.models.awq",
        "peekOfCode": "class AWQModel(HFModel):\n    \"\"\"\n    AWQ model (https://github.com/mit-han-lab/llm-awq)\n    \"\"\"\n    def __init__(self, model_path, quant_path, w_bit=4, q_group_size=128, zero_point=True, **kwargs):\n        super(AWQModel, self).__init__(model_path, init_empty_weights=True, **kwargs)\n        if not quant_path:\n            raise ValueError(f\"AWQ model needs to have the --quant argument provided, with the path to the quantized model\")\n        if not os.path.isfile(quant_path):\n            raise ValueError(f\"AWQ quantized model not found: {quant_path}\")",
        "detail": "packages.llm.local_llm.models.awq",
        "documentation": {}
    },
    {
        "label": "HFModel",
        "kind": 6,
        "importPath": "packages.llm.local_llm.models.hf",
        "description": "packages.llm.local_llm.models.hf",
        "peekOfCode": "class HFModel(LocalLM):\n    \"\"\"\n    Huggingface Transformers model\n    \"\"\"\n    def __init__(self, model_path, load=True, init_empty_weights=False, **kwargs):\n        \"\"\"\n        Initializer\n        \"\"\"\n        super(HFModel, self).__init__(**kwargs)\n        self.model_path = model_path",
        "detail": "packages.llm.local_llm.models.hf",
        "documentation": {}
    },
    {
        "label": "TextIteratorWithStats",
        "kind": 6,
        "importPath": "packages.llm.local_llm.models.hf",
        "description": "packages.llm.local_llm.models.hf",
        "peekOfCode": "class TextIteratorWithStats:\n    def __init__(self, model, streamer):\n        self.model = model\n        self.streamer = streamer\n        self.model.stats.prefill_latency = 0\n        self.model.stats.decode_tokens = 0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        if self.model.stats.decode_tokens == 0:",
        "detail": "packages.llm.local_llm.models.hf",
        "documentation": {}
    },
    {
        "label": "MLCModel",
        "kind": 6,
        "importPath": "packages.llm.local_llm.models.mlc",
        "description": "packages.llm.local_llm.models.mlc",
        "peekOfCode": "class MLCModel(LocalLM):\n    \"\"\"\n    MLC model (https://github.com/mlc-ai/mlc-llm)\n    \"\"\"\n    def __init__(self, model_path, quant='q4f16_ft', max_context_len=None, **kwargs):\n        \"\"\"\n        Parameters:\n          model_path (str) -- the original model on HuggingFace - used for getting\n                              the tokenizer and original model config.  If this is a \n                              Llama2 model, it will be automatically set if Llama/ect",
        "detail": "packages.llm.local_llm.models.mlc",
        "documentation": {}
    },
    {
        "label": "AudioOutputDevice",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.audio_output",
        "description": "packages.llm.local_llm.plugins.audio.audio_output",
        "peekOfCode": "class AudioOutputDevice(Plugin):\n    \"\"\"\n    Output audio to an audio interface attached to the machine.\n    Expects to recieve audio samples as input, np.ndarray with dtype=float or int16\n    \"\"\"\n    def __init__(self, audio_output_device=0, audio_output_channels=1, sample_rate_hz=48000, **kwargs):\n        \"\"\"\n        Parameters:\n          audio_output_device (int) -- audio output device number for PortAudio\n          audio_output_channels (int) -- 1 for mono, 2 for stereo",
        "detail": "packages.llm.local_llm.plugins.audio.audio_output",
        "documentation": {}
    },
    {
        "label": "AudioOutputFile",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.audio_output",
        "description": "packages.llm.local_llm.plugins.audio.audio_output",
        "peekOfCode": "class AudioOutputFile(Plugin):\n    \"\"\"\n    Output audio to a wav file\n    Expects to recieve audio samples as input, np.ndarray with dtype=float or int16\n    TODO:  this doesn't fill in gaps for \"realtime playback\"\n    \"\"\"\n    def __init__(self, audio_output_file='output.wav', audio_output_channels=1, sample_rate_hz=48000, **kwargs):\n        \"\"\"\n        Parameters:\n          audio_output_file (str) -- path to the output wav file",
        "detail": "packages.llm.local_llm.plugins.audio.audio_output",
        "documentation": {}
    },
    {
        "label": "AutoASR",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.auto_asr",
        "description": "packages.llm.local_llm.plugins.audio.auto_asr",
        "peekOfCode": "class AutoASR(Plugin):\n    \"\"\"\n    Base class for ASR model plugins, supporting live transcription of audio streams.\n    \"\"\"\n    OutputFinal=0    # output full transcripts (channel 0)\n    OutputPartial=1  # output partial transcripts (channel 1)\n    @staticmethod\n    def from_pretrained(asr=None, **kwargs):\n        \"\"\"\n        Factory function for automatically creating different types of ASR models.",
        "detail": "packages.llm.local_llm.plugins.audio.auto_asr",
        "documentation": {}
    },
    {
        "label": "AutoTTS",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.auto_tts",
        "description": "packages.llm.local_llm.plugins.audio.auto_tts",
        "peekOfCode": "class AutoTTS(Plugin):\n    \"\"\"\n    Base class for TTS model plugins, supporting streaming, interruption/muting,\n    text buffering for gapless audio, injection of SSML speaker tags for pitch/rate,\n    text filtering (removing of emojis and number-to-text conversion), ect.\n    It's designed for streaming out chunks of audio as they are generated,\n    while recieving a stream of incoming text (usually word-by-word from LLM)\n    \"\"\"\n    def __init__(self, tts_buffering='punctuation', **kwargs):\n        super().__init__(**kwargs)",
        "detail": "packages.llm.local_llm.plugins.audio.auto_tts",
        "documentation": {}
    },
    {
        "label": "FastPitchTTS",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.fastpitch_tts",
        "description": "packages.llm.local_llm.plugins.audio.fastpitch_tts",
        "peekOfCode": "class FastPitchTTS(AutoTTS):\n    \"\"\"\n    Streaming TTS service using FastPitch-HiFiGAN model with ONNXRuntime.\n    This is a single-speaker model with a female voice in English.\n    Inputs:  words to speak (str)\n    Output:  audio samples (np.ndarray, int16)\n    \"\"\"\n    def __init__(self, model='/data/models/tts/fastpitch_hifigan', sample_rate_hz=22050, use_tensorrt=True, **kwargs):\n        \"\"\"\n        Load XTTS model and set default options (many of which can be changed at runtime)",
        "detail": "packages.llm.local_llm.plugins.audio.fastpitch_tts",
        "documentation": {}
    },
    {
        "label": "RivaASR",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.riva_asr",
        "description": "packages.llm.local_llm.plugins.audio.riva_asr",
        "peekOfCode": "class RivaASR(AutoASR):\n    \"\"\"\n    Streaming ASR service using NVIDIA Riva\n    https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html\n    You need to have the Riva server running first:\n    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart_arm64\n    Inputs:  incoming audio samples coming from another audio plugin\n             RivaASR can also open an audio device connected to this machine\n    Output:  two channels, the first for word-by-word 'partial' transcript strings\n             the second is for the full/final sentences",
        "detail": "packages.llm.local_llm.plugins.audio.riva_asr",
        "documentation": {}
    },
    {
        "label": "AudioQueue",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.riva_asr",
        "description": "packages.llm.local_llm.plugins.audio.riva_asr",
        "peekOfCode": "class AudioQueue:\n    \"\"\"\n    Implement same context manager/iterator interfaces as Riva's MicrophoneStream\n    for ingesting ASR audio samples from external sources via the plugin's input queue.\n    \"\"\"\n    def __init__(self, asr):\n        self.asr = asr\n    def __enter__(self):\n        return self\n    def __exit__(self, type, value, traceback):",
        "detail": "packages.llm.local_llm.plugins.audio.riva_asr",
        "documentation": {}
    },
    {
        "label": "RivaTTS",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.riva_tts",
        "description": "packages.llm.local_llm.plugins.audio.riva_tts",
        "peekOfCode": "class RivaTTS(AutoTTS):\n    \"\"\"\n    Streaming TTS service using NVIDIA Riva\n    https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html\n    You need to have the Riva server running first:\n    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart_arm64\n    Inputs:  words to speak (str)\n    Output:  audio samples (np.ndarray, int16)\n    \"\"\"\n    def __init__(self, riva_server='localhost:50051', ",
        "detail": "packages.llm.local_llm.plugins.audio.riva_tts",
        "documentation": {}
    },
    {
        "label": "XTTS",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.audio.xtts",
        "description": "packages.llm.local_llm.plugins.audio.xtts",
        "peekOfCode": "class XTTS(AutoTTS):\n    \"\"\"\n    Streaming TTS service using XTTS model with HiFiGAN decoder in TensorRT.\n    https://huggingface.co/coqui/XTTS-v2\n    https://github.com/coqui-ai/TTS\n    Inputs:  words to speak (str)\n    Output:  audio samples (np.ndarray, int16)\n    You can get the list of voices with tts.voices, and list of languages with tts.languages\n    The speed can be set with tts.rate (1.0 = normal). The default voice is '...' with rate 1.0\n    \"\"\"",
        "detail": "packages.llm.local_llm.plugins.audio.xtts",
        "documentation": {}
    },
    {
        "label": "Callback",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.callback",
        "description": "packages.llm.local_llm.plugins.callback",
        "peekOfCode": "class Callback(Plugin):\n    \"\"\"\n    Wrapper for calling a function with the same signature as Plugin.process()\n    This is automatically used by Plugin.add() so it's typically not needed.\n    Callbacks are threaded by default and will be run asynchronously.\n    If it's a lightweight non-blocking function, you can set threaded=False\n    \"\"\"\n    def __init__(self, function, threaded=False, **kwargs):\n        \"\"\"\n        Parameters:",
        "detail": "packages.llm.local_llm.plugins.callback",
        "documentation": {}
    },
    {
        "label": "ChatQuery",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.chat_query",
        "description": "packages.llm.local_llm.plugins.chat_query",
        "peekOfCode": "class ChatQuery(Plugin):\n    \"\"\"\n    Plugin that feeds incoming text or ChatHistory to LLM and generates the reply.\n    It can either internally manage the ChatHistory, or that can be done externally.\n    Inputs:  (str or list[str]) -- one or more text prompts\n             (dict) -- an existing ChatEntry dict\n             (ChatHistory) -- use latest entry from chat history\n    Outputs:  channel 0 (str) -- the partially-generated output text, token-by-token\n              channel 1 (str) -- the partially-generated output text, word-by-word\n              channel 2 (str) -- the entire final output text, after generation is complete",
        "detail": "packages.llm.local_llm.plugins.chat_query",
        "documentation": {}
    },
    {
        "label": "EventFilter",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.event_filter",
        "description": "packages.llm.local_llm.plugins.event_filter",
        "peekOfCode": "class EventFilter(Plugin):\n    \"\"\"\n    Plugin that filters output text from LLM and triggers events when it matches/changes\n    \"\"\"\n    def __init__(self, filters=None, server=None, **kwargs):\n        \"\"\"\n        Parameters:\n          filters (str or list[str]) -- see parse_filters() function\n          kwargs (dict) -- these get passed to the Plugin constructor\n        \"\"\"",
        "detail": "packages.llm.local_llm.plugins.event_filter",
        "documentation": {}
    },
    {
        "label": "NanoDB",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.nanodb",
        "description": "packages.llm.local_llm.plugins.nanodb",
        "peekOfCode": "class NanoDB(Plugin):\n    \"\"\"\n    Plugin that loads a NanoDB database and searches it for incoming text/images   \n    \"\"\"\n    def __init__(self, path=None, model='ViT-L/14@336px', reserve=1024, k=1, **kwargs):\n        \"\"\"\n        Parameters:\n          path (str) -- directory to either load or create NanoDB at\n          model (str) -- the CLIP embedding model to use\n          reserve (int) -- the memory to reserve for the database in MB",
        "detail": "packages.llm.local_llm.plugins.nanodb",
        "documentation": {}
    },
    {
        "label": "PrintStream",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.print_stream",
        "description": "packages.llm.local_llm.plugins.print_stream",
        "peekOfCode": "class PrintStream(Plugin):\n    \"\"\"\n    Output plugin that prints chatbot responses to stdout.\n    \"\"\"\n    def __init__(self, partial=True, prefix=None, color='green', **kwargs):\n        \"\"\"\n        Parameters:\n          partial (bool) -- if true, print token-by-token (otherwise, end with newline)\n          prefix (str) -- text to print out before incoming text\n          color (str) -- the color to print the output stream (or None for no colors)",
        "detail": "packages.llm.local_llm.plugins.print_stream",
        "documentation": {}
    },
    {
        "label": "ProcessProxy",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.process_proxy",
        "description": "packages.llm.local_llm.plugins.process_proxy",
        "peekOfCode": "class ProcessProxy(Plugin):\n    \"\"\"\n    Proxy wrapper for running a plugin in a subprocess\n    \"\"\"\n    def __init__(self, plugin_factory, **kwargs):\n        \"\"\"\n        Parameters:\n          plugin_factory (callable) -- Factory function that returns a plugin instance.\n                                       This will be called from the new process to create it.\n        \"\"\"",
        "detail": "packages.llm.local_llm.plugins.process_proxy",
        "documentation": {}
    },
    {
        "label": "OutputProxy",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.process_proxy",
        "description": "packages.llm.local_llm.plugins.process_proxy",
        "peekOfCode": "class OutputProxy(Plugin):\n    def __init__(self, pipe, channel, **kwargs):\n        super().__init__(threaded=False)\n        self.pipe = pipe\n        self.channel = channel\n        self.enabled = True\n    def process(self, input, **kwargs):\n        #logging.debug(f\"subprocess sending {type(input)} {input} (channel={self.channel})\")\n        try:\n            if self.enabled:",
        "detail": "packages.llm.local_llm.plugins.process_proxy",
        "documentation": {}
    },
    {
        "label": "RateLimit",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.rate_limit",
        "description": "packages.llm.local_llm.plugins.rate_limit",
        "peekOfCode": "class RateLimit(Plugin):\n    \"\"\"\n    Rate limiter plugin with the ability to pause/resume from the queue.\n      video_limiter = RateLimit(30)  # 30 FPS\n      audio_limiter = RateLimit(48000, chunk=4800)  \n    It can also chunk indexable outputs into smaller amounts of data at a time.\n    \"\"\"\n    def __init__(self, rate=None, chunk=None, **kwargs):\n        \"\"\"\n        Parameters:",
        "detail": "packages.llm.local_llm.plugins.rate_limit",
        "documentation": {}
    },
    {
        "label": "UserPrompt",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.user_prompt",
        "description": "packages.llm.local_llm.plugins.user_prompt",
        "peekOfCode": "class UserPrompt(Plugin):\n    \"\"\"\n    Source plugin that reads text prompts, either interactively from stdin,\n    or from a .txt or .json file.  It will also forward/open any text inputs.\n    Outputs a string of the prompt text (or list of strings if multiple prompts).\n    \"\"\"\n    def __init__(self, prompt=None, interactive=False, prefix=None, **kwargs):\n        \"\"\"\n        Parameters:\n          prompt (str) -- optional initial prompt or path to txt/json file",
        "detail": "packages.llm.local_llm.plugins.user_prompt",
        "documentation": {}
    },
    {
        "label": "VideoSource",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.video",
        "description": "packages.llm.local_llm.plugins.video",
        "peekOfCode": "class VideoSource(Plugin):\n    \"\"\"\n    Captures or loads a video/camera stream or sequence of images\n    https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-streaming.md\n    \"\"\"\n    def __init__(self, video_input='/dev/video0', \n                 video_input_width=None, video_input_height=None, \n                 video_input_codec=None, video_input_framerate=None, \n                 video_input_save=None, return_tensors='cuda', **kwargs):\n        \"\"\"",
        "detail": "packages.llm.local_llm.plugins.video",
        "documentation": {}
    },
    {
        "label": "VideoOutput",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugins.video",
        "description": "packages.llm.local_llm.plugins.video",
        "peekOfCode": "class VideoOutput(Plugin):\n    \"\"\"\n    Saves images to a compressed video or directory of individual images, the display, or a network stream.\n    https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-streaming.md\n    \"\"\"\n    def __init__(self, video_output=None, video_output_codec=None, video_output_bitrate=None, video_output_save=None, **kwargs):\n        \"\"\"\n        Parameters:\n          input (str) -- path to video file, directory of images, or stream URL\n          output_codec (str) -- force a particular codec ('h264', 'h265', 'vp8', 'vp9', 'mjpeg', ect)",
        "detail": "packages.llm.local_llm.plugins.video",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.asr",
        "description": "packages.llm.local_llm.test.asr",
        "peekOfCode": "args = ArgParser(extras=['asr', 'audio_input', 'log']).parse_args()\nasr = AutoASR.from_pretrained(**vars(args))\nasr.add(PrintStream(partial=False, prefix='## ', color='green'), AutoASR.OutputFinal)\nasr.add(PrintStream(partial=False, prefix='>> ', color='blue'), AutoASR.OutputPartial)\nasr.start().join()",
        "detail": "packages.llm.local_llm.test.asr",
        "documentation": {}
    },
    {
        "label": "asr",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.asr",
        "description": "packages.llm.local_llm.test.asr",
        "peekOfCode": "asr = AutoASR.from_pretrained(**vars(args))\nasr.add(PrintStream(partial=False, prefix='## ', color='green'), AutoASR.OutputFinal)\nasr.add(PrintStream(partial=False, prefix='>> ', color='blue'), AutoASR.OutputPartial)\nasr.start().join()",
        "detail": "packages.llm.local_llm.test.asr",
        "documentation": {}
    },
    {
        "label": "print_help",
        "kind": 2,
        "importPath": "packages.llm.local_llm.test.asr_tts_loopback",
        "description": "packages.llm.local_llm.test.asr_tts_loopback",
        "peekOfCode": "def print_help():\n    print(f\"\\nSpeak into the mic, or enter these commands:\\n\")\n    print(f\"  /voices            List the voice names\")\n    print(f\"  /voice Voice Name  Change the voice (current='{tts.voice}')\")\n    print(f\"  /languages         List the languages\")\n    print(f\"  /language en-US    Set the language code (current='{tts.language}')\")\n    print(f\"  /rate 1.0          Set the speaker rate (current={tts.rate:.2f})\")\n    print(f\"  /buffer none       Disable input buffering (current='{','.join(tts.buffering)}')\")\n    print(f\"  /help or /?        Print the help text\\n\")  \n    print(f\"Press Ctrl+C to exit.\\n\")",
        "detail": "packages.llm.local_llm.test.asr_tts_loopback",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.asr_tts_loopback",
        "description": "packages.llm.local_llm.test.asr_tts_loopback",
        "peekOfCode": "args = ArgParser(extras=['asr', 'tts', 'audio_input', 'audio_output', 'log']).parse_args()\nasr = AutoASR.from_pretrained(**vars(args))\ntts = AutoTTS.from_pretrained(**vars(args))\nasr.add(PrintStream(partial=False, prefix='## ', color='green'), AutoASR.OutputFinal)\nasr.add(PrintStream(partial=False, prefix='>> ', color='blue'), AutoASR.OutputPartial)\nasr.add(tts, AutoASR.OutputFinal)\nif args.audio_output_device is not None:\n    tts.add(AudioOutputDevice(**vars(args)))\nif args.audio_output_file is not None:\n    tts.add(AudioOutputFile(**vars(args)))",
        "detail": "packages.llm.local_llm.test.asr_tts_loopback",
        "documentation": {}
    },
    {
        "label": "asr",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.asr_tts_loopback",
        "description": "packages.llm.local_llm.test.asr_tts_loopback",
        "peekOfCode": "asr = AutoASR.from_pretrained(**vars(args))\ntts = AutoTTS.from_pretrained(**vars(args))\nasr.add(PrintStream(partial=False, prefix='## ', color='green'), AutoASR.OutputFinal)\nasr.add(PrintStream(partial=False, prefix='>> ', color='blue'), AutoASR.OutputPartial)\nasr.add(tts, AutoASR.OutputFinal)\nif args.audio_output_device is not None:\n    tts.add(AudioOutputDevice(**vars(args)))\nif args.audio_output_file is not None:\n    tts.add(AudioOutputFile(**vars(args)))\nasr.start()",
        "detail": "packages.llm.local_llm.test.asr_tts_loopback",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.asr_tts_loopback",
        "description": "packages.llm.local_llm.test.asr_tts_loopback",
        "peekOfCode": "tts = AutoTTS.from_pretrained(**vars(args))\nasr.add(PrintStream(partial=False, prefix='## ', color='green'), AutoASR.OutputFinal)\nasr.add(PrintStream(partial=False, prefix='>> ', color='blue'), AutoASR.OutputPartial)\nasr.add(tts, AutoASR.OutputFinal)\nif args.audio_output_device is not None:\n    tts.add(AudioOutputDevice(**vars(args)))\nif args.audio_output_file is not None:\n    tts.add(AudioOutputFile(**vars(args)))\nasr.start()\ndef print_help():",
        "detail": "packages.llm.local_llm.test.asr_tts_loopback",
        "documentation": {}
    },
    {
        "label": "MultiprocessChat",
        "kind": 6,
        "importPath": "packages.llm.local_llm.test.mp_chat",
        "description": "packages.llm.local_llm.test.mp_chat",
        "peekOfCode": "class MultiprocessChat(Agent):\n    \"\"\"\n    Test of running a LLM and chat session in a subprocess.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.pipeline = Pipeline([\n            UserPrompt(interactive=False, **kwargs),  # interactive=False if kwargs.get('prompt') else True\n            ProcessProxy((lambda **kwargs: ChatQuery(**kwargs)), **kwargs),\n            PrintStream(color='green')  ",
        "detail": "packages.llm.local_llm.test.mp_chat",
        "documentation": {}
    },
    {
        "label": "MultiprocessTest",
        "kind": 6,
        "importPath": "packages.llm.local_llm.test.mp_test",
        "description": "packages.llm.local_llm.test.mp_test",
        "peekOfCode": "class MultiprocessTest(Agent):\n    \"\"\"\n    This is a test of the ProcessProxy plugin for running pipelines and plugins in their own processes.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.pipeline = Pipeline([\n            ProcessProxy((lambda **kwargs: PrintStream(**kwargs)), color='green', relay=True, **kwargs),\n            PrintStream(color='blue', **kwargs),\n        ])",
        "detail": "packages.llm.local_llm.test.mp_test",
        "documentation": {}
    },
    {
        "label": "MultiprocessVideo",
        "kind": 6,
        "importPath": "packages.llm.local_llm.test.mp_video",
        "description": "packages.llm.local_llm.test.mp_video",
        "peekOfCode": "class MultiprocessVideo(Agent):\n    \"\"\"\n    Test of running a video stream across processes.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.video_source = VideoSource(return_tensors='np', **kwargs)\n        self.video_output = ProcessProxy((lambda **kwargs: VideoOutput(**kwargs)), **kwargs)\n        self.video_source.add(self.on_video, threaded=False)\n        self.video_source.add(self.video_output)",
        "detail": "packages.llm.local_llm.test.mp_video",
        "documentation": {}
    },
    {
        "label": "print_prompt",
        "kind": 2,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "def print_prompt():\n    termcolor.cprint('\\n>> ', 'blue', end='', flush=True)\ndef print_help():\n    print(f\"Enter text to synthesize, or one of these commands:\\n\")\n    print(f\"  /defaults          Generate a default test sequence\")\n    print(f\"  /voices            List the voice names\")\n    print(f\"  /voice Voice Name  Change the voice (current='{tts.voice}')\")\n    print(f\"  /languages         List the languages\")\n    print(f\"  /language en-US    Set the language code (current='{tts.language}')\")\n    print(f\"  /rate 1.0          Set the speaker rate (current={tts.rate:.2f})\")",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "print_help",
        "kind": 2,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "def print_help():\n    print(f\"Enter text to synthesize, or one of these commands:\\n\")\n    print(f\"  /defaults          Generate a default test sequence\")\n    print(f\"  /voices            List the voice names\")\n    print(f\"  /voice Voice Name  Change the voice (current='{tts.voice}')\")\n    print(f\"  /languages         List the languages\")\n    print(f\"  /language en-US    Set the language code (current='{tts.language}')\")\n    print(f\"  /rate 1.0          Set the speaker rate (current={tts.rate:.2f})\")\n    print(f\"  /buffer none       Disable input buffering (current='{','.join(tts.buffering)}')\")\n    print(f\"  /interrupt or /i   Interrupt/mute the TTS output\")",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "commands",
        "kind": 2,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "def commands(text):\n    try:\n        cmd = text.lower().strip()\n        if cmd.startswith('/default'):\n            tts(\"Hello there, how are you today? \")\n            tts(\"The weather is 76 degrees out and sunny. \")\n            tts(\"Your first meeting is in an hour downtown, with normal traffic. \")\n            tts(\"Can I interest you in anything quick for breakfast?\")\n        elif cmd.startswith('/voices'):\n            print(tts.voices)",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "on_interrupt",
        "kind": 2,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "def on_interrupt():\n    tts.interrupt()\n    print_prompt()\ntts = AutoTTS.from_pretrained(**vars(args))\ninterrupt = KeyboardInterrupt(callback=on_interrupt)\nif args.audio_output_device is not None:\n    tts.add(AudioOutputDevice(**vars(args)))\nif args.audio_output_file is not None:\n    tts.add(AudioOutputFile(**vars(args)))\nprompt = UserPrompt(interactive=True, **vars(args)).add(",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "args = ArgParser(extras=['tts', 'audio_output', 'prompt', 'log']).parse_args()\ndef print_prompt():\n    termcolor.cprint('\\n>> ', 'blue', end='', flush=True)\ndef print_help():\n    print(f\"Enter text to synthesize, or one of these commands:\\n\")\n    print(f\"  /defaults          Generate a default test sequence\")\n    print(f\"  /voices            List the voice names\")\n    print(f\"  /voice Voice Name  Change the voice (current='{tts.voice}')\")\n    print(f\"  /languages         List the languages\")\n    print(f\"  /language en-US    Set the language code (current='{tts.language}')\")",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "tts = AutoTTS.from_pretrained(**vars(args))\ninterrupt = KeyboardInterrupt(callback=on_interrupt)\nif args.audio_output_device is not None:\n    tts.add(AudioOutputDevice(**vars(args)))\nif args.audio_output_file is not None:\n    tts.add(AudioOutputFile(**vars(args)))\nprompt = UserPrompt(interactive=True, **vars(args)).add(\n    Callback(commands).add(tts)\n)\nprint_help()",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "interrupt",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "interrupt = KeyboardInterrupt(callback=on_interrupt)\nif args.audio_output_device is not None:\n    tts.add(AudioOutputDevice(**vars(args)))\nif args.audio_output_file is not None:\n    tts.add(AudioOutputFile(**vars(args)))\nprompt = UserPrompt(interactive=True, **vars(args)).add(\n    Callback(commands).add(tts)\n)\nprint_help()\nprompt.start().join()",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.tts",
        "description": "packages.llm.local_llm.test.tts",
        "peekOfCode": "prompt = UserPrompt(interactive=True, **vars(args)).add(\n    Callback(commands).add(tts)\n)\nprint_help()\nprompt.start().join()",
        "detail": "packages.llm.local_llm.test.tts",
        "documentation": {}
    },
    {
        "label": "on_video",
        "kind": 2,
        "importPath": "packages.llm.local_llm.test.video",
        "description": "packages.llm.local_llm.test.video",
        "peekOfCode": "def on_video(image):\n    num_frames = video_source.stream.GetFrameCount()\n    if num_frames % 25 == 0:\n        logging.info(f'captured {num_frames} frames ({image.width}x{image.height}) from {video_source.resource}')\nvideo_source = VideoSource(**vars(args))\nvideo_output = VideoOutput(**vars(args))\nvideo_source.add(on_video, threaded=False)\nvideo_source.add(video_output)\nvideo_source.start().join()",
        "detail": "packages.llm.local_llm.test.video",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.video",
        "description": "packages.llm.local_llm.test.video",
        "peekOfCode": "args = ArgParser(extras=['video_input', 'video_output', 'log']).parse_args()\ndef on_video(image):\n    num_frames = video_source.stream.GetFrameCount()\n    if num_frames % 25 == 0:\n        logging.info(f'captured {num_frames} frames ({image.width}x{image.height}) from {video_source.resource}')\nvideo_source = VideoSource(**vars(args))\nvideo_output = VideoOutput(**vars(args))\nvideo_source.add(on_video, threaded=False)\nvideo_source.add(video_output)\nvideo_source.start().join()",
        "detail": "packages.llm.local_llm.test.video",
        "documentation": {}
    },
    {
        "label": "video_source",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.video",
        "description": "packages.llm.local_llm.test.video",
        "peekOfCode": "video_source = VideoSource(**vars(args))\nvideo_output = VideoOutput(**vars(args))\nvideo_source.add(on_video, threaded=False)\nvideo_source.add(video_output)\nvideo_source.start().join()",
        "detail": "packages.llm.local_llm.test.video",
        "documentation": {}
    },
    {
        "label": "video_output",
        "kind": 5,
        "importPath": "packages.llm.local_llm.test.video",
        "description": "packages.llm.local_llm.test.video",
        "peekOfCode": "video_output = VideoOutput(**vars(args))\nvideo_source.add(on_video, threaded=False)\nvideo_source.add(video_output)\nvideo_source.start().join()",
        "detail": "packages.llm.local_llm.test.video",
        "documentation": {}
    },
    {
        "label": "ArgParser",
        "kind": 6,
        "importPath": "packages.llm.local_llm.utils.args",
        "description": "packages.llm.local_llm.utils.args",
        "peekOfCode": "class ArgParser(argparse.ArgumentParser):\n    \"\"\"\n    Adds selectable extra args that are commonly used by this project\n    \"\"\"\n    Defaults = ['model', 'chat', 'generation', 'log']\n    Audio = ['audio_input', 'audio_output']\n    Video = ['video_input', 'video_output']\n    Riva = ['asr', 'tts']\n    def __init__(self, extras=Defaults, **kwargs):\n        super().__init__(formatter_class=argparse.ArgumentDefaultsHelpFormatter, **kwargs)",
        "detail": "packages.llm.local_llm.utils.args",
        "documentation": {}
    },
    {
        "label": "convert_audio",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.audio",
        "description": "packages.llm.local_llm.utils.audio",
        "peekOfCode": "def convert_audio(samples, dtype=np.int16):\n    \"\"\"\n    Convert between audio datatypes like float<->int16 and apply sample re-scaling.\n    If the samples are a raw bytes array, it's assumed that they are in int16 format.\n    Supports audio samples as byte buffer, numpy ndarray, and torch.Tensor.  Converted\n    byte buffers will be returned as ndarray, otherwise the same object type as input.\n    \"\"\"\n    if isinstance(samples, bytes):\n        samples = np.frombuffer(samples, dtype=np.int16)\n    elif not isinstance(samples, (np.ndarray, torch.Tensor)):",
        "detail": "packages.llm.local_llm.utils.audio",
        "documentation": {}
    },
    {
        "label": "audio_rms",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.audio",
        "description": "packages.llm.local_llm.utils.audio",
        "peekOfCode": "def audio_rms(samples):\n    \"\"\"\n    Compute the average audio RMS (returns a float between 0 and 1)\n    \"\"\"\n    return np.sqrt(np.mean(convert_audio(samples, dtype=np.float32)**2))\ndef audio_silent(samples, threshold=0.0):\n    \"\"\"\n    Detect if the audio samples are silent or muted.\n    If threshold < 0, false will be returned (silence detection disabled).\n    If threshold > 0, the audio's average RMS will be compared to the threshold.",
        "detail": "packages.llm.local_llm.utils.audio",
        "documentation": {}
    },
    {
        "label": "audio_silent",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.audio",
        "description": "packages.llm.local_llm.utils.audio",
        "peekOfCode": "def audio_silent(samples, threshold=0.0):\n    \"\"\"\n    Detect if the audio samples are silent or muted.\n    If threshold < 0, false will be returned (silence detection disabled).\n    If threshold > 0, the audio's average RMS will be compared to the threshold.\n    If threshold = 0, it will check for any non-zero samples (faster than RMS)\n    Returns true if audio levels are below threshold, otherwise false.\n    \"\"\"\n    if threshold < 0:\n        return False",
        "detail": "packages.llm.local_llm.utils.audio",
        "documentation": {}
    },
    {
        "label": "is_image",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "def is_image(image):\n    \"\"\"\n    Returns true if the object is a PIL.Image, np.ndarray, torch.Tensor, or jetson_utils.cudaImage\n    \"\"\"\n    return isinstance(image, ImageTypes)\ndef image_size(image):\n    \"\"\"\n    Returns the dimensions of the image as a tuple (height, width, channels)\n    \"\"\"\n    if isinstance(image, (cudaImage, np.ndarray, torch.Tensor)):",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "image_size",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "def image_size(image):\n    \"\"\"\n    Returns the dimensions of the image as a tuple (height, width, channels)\n    \"\"\"\n    if isinstance(image, (cudaImage, np.ndarray, torch.Tensor)):\n        return image.shape\n    elif isinstance(image, PIL.Image.Image):\n        return image.size\n    else:\n        raise TypeError(f\"expected an image of type {ImageTypes} (was {type(image)})\")",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "def load_image(path):\n    \"\"\"\n    Load an image from a local path or URL\n    TODO have this use jetson_utils instead\n    \"\"\"\n    if path.startswith('http') or path.startswith('https'):\n        logging.debug(f'-- downloading {path}')\n        response = requests.get(path)\n        image = PIL.Image.open(io.BytesIO(response.content)).convert('RGB')\n    else:",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "cuda_image",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "def cuda_image(image):\n    \"\"\"\n    Convert an image from PIL.Image, np.ndarray, torch.Tensor, or __gpu_array_interface__\n    to a jetson_utils.cudaImage on the GPU (without using memory copies when possible)\n    TODO implement __gpu_array_interface__\n    TODO torch image formats https://github.com/dusty-nv/jetson-utils/blob/f0bff5c502f9ac6b10aa2912f1324797df94bc2d/python/examples/cuda-from-pytorch.py#L47\n    \"\"\"\n    if not is_image(image):\n        raise TypeError(f\"expected an image of type {ImageTypes} (was {type(image)})\")\n    if isinstance(image, cudaImage):",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "torch_image",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "def torch_image(image):\n    \"\"\"\n    Convert the image to a type that is compatible with PyTorch (torch.Tensor, ndarray, PIL.Image)\n    \"\"\"\n    if isinstance(image, cudaImage):\n        return torch.as_tensor(image, device='cuda')\n    elif is_image(image):\n        return image \n    raise TypeError(f\"expected an image of type {ImageTypes} (was {type(image)})\")\ndef torch_image_format(tensor):",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "torch_image_format",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "def torch_image_format(tensor):\n    \"\"\"\n    Determine the cudaImage format string (eg 'rgb32f', 'rgba32f', ect) from a PyTorch tensor.\n    Only float and uint8 tensors are supported because those datatypes are supported by cudaImage.\n    \"\"\"\n    if tensor.dtype != torch.float32 and tensor.dtype != torch.uint8:\n        raise ValueError(f\"PyTorch tensor datatype should be torch.float32 or torch.uint8 (was {tensor.dtype})\")\n    if len(tensor.shape)>= 4:     # NCHW layout\n        channels = tensor.shape[1]\n    elif len(tensor.shape) == 3:   # CHW layout",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "ImageTypes",
        "kind": 5,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "ImageTypes = (PIL.Image.Image, np.ndarray, torch.Tensor, cudaImage)\nImageExtensions = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')\ndef is_image(image):\n    \"\"\"\n    Returns true if the object is a PIL.Image, np.ndarray, torch.Tensor, or jetson_utils.cudaImage\n    \"\"\"\n    return isinstance(image, ImageTypes)\ndef image_size(image):\n    \"\"\"\n    Returns the dimensions of the image as a tuple (height, width, channels)",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "ImageExtensions",
        "kind": 5,
        "importPath": "packages.llm.local_llm.utils.image",
        "description": "packages.llm.local_llm.utils.image",
        "peekOfCode": "ImageExtensions = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')\ndef is_image(image):\n    \"\"\"\n    Returns true if the object is a PIL.Image, np.ndarray, torch.Tensor, or jetson_utils.cudaImage\n    \"\"\"\n    return isinstance(image, ImageTypes)\ndef image_size(image):\n    \"\"\"\n    Returns the dimensions of the image as a tuple (height, width, channels)\n    \"\"\"",
        "detail": "packages.llm.local_llm.utils.image",
        "documentation": {}
    },
    {
        "label": "KeyboardInterrupt",
        "kind": 6,
        "importPath": "packages.llm.local_llm.utils.keyboard",
        "description": "packages.llm.local_llm.utils.keyboard",
        "peekOfCode": "class KeyboardInterrupt():\n    \"\"\"\n    Ctrl+C handler - if done once, sets the interrupt flag and optionally calls a callback.\n    If done twice in succession, exits when 'timeout' is set to a positive number of seconds.\n    \"\"\"\n    def __init__(self, callback=None, timeout=2.0):\n        self.timeout = -1.0 if timeout is None else timeout\n        self.callback = callback\n        self.interrupted = False\n        self.last_interrupt = 0.0",
        "detail": "packages.llm.local_llm.utils.keyboard",
        "documentation": {}
    },
    {
        "label": "LogFormatter",
        "kind": 6,
        "importPath": "packages.llm.local_llm.utils.log",
        "description": "packages.llm.local_llm.utils.log",
        "peekOfCode": "class LogFormatter(logging.Formatter):\n    \"\"\"\n    Colorized log formatter (inspired from https://stackoverflow.com/a/56944256)\n    Use LogFormatter.config() to enable it with the desired logging level.\n    \"\"\"\n    DefaultFormat = \"%(asctime)s | %(levelname)s | %(message)s\"\n    DefaultDateFormat = \"%H:%M:%S\"\n    DefaultColors = {\n        logging.DEBUG: ('light_grey', 'dark'),\n        logging.INFO: None,",
        "detail": "packages.llm.local_llm.utils.log",
        "documentation": {}
    },
    {
        "label": "TQDMRedirectStdOut",
        "kind": 6,
        "importPath": "packages.llm.local_llm.utils.log",
        "description": "packages.llm.local_llm.utils.log",
        "peekOfCode": "class TQDMRedirectStdOut(object):\n    file = None\n    def __init__(self, file):\n        self.file = file\n    def write(self, x):\n        if len(x.rstrip()) > 0:  # Avoid print() second call (useless \\n)\n            tqdm.tqdm.write(x, file=self.file)\n@contextlib.contextmanager\ndef tqdm_redirect_stdout():\n    save_stdout = sys.stdout",
        "detail": "packages.llm.local_llm.utils.log",
        "documentation": {}
    },
    {
        "label": "tqdm_redirect_stdout",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.log",
        "description": "packages.llm.local_llm.utils.log",
        "peekOfCode": "def tqdm_redirect_stdout():\n    save_stdout = sys.stdout\n    sys.stdout = TQDMRedirectStdOut(sys.stdout)\n    yield\n    sys.stdout = save_stdout",
        "detail": "packages.llm.local_llm.utils.log",
        "documentation": {}
    },
    {
        "label": "logging.SUCCESS",
        "kind": 5,
        "importPath": "packages.llm.local_llm.utils.log",
        "description": "packages.llm.local_llm.utils.log",
        "peekOfCode": "logging.SUCCESS = 35 # https://docs.python.org/3/library/logging.html#logging-levels\nclass LogFormatter(logging.Formatter):\n    \"\"\"\n    Colorized log formatter (inspired from https://stackoverflow.com/a/56944256)\n    Use LogFormatter.config() to enable it with the desired logging level.\n    \"\"\"\n    DefaultFormat = \"%(asctime)s | %(levelname)s | %(message)s\"\n    DefaultDateFormat = \"%H:%M:%S\"\n    DefaultColors = {\n        logging.DEBUG: ('light_grey', 'dark'),",
        "detail": "packages.llm.local_llm.utils.log",
        "documentation": {}
    },
    {
        "label": "ONNXRuntimeModel",
        "kind": 6,
        "importPath": "packages.llm.local_llm.utils.model",
        "description": "packages.llm.local_llm.utils.model",
        "peekOfCode": "class ONNXRuntimeModel:\n    \"\"\"\n    Base class for OnnxRuntime models.\n    \"\"\"\n    def __init__(self, model, providers='CUDAExecutionProvider', debug=False, **kwargs):\n        \"\"\"\n        Load an ONNX Runtime model.\n        \"\"\"\n        self.model_path = model\n        if isinstance(providers, str):",
        "detail": "packages.llm.local_llm.utils.model",
        "documentation": {}
    },
    {
        "label": "download_model",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.model",
        "description": "packages.llm.local_llm.utils.model",
        "peekOfCode": "def download_model(model, type='model', cache_dir='$TRANSFORMERS_CACHE', use_safetensors=False, **kwargs):\n    \"\"\"\n    Get the local path to a cached model or file in the cache_dir, or download it from HuggingFace Hub if needed.\n    If the asset is private and authentication is required, set the HUGGINGFACE_TOKEN environment variable.\n    cache_dir is where the model gets downloaded to - by default, set to $TRANSFORMERS_CACHE (/data/models/huggingface)\n    By default, the PyTorch .bin weights will be downloaded instead of the .safetensors (use_safetensors=False)\n    \"\"\"\n    token = os.environ.get('HUGGINGFACE_TOKEN', os.environ.get('HUGGING_FACE_HUB_TOKEN'))\n    if token:\n        login(token=token)",
        "detail": "packages.llm.local_llm.utils.model",
        "documentation": {}
    },
    {
        "label": "default_model_api",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.model",
        "description": "packages.llm.local_llm.utils.model",
        "peekOfCode": "def default_model_api(model_path, quant_path=None):\n    \"\"\"\n    Given the local path to a model, determine the type of API to use to load it.\n    TODO check the actual model files / configs instead of just parsing the paths\n    \"\"\"\n    if quant_path:\n        quant_api = default_model_api(quant_path)\n        if quant_api != 'hf':\n            return quant_api\n    model_path = model_path.lower()",
        "detail": "packages.llm.local_llm.utils.model",
        "documentation": {}
    },
    {
        "label": "load_prompts",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.prompts",
        "description": "packages.llm.local_llm.utils.prompts",
        "peekOfCode": "def load_prompts(prompts):\n    \"\"\"\n    Load prompts from a list of txt or json files\n    (or if these are strings, just return the strings)\n    \"\"\"\n    if prompts is None:\n        return None\n    if isinstance(prompts, str):\n        prompts = [prompts]\n    prompt_list = []",
        "detail": "packages.llm.local_llm.utils.prompts",
        "documentation": {}
    },
    {
        "label": "DefaultChatPrompts",
        "kind": 5,
        "importPath": "packages.llm.local_llm.utils.prompts",
        "description": "packages.llm.local_llm.utils.prompts",
        "peekOfCode": "DefaultChatPrompts = [\n    \"What is the weather forecast today?\",\n    \"What is the fable involving a fox and grapes?\",\n    \"What's a good recipe for making tabouli?\",\n    \"How do I allocate memory in C?\",\n    \"Implement a Python function to compute the Fibonacci numbers.\",\n    \"What is the product of 9 and 8?\",\n    \"Is Pluto really a planet or not?\",\n    \"When was the Hoover Dam built?\",\n    \"What's a training plan to run a marathon?\",",
        "detail": "packages.llm.local_llm.utils.prompts",
        "documentation": {}
    },
    {
        "label": "DefaultCompletionPrompts",
        "kind": 5,
        "importPath": "packages.llm.local_llm.utils.prompts",
        "description": "packages.llm.local_llm.utils.prompts",
        "peekOfCode": "DefaultCompletionPrompts = [\n    \"Once upon a time,\",\n    \"A great place to live is\",\n    \"In a world where dreams are shared,\",\n    \"The weather forecast today is\",\n    \"Large language models are\",\n    \"Space exploration is exciting\",\n    \"The history of the Hoover Dam is\",\n    \"San Fransisco is a city in\",\n    \"To train for running a marathon,\",",
        "detail": "packages.llm.local_llm.utils.prompts",
        "documentation": {}
    },
    {
        "label": "print_table",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.table",
        "description": "packages.llm.local_llm.utils.table",
        "peekOfCode": "def print_table(rows, header=None, footer=None, color='green', attrs=None):\n    \"\"\"\n    Print a table from a list[list] of rows/columns, or a 2-column dict \n    where the keys are column 1, and the values are column 2.\n    Header is a list of columns or rows that are inserted at the top.\n    Footer is a list of columns or rows that are added to the end.\n    color names and style attributes are from termcolor library:\n      https://github.com/termcolor/termcolor#text-properties\n    \"\"\"\n    if isinstance(rows, dict):",
        "detail": "packages.llm.local_llm.utils.table",
        "documentation": {}
    },
    {
        "label": "cudaArrayInterface",
        "kind": 6,
        "importPath": "packages.llm.local_llm.utils.tensor",
        "description": "packages.llm.local_llm.utils.tensor",
        "peekOfCode": "class cudaArrayInterface():\n    \"\"\"\n    Exposes __cuda_array_interface__ - typically used as a temporary view into a larger buffer\n    https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html\n    \"\"\"\n    def __init__(self, data, shape, dtype=np.float32):\n        if dtype == np.float32:\n            typestr = 'f4'\n        elif dtype == np.float64:\n            typestr = 'f8'",
        "detail": "packages.llm.local_llm.utils.tensor",
        "documentation": {}
    },
    {
        "label": "torch_dtype",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.tensor",
        "description": "packages.llm.local_llm.utils.tensor",
        "peekOfCode": "def torch_dtype(dtype):\n    \"\"\"\n    Convert numpy.dtype or str to torch.dtype\n    \"\"\"\n    return torch_dtype_dict[str(dtype)]\ndef convert_dtype(dtype, to='np'):\n    \"\"\"\n    Convert a string, numpy type, or torch.dtype to either numpy or PyTorch\n    \"\"\"\n    if to == 'pt':",
        "detail": "packages.llm.local_llm.utils.tensor",
        "documentation": {}
    },
    {
        "label": "convert_dtype",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.tensor",
        "description": "packages.llm.local_llm.utils.tensor",
        "peekOfCode": "def convert_dtype(dtype, to='np'):\n    \"\"\"\n    Convert a string, numpy type, or torch.dtype to either numpy or PyTorch\n    \"\"\"\n    if to == 'pt':\n        if isinstance(dtype, torch.dtype):\n            return dtype\n        else:\n            return torch_dtype(dtype)\n    elif to == 'np':",
        "detail": "packages.llm.local_llm.utils.tensor",
        "documentation": {}
    },
    {
        "label": "convert_tensor",
        "kind": 2,
        "importPath": "packages.llm.local_llm.utils.tensor",
        "description": "packages.llm.local_llm.utils.tensor",
        "peekOfCode": "def convert_tensor(tensor, return_tensors='pt', device=None, dtype=None, **kwargs):\n    \"\"\"\n    Convert tensors between numpy/torch/ect\n    \"\"\"\n    if tensor is None:\n        return None\n    if isinstance(tensor, np.ndarray):\n        if return_tensors == 'np':\n            return tensor\n        elif return_tensors == 'pt':",
        "detail": "packages.llm.local_llm.utils.tensor",
        "documentation": {}
    },
    {
        "label": "torch_dtype_dict",
        "kind": 5,
        "importPath": "packages.llm.local_llm.utils.tensor",
        "description": "packages.llm.local_llm.utils.tensor",
        "peekOfCode": "torch_dtype_dict = {\n    'bool'       : torch.bool,\n    'uint8'      : torch.uint8,\n    'int8'       : torch.int8,\n    'int16'      : torch.int16,\n    'int32'      : torch.int32,\n    'int64'      : torch.int64,\n    'float16'    : torch.float16,\n    'float32'    : torch.float32,\n    'float64'    : torch.float64,",
        "detail": "packages.llm.local_llm.utils.tensor",
        "documentation": {}
    },
    {
        "label": "CLIPModel",
        "kind": 6,
        "importPath": "packages.llm.local_llm.vision.clip",
        "description": "packages.llm.local_llm.vision.clip",
        "peekOfCode": "class CLIPModel():\n    \"\"\"\n    CLIP feature extractor and projector for generating image embeddings.\n    \"\"\"\n    @staticmethod\n    def from_pretrained(model=\"openai/clip-vit-large-patch14-336\", use_cache=True, **kwargs):\n        global _clip_model_cache\n        if use_cache and model in _clip_model_cache:\n            return _clip_model_cache[model]\n        instance = CLIPModel(model, **kwargs)",
        "detail": "packages.llm.local_llm.vision.clip",
        "documentation": {}
    },
    {
        "label": "_clip_model_cache",
        "kind": 5,
        "importPath": "packages.llm.local_llm.vision.clip",
        "description": "packages.llm.local_llm.vision.clip",
        "peekOfCode": "_clip_model_cache = {}\nclass CLIPModel():\n    \"\"\"\n    CLIP feature extractor and projector for generating image embeddings.\n    \"\"\"\n    @staticmethod\n    def from_pretrained(model=\"openai/clip-vit-large-patch14-336\", use_cache=True, **kwargs):\n        global _clip_model_cache\n        if use_cache and model in _clip_model_cache:\n            return _clip_model_cache[model]",
        "detail": "packages.llm.local_llm.vision.clip",
        "documentation": {}
    },
    {
        "label": "CLIPImageEmbedding",
        "kind": 6,
        "importPath": "packages.llm.local_llm.vision.clip_hf",
        "description": "packages.llm.local_llm.vision.clip_hf",
        "peekOfCode": "class CLIPImageEmbedding():\n    \"\"\"\n    CLIP feature extractor and projector for generating image embeddings.\n    \"\"\"\n    @staticmethod\n    def from_pretrained(model=\"openai/clip-vit-large-patch14-336\", dtype=torch.float32, use_cache=True, **kwargs):\n        global _clip_model_cache\n        if use_cache and model in _clip_model_cache['image']:\n            return _clip_model_cache['image'][model]\n        inst = CLIPImageEmbedding(model, dtype=dtype, **kwargs)",
        "detail": "packages.llm.local_llm.vision.clip_hf",
        "documentation": {}
    },
    {
        "label": "_clip_model_cache",
        "kind": 5,
        "importPath": "packages.llm.local_llm.vision.clip_hf",
        "description": "packages.llm.local_llm.vision.clip_hf",
        "peekOfCode": "_clip_model_cache = dict(image={}, text={})\nclass CLIPImageEmbedding():\n    \"\"\"\n    CLIP feature extractor and projector for generating image embeddings.\n    \"\"\"\n    @staticmethod\n    def from_pretrained(model=\"openai/clip-vit-large-patch14-336\", dtype=torch.float32, use_cache=True, **kwargs):\n        global _clip_model_cache\n        if use_cache and model in _clip_model_cache['image']:\n            return _clip_model_cache['image'][model]",
        "detail": "packages.llm.local_llm.vision.clip_hf",
        "documentation": {}
    },
    {
        "label": "MMProjector",
        "kind": 6,
        "importPath": "packages.llm.local_llm.vision.mm_projector",
        "description": "packages.llm.local_llm.vision.mm_projector",
        "peekOfCode": "class MMProjector():\n    \"\"\"\n    Multimodal projector MLP used by Llava and other Vision-Language Models\n    to map from CLIP vision embedding space to the LLM's word embedding space.\n    \"\"\"\n    @staticmethod\n    def from_pretrained(model, dtype=torch.float16):\n        \"\"\"\n        Load the projector from the HuggingFace Transformers model (Llava)\n        If the model directory doesn't already have mm_projector.bin, its",
        "detail": "packages.llm.local_llm.vision.mm_projector",
        "documentation": {}
    },
    {
        "label": "WebServer",
        "kind": 6,
        "importPath": "packages.llm.local_llm.web.server",
        "description": "packages.llm.local_llm.web.server",
        "peekOfCode": "class WebServer():\n    \"\"\"\n    HTTP/HTTPS Flask webserver with websocket messaging.\n    Use this by either creating an instance and providing msg_callback,\n    or inherit from it and implement on_message() in a subclass.\n    You can also add Flask routes to Webserver.app before start() is called.\n    TODO:  multi-client?\n           remove send queue because the connection dropping is messing things up\n           instead, have one master self.websocket that send_message() calls directly\n    \"\"\"",
        "detail": "packages.llm.local_llm.web.server",
        "documentation": {}
    },
    {
        "label": "SendFromDirectory",
        "kind": 6,
        "importPath": "packages.llm.local_llm.web.server",
        "description": "packages.llm.local_llm.web.server",
        "peekOfCode": "class SendFromDirectory():\n    def __init__(self, root):\n        self.root = root\n    def send(self, path):\n        return flask.send_from_directory(self.root, path)",
        "detail": "packages.llm.local_llm.web.server",
        "documentation": {}
    },
    {
        "label": "Agent",
        "kind": 6,
        "importPath": "packages.llm.local_llm.agent",
        "description": "packages.llm.local_llm.agent",
        "peekOfCode": "class Agent():\n    \"\"\"\n    Agents create/manage a pipeline of plugins\n    \"\"\"\n    def __init__(self, pipeline=[], **kwargs):\n        \"\"\"\n        pipeline should be a list of source plugins from the graph\n        \"\"\"\n        if isinstance(pipeline, Plugin):\n            self.pipeline = [pipeline]",
        "detail": "packages.llm.local_llm.agent",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "kind": 2,
        "importPath": "packages.llm.local_llm.agent",
        "description": "packages.llm.local_llm.agent",
        "peekOfCode": "def Pipeline(plugins):\n    \"\"\"\n    Connect the `plugins` list feed-forward style where each is an input to the next.\n    This uses plugin.add(), but specifying pipelines in list notation can be cleaner.\n    Returns the first plugin in the pipeline, from which other plugins can be found.\n    \"\"\"\n    if len(plugins) == 0:\n        return None\n    for i in range(len(plugins)-1):\n        plugins[i].add(plugins[i+1])",
        "detail": "packages.llm.local_llm.agent",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.local_llm.completion",
        "description": "packages.llm.local_llm.completion",
        "peekOfCode": "parser = ArgParser()\nparser.add_argument(\"--no-streaming\", action=\"store_true\", help=\"wait to output entire reply instead of token by token\")\nargs = parser.parse_args()\nprompts = load_prompts(args.prompt)\ninterrupt = KeyboardInterrupt()\n# load model\nmodel = LocalLM.from_pretrained(\n    args.model, \n    quant=args.quant, \n    api=args.api",
        "detail": "packages.llm.local_llm.completion",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.local_llm.completion",
        "description": "packages.llm.local_llm.completion",
        "peekOfCode": "args = parser.parse_args()\nprompts = load_prompts(args.prompt)\ninterrupt = KeyboardInterrupt()\n# load model\nmodel = LocalLM.from_pretrained(\n    args.model, \n    quant=args.quant, \n    api=args.api\n)\nwhile True: ",
        "detail": "packages.llm.local_llm.completion",
        "documentation": {}
    },
    {
        "label": "prompts",
        "kind": 5,
        "importPath": "packages.llm.local_llm.completion",
        "description": "packages.llm.local_llm.completion",
        "peekOfCode": "prompts = load_prompts(args.prompt)\ninterrupt = KeyboardInterrupt()\n# load model\nmodel = LocalLM.from_pretrained(\n    args.model, \n    quant=args.quant, \n    api=args.api\n)\nwhile True: \n    # get the next prompt from the list, or from the user interactivey",
        "detail": "packages.llm.local_llm.completion",
        "documentation": {}
    },
    {
        "label": "interrupt",
        "kind": 5,
        "importPath": "packages.llm.local_llm.completion",
        "description": "packages.llm.local_llm.completion",
        "peekOfCode": "interrupt = KeyboardInterrupt()\n# load model\nmodel = LocalLM.from_pretrained(\n    args.model, \n    quant=args.quant, \n    api=args.api\n)\nwhile True: \n    # get the next prompt from the list, or from the user interactivey\n    if isinstance(prompts, list):",
        "detail": "packages.llm.local_llm.completion",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.local_llm.completion",
        "description": "packages.llm.local_llm.completion",
        "peekOfCode": "model = LocalLM.from_pretrained(\n    args.model, \n    quant=args.quant, \n    api=args.api\n)\nwhile True: \n    # get the next prompt from the list, or from the user interactivey\n    if isinstance(prompts, list):\n        if len(prompts) > 0:\n            user_prompt = prompts.pop(0)",
        "detail": "packages.llm.local_llm.completion",
        "documentation": {}
    },
    {
        "label": "LocalLM",
        "kind": 6,
        "importPath": "packages.llm.local_llm.local_llm",
        "description": "packages.llm.local_llm.local_llm",
        "peekOfCode": "class LocalLM():\n    \"\"\"\n    Base class for local LLM APIs. It defines common Huggingface-like interfaces for\n    model loading, text generation, chat, tokenization/detokenization, and streaming.\n    It also supports vision models like Llava and generating image embeddings with CLIP.\n    Supported API backends include: AutoGPTQ, AWQ, MLC (TODO llama.cpp, exllama2)\n    Use LocalLM.from_pretrained() rather than instantiating this class directly.\n    \"\"\"\n    @staticmethod\n    def from_pretrained(model, api=None, **kwargs):",
        "detail": "packages.llm.local_llm.local_llm",
        "documentation": {}
    },
    {
        "label": "Plugin",
        "kind": 6,
        "importPath": "packages.llm.local_llm.plugin",
        "description": "packages.llm.local_llm.plugin",
        "peekOfCode": "class Plugin(threading.Thread):\n    \"\"\"\n    Base class for plugins that process incoming/outgoing data from connections\n    with other plugins, forming a pipeline or graph.  Plugins can run either\n    single-threaded or in an independent thread that processes data out of a queue.\n    Frequent categories of plugins:\n      * sources:  text prompts, images/video\n      * llm_queries, RAG, dynamic LLM calls, image postprocessors\n      * outputs:  print to stdout, save images/video\n    Parameters:",
        "detail": "packages.llm.local_llm.plugin",
        "documentation": {}
    },
    {
        "label": "get_max_rss",
        "kind": 2,
        "importPath": "packages.llm.minigpt4.benchmark",
        "description": "packages.llm.minigpt4.benchmark",
        "peekOfCode": "def get_max_rss():  # peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \nminigpt4_chatbot = minigpt4_library.MiniGPT4ChatBot(args.model_path, args.llm_model_path, verbosity=minigpt4_library.Verbosity.DEBUG) # SILENT, ERR, INFO, DEBUG\nmodel_name=f\"{os.path.basename(args.model_path)}+{os.path.basename(args.llm_model_path)}\"\nprint(f\"-- opening {args.image}\")\nimage = Image.open(args.image).convert('RGB')\navg_encoder=0\navg_latency=0\navg_tokens_sec=0\nfor run in range(args.runs + args.warmup):",
        "detail": "packages.llm.minigpt4.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.minigpt4.benchmark",
        "description": "packages.llm.minigpt4.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('model_path', help='Path to model file')\nparser.add_argument('llm_model_path', help='Path to llm model file')\nparser.add_argument('-p', '--prompt', action='append', nargs='*')\nparser.add_argument('-i', '--image', default='/data/images/hoover.jpg', help=\"Path to the image to test\")\nparser.add_argument('-r', '--runs', type=int, default=2, help=\"Number of inferencing runs to do (for timing)\")\nparser.add_argument('-w', '--warmup', type=int, default=1, help='the number of warmup iterations')\nparser.add_argument('-s', '--save', type=str, default='', help='CSV file to save benchmarking results to')\nparser.add_argument('--max-new-tokens', type=int, default=64, help=\"Limit the length of LLM output\")\nargs = parser.parse_args()",
        "detail": "packages.llm.minigpt4.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.minigpt4.benchmark",
        "description": "packages.llm.minigpt4.benchmark",
        "peekOfCode": "args = parser.parse_args()\nif not args.prompt:\n    args.prompt = [\n        \"What does the sign in the image say?\",\n        \"How far is the exit?\",\n        \"What kind of environment is it in?\",\n        \"Does it look like it's going to rain?\",\n    ]\nelse:\n    args.prompt = [x[0] for x in args.prompt]",
        "detail": "packages.llm.minigpt4.benchmark",
        "documentation": {}
    },
    {
        "label": "minigpt4_chatbot",
        "kind": 5,
        "importPath": "packages.llm.minigpt4.benchmark",
        "description": "packages.llm.minigpt4.benchmark",
        "peekOfCode": "minigpt4_chatbot = minigpt4_library.MiniGPT4ChatBot(args.model_path, args.llm_model_path, verbosity=minigpt4_library.Verbosity.DEBUG) # SILENT, ERR, INFO, DEBUG\nmodel_name=f\"{os.path.basename(args.model_path)}+{os.path.basename(args.llm_model_path)}\"\nprint(f\"-- opening {args.image}\")\nimage = Image.open(args.image).convert('RGB')\navg_encoder=0\navg_latency=0\navg_tokens_sec=0\nfor run in range(args.runs + args.warmup):\n    time_begin=time.perf_counter()\n    minigpt4_chatbot.upload_image(image)",
        "detail": "packages.llm.minigpt4.benchmark",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "packages.llm.minigpt4.benchmark",
        "description": "packages.llm.minigpt4.benchmark",
        "peekOfCode": "image = Image.open(args.image).convert('RGB')\navg_encoder=0\navg_latency=0\navg_tokens_sec=0\nfor run in range(args.runs + args.warmup):\n    time_begin=time.perf_counter()\n    minigpt4_chatbot.upload_image(image)\n    time_encoder=time.perf_counter() - time_begin\n    print(f\"{model_name} encoder:  {time_encoder:.3f} seconds\\n\")\n    if run >= args.warmup:",
        "detail": "packages.llm.minigpt4.benchmark",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.llm.minigpt4.config",
        "description": "packages.llm.minigpt4.config",
        "peekOfCode": "package['build_args'] = {\n    'CUDA_ARCHITECTURES': ';'.join([str(x) for x in CUDA_ARCHITECTURES])\n}",
        "detail": "packages.llm.minigpt4.config",
        "documentation": {}
    },
    {
        "label": "load_prompts",
        "kind": 2,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "def load_prompts(prompts):\n    \"\"\"\n    Load prompts from a list of txt or json files\n    (or if these are strings, just return the strings)\n    \"\"\"\n    prompt_list = []\n    for prompt in prompts:\n        ext = os.path.splitext(prompt)[1]\n        if ext == '.json':\n            with open(prompt) as file:",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default=\"Llama-2-7b-chat-hf-q4f16_1\")\nparser.add_argument('--model-lib-path', type=str, default=None)\nparser.add_argument(\"--prompt\", action='append', nargs='*')\nparser.add_argument(\"--chat\", action=\"store_true\")\nparser.add_argument(\"--streaming\", action=\"store_true\")\nparser.add_argument(\"--max-new-tokens\", type=int, default=128)\nparser.add_argument(\"--max-num-prompts\", type=int, default=None)\nparser.add_argument('--save', type=str, default='', help='CSV file to save benchmarking results to')\nargs = parser.parse_args()",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "args = parser.parse_args()\n#if 'chat' in args.model.lower() and not args.chat:\n#    args.chat = True\nif not args.prompt:\n    if args.chat:  # https://modal.com/docs/guide/ex/vllm_inference\n        args.prompt = [\n            \"What is the meaning of life?\",\n            \"How many points did you list out?\",\n            \"What is the weather forecast today?\",\n            \"What is the fable involving a fox and grapes?\",",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "args.prompt",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "args.prompt = load_prompts(args.prompt)\nif args.max_num_prompts:\n    args.prompt = args.prompt[:args.max_num_prompts]\nprint(f\"-- loading {args.model}\")\n#conv_config = ConvConfig(system='Please show as much happiness as you can when talking to me.')\n#chat_config = ChatConfig(max_gen_len=256, conv_config=conv_config)\n#conv_config = ConvConfig(system='Please show as much sadness as you can when talking to me.')\n#chat_config = ChatConfig(max_gen_len=128, conv_config=conv_config)\n#cm.reset_chat(chat_config)\ncfg = ChatConfig(max_gen_len=args.max_new_tokens)",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "#conv_config",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "#conv_config = ConvConfig(system='Please show as much happiness as you can when talking to me.')\n#chat_config = ChatConfig(max_gen_len=256, conv_config=conv_config)\n#conv_config = ConvConfig(system='Please show as much sadness as you can when talking to me.')\n#chat_config = ChatConfig(max_gen_len=128, conv_config=conv_config)\n#cm.reset_chat(chat_config)\ncfg = ChatConfig(max_gen_len=args.max_new_tokens)\nif not args.chat:\n    cfg.conv_template = 'LM'\ncm = ChatModule(model=args.model, model_lib_path=args.model_lib_path, chat_config=cfg)\navg_prefill_rate = 0",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "#chat_config",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "#chat_config = ChatConfig(max_gen_len=256, conv_config=conv_config)\n#conv_config = ConvConfig(system='Please show as much sadness as you can when talking to me.')\n#chat_config = ChatConfig(max_gen_len=128, conv_config=conv_config)\n#cm.reset_chat(chat_config)\ncfg = ChatConfig(max_gen_len=args.max_new_tokens)\nif not args.chat:\n    cfg.conv_template = 'LM'\ncm = ChatModule(model=args.model, model_lib_path=args.model_lib_path, chat_config=cfg)\navg_prefill_rate = 0\navg_prefill_time = 0",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "#conv_config",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "#conv_config = ConvConfig(system='Please show as much sadness as you can when talking to me.')\n#chat_config = ChatConfig(max_gen_len=128, conv_config=conv_config)\n#cm.reset_chat(chat_config)\ncfg = ChatConfig(max_gen_len=args.max_new_tokens)\nif not args.chat:\n    cfg.conv_template = 'LM'\ncm = ChatModule(model=args.model, model_lib_path=args.model_lib_path, chat_config=cfg)\navg_prefill_rate = 0\navg_prefill_time = 0\navg_decode_rate = 0",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "#chat_config",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "#chat_config = ChatConfig(max_gen_len=128, conv_config=conv_config)\n#cm.reset_chat(chat_config)\ncfg = ChatConfig(max_gen_len=args.max_new_tokens)\nif not args.chat:\n    cfg.conv_template = 'LM'\ncm = ChatModule(model=args.model, model_lib_path=args.model_lib_path, chat_config=cfg)\navg_prefill_rate = 0\navg_prefill_time = 0\navg_decode_rate = 0\navg_decode_time = 0",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "cfg = ChatConfig(max_gen_len=args.max_new_tokens)\nif not args.chat:\n    cfg.conv_template = 'LM'\ncm = ChatModule(model=args.model, model_lib_path=args.model_lib_path, chat_config=cfg)\navg_prefill_rate = 0\navg_prefill_time = 0\navg_decode_rate = 0\navg_decode_time = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "cm",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "cm = ChatModule(model=args.model, model_lib_path=args.model_lib_path, chat_config=cfg)\navg_prefill_rate = 0\navg_prefill_time = 0\navg_decode_rate = 0\navg_decode_time = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        num_input_tokens = prompt['num_tokens']\n        prompt = prompt['text']\n    else:",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_prefill_rate",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "avg_prefill_rate = 0\navg_prefill_time = 0\navg_decode_rate = 0\navg_decode_time = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        num_input_tokens = prompt['num_tokens']\n        prompt = prompt['text']\n    else:\n        num_input_tokens = cm.embed_text(prompt).shape[1]",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_prefill_time",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "avg_prefill_time = 0\navg_decode_rate = 0\navg_decode_time = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        num_input_tokens = prompt['num_tokens']\n        prompt = prompt['text']\n    else:\n        num_input_tokens = cm.embed_text(prompt).shape[1]\n        cm.reset_chat()",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_decode_rate",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "avg_decode_rate = 0\navg_decode_time = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        num_input_tokens = prompt['num_tokens']\n        prompt = prompt['text']\n    else:\n        num_input_tokens = cm.embed_text(prompt).shape[1]\n        cm.reset_chat()\n    print(f\"\\nPROMPT:  {prompt}\\n\")",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_decode_time",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "avg_decode_time = 0\nfor i, prompt in enumerate(args.prompt):\n    if isinstance(prompt, dict):\n        num_input_tokens = prompt['num_tokens']\n        prompt = prompt['text']\n    else:\n        num_input_tokens = cm.embed_text(prompt).shape[1]\n        cm.reset_chat()\n    print(f\"\\nPROMPT:  {prompt}\\n\")\n    if args.streaming:",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "kind": 5,
        "importPath": "packages.llm.mlc.benchmark",
        "description": "packages.llm.mlc.benchmark",
        "peekOfCode": "memory_usage = (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  # https://stackoverflow.com/a/7669482\nprint(f\"Peak memory usage:  {memory_usage:.2f} MB\")\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, api, model, precision, input_tokens, output_tokens, prefill_time, prefill_rate, decode_time, decode_rate, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, mlc, \")\n        file.write(f\"{args.model}, {args.model.split('-')[-1]}, {num_input_tokens}, {args.max_new_tokens}, \")\n        file.write(f\"{avg_prefill_time}, {avg_prefill_rate}, {avg_decode_time}, {avg_decode_rate}, {memory_usage}\\n\")",
        "detail": "packages.llm.mlc.benchmark",
        "documentation": {}
    },
    {
        "label": "mlc",
        "kind": 2,
        "importPath": "packages.llm.mlc.config",
        "description": "packages.llm.mlc.config",
        "peekOfCode": "def mlc(commit, patch=None, version='0.1', tvm='0.15', llvm=17, requires=None, default=False):\n    pkg = package.copy()\n    if default:\n        pkg['alias'] = 'mlc'\n    if requires:\n        pkg['requires'] = requires   \n    pkg['name'] = f'mlc:{version}'\n    pkg['notes'] = f\"[mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm/tree/{commit}) commit SHA [`{commit}`](https://github.com/mlc-ai/mlc-llm/tree/{commit})\"\n    pkg['build_args'] = {\n        'MLC_VERSION': version,",
        "detail": "packages.llm.mlc.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.mlc.config",
        "description": "packages.llm.mlc.config",
        "peekOfCode": "package = [\n    mlc('51fb0f4', 'patches/51fb0f4.diff', version='0.1.0', tvm='0.15.0', default=(L4T_VERSION.major == 35), requires='==35.*'), # 12/15/2023\n    mlc('607dc5a', 'patches/607dc5a.diff', version='0.1.0', tvm='0.15.0', default=(L4T_VERSION.major >= 36), requires='>=36'),  # 02/27/2024\n    mlc('3403a4e', 'patches/3403a4e.diff', version='0.1.1', tvm='0.16.0', requires='>=36')  # 4/15/2024\n]\n#latest_sha = github_latest_commit(repo, branch='main')\n#log_debug('-- MLC latest commit:', latest_sha)\n'''\npackage = [\n    mlc('731616e', 'patches/3feed05.diff', tag='dev'),",
        "detail": "packages.llm.mlc.config",
        "documentation": {}
    },
    {
        "label": "#latest_sha",
        "kind": 5,
        "importPath": "packages.llm.mlc.config",
        "description": "packages.llm.mlc.config",
        "peekOfCode": "#latest_sha = github_latest_commit(repo, branch='main')\n#log_debug('-- MLC latest commit:', latest_sha)\n'''\npackage = [\n    mlc('731616e', 'patches/3feed05.diff', tag='dev'),\n    mlc('9bf5723', 'patches/9bf5723.diff', requires='==35.*'), # 10/20/2023\n    mlc('51fb0f4', 'patches/51fb0f4.diff', default=(L4T_VERSION.major == 35)), # 12/15/2023\n    mlc('3feed05', 'patches/3feed05.diff', requires='>=36'), # 02/08/2024\n    #mlc('6cf63bb', 'patches/3feed05.diff', requires='>=36'),  # 02/16/2024\n    #mlc('c30348a', 'patches/3feed05.diff', requires='>=36'),  # 02/19/2024",
        "detail": "packages.llm.mlc.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.mlc.config",
        "description": "packages.llm.mlc.config",
        "peekOfCode": "package = [\n    mlc('731616e', 'patches/3feed05.diff', tag='dev'),\n    mlc('9bf5723', 'patches/9bf5723.diff', requires='==35.*'), # 10/20/2023\n    mlc('51fb0f4', 'patches/51fb0f4.diff', default=(L4T_VERSION.major == 35)), # 12/15/2023\n    mlc('3feed05', 'patches/3feed05.diff', requires='>=36'), # 02/08/2024\n    #mlc('6cf63bb', 'patches/3feed05.diff', requires='>=36'),  # 02/16/2024\n    #mlc('c30348a', 'patches/3feed05.diff', requires='>=36'),  # 02/19/2024\n    #mlc('a2d9eea', 'patches/3feed05.diff', requires='>=36'),  # 02/19/2024\n    mlc('5584cac', 'patches/3feed05.diff', requires='>=36'),   # 02/21/2024\n    mlc('607dc5a', 'patches/607dc5a.diff', default=(L4T_VERSION.major >= 36), requires='>=36'),  # 02/27/2024",
        "detail": "packages.llm.mlc.config",
        "documentation": {}
    },
    {
        "label": "NanoLLM",
        "kind": 2,
        "importPath": "packages.llm.nano_llm.config",
        "description": "packages.llm.nano_llm.config",
        "peekOfCode": "def NanoLLM(version, branch=None, requires=None, default=False, ros=['foxy', 'galactic', 'humble', 'iron']):\n    pkg = package.copy()\n    pkg['name'] = f\"nano_llm:{version}\"\n    if default:\n        pkg['alias'] = 'nano_llm'\n    if requires:\n        pkg['requires'] = requires   \n    if L4T_VERSION.major >= 36:\n        pkg['depends'] = ['awq'] + pkg['depends'] + ['whisper_trt']\n    if not branch:",
        "detail": "packages.llm.nano_llm.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.nano_llm.config",
        "description": "packages.llm.nano_llm.config",
        "peekOfCode": "package = [\n    NanoLLM('main', default=True),\n    NanoLLM('24.4'),\n    NanoLLM('24.4.1'),\n    NanoLLM('24.5'),\n    NanoLLM('24.5.1'),\n    NanoLLM('24.6'),\n    NanoLLM('24.7'),\n]",
        "detail": "packages.llm.nano_llm.config",
        "documentation": {}
    },
    {
        "label": "get_max_memory_usage",
        "kind": 2,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "def get_max_memory_usage(PID: str = \"self\") -> None:\n    ''' Maximum memory usage in bytes '''\n    with open(f'/proc/{PID}/status', encoding='utf-8') as f:\n        memusage = f.read().split('VmPeak:')[1].split('\\n')[0][:-3]\n    return int(memusage.strip()) / 1024\ndef send_test_prompt(json_data: dict, url:str = \"\") -> requests.Response:\n    ''' send a test prompt to local ollama container '''\n    if not url:\n        url = \"127.0.0.1:11434\"\n    return requests.post(url, json=json.dumps(json_data))",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "send_test_prompt",
        "kind": 2,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "def send_test_prompt(json_data: dict, url:str = \"\") -> requests.Response:\n    ''' send a test prompt to local ollama container '''\n    if not url:\n        url = \"127.0.0.1:11434\"\n    return requests.post(url, json=json.dumps(json_data))\ndef run_benchmark(runs: int, json_data: dict, test_url: str = \"\") -> None:\n    ''' run the benchmark '''\n    time_avg = 0.0\n    for run in range(runs):\n        time_begin = time.perf_counter()",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "run_benchmark",
        "kind": 2,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "def run_benchmark(runs: int, json_data: dict, test_url: str = \"\") -> None:\n    ''' run the benchmark '''\n    time_avg = 0.0\n    for run in range(runs):\n        time_begin = time.perf_counter()\n        response = send_test_prompt(json_data, test_url)\n        time_elapsed = (time.perf_counter() - time_begin)\n        if not response.ok:\n            pp(f'received error code from api service: {response.status_code}')\n            continue",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PROMPT",
        "kind": 5,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "DEFAULT_PROMPT = {\n  \"model\": \"tinyllama\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"seed\": 123,\n    \"temperature\": 0\n  },\n  \"format\": \"json\",\n  \"stream\": False,\n}",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('-m', '--model', type=str, default='tinyllama', required=True, help=\"name of model to run\")\nparser.add_argument('-p', '--prompt', type=str, default=DEFAULT_PROMPT.get(\"prompt\"))\nparser.add_argument('--runs', type=int, default=2, help='the number of benchmark timing iterations')\nparser.add_argument('--OLLAMA_PID', type=str, default=\"\", required=True, help='the pid of the ollama process')\nargs = parser.parse_args()\nprint(args)\ndata = DEFAULT_PROMPT.copy()\ndata['prompt'] = args.prompt\ndef get_max_memory_usage(PID: str = \"self\") -> None:",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\ndata = DEFAULT_PROMPT.copy()\ndata['prompt'] = args.prompt\ndef get_max_memory_usage(PID: str = \"self\") -> None:\n    ''' Maximum memory usage in bytes '''\n    with open(f'/proc/{PID}/status', encoding='utf-8') as f:\n        memusage = f.read().split('VmPeak:')[1].split('\\n')[0][:-3]\n    return int(memusage.strip()) / 1024\ndef send_test_prompt(json_data: dict, url:str = \"\") -> requests.Response:",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "data = DEFAULT_PROMPT.copy()\ndata['prompt'] = args.prompt\ndef get_max_memory_usage(PID: str = \"self\") -> None:\n    ''' Maximum memory usage in bytes '''\n    with open(f'/proc/{PID}/status', encoding='utf-8') as f:\n        memusage = f.read().split('VmPeak:')[1].split('\\n')[0][:-3]\n    return int(memusage.strip()) / 1024\ndef send_test_prompt(json_data: dict, url:str = \"\") -> requests.Response:\n    ''' send a test prompt to local ollama container '''\n    if not url:",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "data['prompt']",
        "kind": 5,
        "importPath": "packages.llm.ollama.benchmark",
        "description": "packages.llm.ollama.benchmark",
        "peekOfCode": "data['prompt'] = args.prompt\ndef get_max_memory_usage(PID: str = \"self\") -> None:\n    ''' Maximum memory usage in bytes '''\n    with open(f'/proc/{PID}/status', encoding='utf-8') as f:\n        memusage = f.read().split('VmPeak:')[1].split('\\n')[0][:-3]\n    return int(memusage.strip()) / 1024\ndef send_test_prompt(json_data: dict, url:str = \"\") -> requests.Response:\n    ''' send a test prompt to local ollama container '''\n    if not url:\n        url = \"127.0.0.1:11434\"",
        "detail": "packages.llm.ollama.benchmark",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.llm.ollama.config",
        "description": "packages.llm.ollama.config",
        "peekOfCode": "package['build_args'] = {\n    'OLLAMA_REPO': 'ollama/ollama',\n    'OLLAMA_BRANCH': 'main',\n    'GOLANG_VERSION': '1.22.1',\n    'CMAKE_VERSION': '3.22.1',\n    'JETPACK_VERSION': str(JETPACK_VERSION),\n    'CMAKE_CUDA_ARCHITECTURES': ';'.join([str(x) for x in CUDA_ARCHITECTURES]),\n}",
        "detail": "packages.llm.ollama.config",
        "documentation": {}
    },
    {
        "label": "vla_mimicgen",
        "kind": 5,
        "importPath": "packages.llm.openvla.config",
        "description": "packages.llm.openvla.config",
        "peekOfCode": "vla_mimicgen = package.copy()\nvla_mimicgen['name'] = 'openvla:mimicgen'\nvla_mimicgen['depends'] = vla_mimicgen['depends'] + ['mimicgen']\npackage = [package, vla_mimicgen]",
        "detail": "packages.llm.openvla.config",
        "documentation": {}
    },
    {
        "label": "vla_mimicgen['name']",
        "kind": 5,
        "importPath": "packages.llm.openvla.config",
        "description": "packages.llm.openvla.config",
        "peekOfCode": "vla_mimicgen['name'] = 'openvla:mimicgen'\nvla_mimicgen['depends'] = vla_mimicgen['depends'] + ['mimicgen']\npackage = [package, vla_mimicgen]",
        "detail": "packages.llm.openvla.config",
        "documentation": {}
    },
    {
        "label": "vla_mimicgen['depends']",
        "kind": 5,
        "importPath": "packages.llm.openvla.config",
        "description": "packages.llm.openvla.config",
        "peekOfCode": "vla_mimicgen['depends'] = vla_mimicgen['depends'] + ['mimicgen']\npackage = [package, vla_mimicgen]",
        "detail": "packages.llm.openvla.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.openvla.config",
        "description": "packages.llm.openvla.config",
        "peekOfCode": "package = [package, vla_mimicgen]",
        "detail": "packages.llm.openvla.config",
        "documentation": {}
    },
    {
        "label": "processor",
        "kind": 5,
        "importPath": "packages.llm.openvla.test",
        "description": "packages.llm.openvla.test",
        "peekOfCode": "processor = AutoProcessor.from_pretrained(model, trust_remote_code=True)\nvla = AutoModelForVision2Seq.from_pretrained(\n    model, \n    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n    torch_dtype=torch.bfloat16, \n    low_cpu_mem_usage=True, \n    trust_remote_code=True\n).to(\"cuda:0\")\nprint(vla.config)\n# Grab image input & format prompt",
        "detail": "packages.llm.openvla.test",
        "documentation": {}
    },
    {
        "label": "vla",
        "kind": 5,
        "importPath": "packages.llm.openvla.test",
        "description": "packages.llm.openvla.test",
        "peekOfCode": "vla = AutoModelForVision2Seq.from_pretrained(\n    model, \n    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n    torch_dtype=torch.bfloat16, \n    low_cpu_mem_usage=True, \n    trust_remote_code=True\n).to(\"cuda:0\")\nprint(vla.config)\n# Grab image input & format prompt\nimage = Image.open(\"/data/images/lake.jpg\").convert('RGB')",
        "detail": "packages.llm.openvla.test",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "packages.llm.openvla.test",
        "description": "packages.llm.openvla.test",
        "peekOfCode": "image = Image.open(\"/data/images/lake.jpg\").convert('RGB')\nprompt = \"In: What action should the robot take to stop?\\nOut:\"\nprint('prompt:', prompt)\n# Predict Action (7-DoF; un-normalize for BridgeData V2)\ninputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\nprint('inputs:', list(inputs.keys()))\naction = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n# Execute...\nprint('action:', action)",
        "detail": "packages.llm.openvla.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "packages.llm.openvla.test",
        "description": "packages.llm.openvla.test",
        "peekOfCode": "prompt = \"In: What action should the robot take to stop?\\nOut:\"\nprint('prompt:', prompt)\n# Predict Action (7-DoF; un-normalize for BridgeData V2)\ninputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\nprint('inputs:', list(inputs.keys()))\naction = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n# Execute...\nprint('action:', action)",
        "detail": "packages.llm.openvla.test",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "packages.llm.openvla.test",
        "description": "packages.llm.openvla.test",
        "peekOfCode": "inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\nprint('inputs:', list(inputs.keys()))\naction = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n# Execute...\nprint('action:', action)",
        "detail": "packages.llm.openvla.test",
        "documentation": {}
    },
    {
        "label": "action",
        "kind": 5,
        "importPath": "packages.llm.openvla.test",
        "description": "packages.llm.openvla.test",
        "peekOfCode": "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n# Execute...\nprint('action:', action)",
        "detail": "packages.llm.openvla.test",
        "documentation": {}
    },
    {
        "label": "is_onnx",
        "kind": 2,
        "importPath": "packages.llm.optimum.test_gpt",
        "description": "packages.llm.optimum.test_gpt",
        "peekOfCode": "def is_onnx(model):\n    \"\"\"\n    Determine if the model has already been exported to ONNX\n    \"\"\"\n    if 'onnx' in model or model.startswith('optimum'):\n        return True  # check the name for models hosted online\n    if os.path.isdir(model):  # check file extensions for local models\n        for file in os.listdir(model):\n            if os.path.splitext(file)[1] == '.onnx':\n                return True",
        "detail": "packages.llm.optimum.test_gpt",
        "documentation": {}
    },
    {
        "label": "benchmark_gpt",
        "kind": 2,
        "importPath": "packages.llm.optimum.test_gpt",
        "description": "packages.llm.optimum.test_gpt",
        "peekOfCode": "def benchmark_gpt(model='distilgpt2', provider='TensorrtExecutionProvider',\n                  runs=25, warmup=10, do_sample=False, fp16=False, int8=False, \n                  output='', verbose=False, **kwargs):\n    \"\"\"\n    Run benchmarking on a text generation language model.\n    Models to try:  distilgpt2, optimum/gpt2, MBZUAI/LaMini-GPT-124M\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    memory_begin = process.memory_info().vms  # https://stackoverflow.com/a/21049737/6037395\n    print(f\"loading {model} with '{provider}'\")",
        "detail": "packages.llm.optimum.test_gpt",
        "documentation": {}
    },
    {
        "label": "quantize_gpt",
        "kind": 2,
        "importPath": "packages.llm.optimum.test_gpt",
        "description": "packages.llm.optimum.test_gpt",
        "peekOfCode": "def quantize_gpt(model='distilgpt2', provider='TensorrtExecutionProvider', output='', **kwargs):\n    \"\"\"\n    Apply static int8 quantization to model using the specified dataset for calibration\n    \"\"\"\n    if not output:\n        output = os.path.join('data/transformers', f'{os.path.basename(model)}-int8')\n    print(f\"loading {model} with '{provider}' for int8 quantization\")\n    onnx_model = ORTModelForCausalLM.from_pretrained(model,\n        export=not is_onnx(model), \n        use_cache=False",
        "detail": "packages.llm.optimum.test_gpt",
        "documentation": {}
    },
    {
        "label": "tensorrt_llm",
        "kind": 2,
        "importPath": "packages.llm.tensorrt_llm.config",
        "description": "packages.llm.tensorrt_llm.config",
        "peekOfCode": "def tensorrt_llm(version, branch=None, patch=None, src=None, depends=None, requires=None, default=False):\n    trt_llm = package.copy()\n    trt_llm['name'] = f'tensorrt_llm:{version}'\n    if not branch:\n        branch = 'v' + version\n        if len(branch.split('.')) < 3:\n            branch = branch + '.0'\n    if not patch:\n        patch = 'patches/empty.diff'\n    if not src:",
        "detail": "packages.llm.tensorrt_llm.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.tensorrt_llm.config",
        "description": "packages.llm.tensorrt_llm.config",
        "peekOfCode": "package = [\n    tensorrt_llm('0.11.dev0', src='sources/tensorrt_llm-0.11.0.dev0.tar.gz', requires=['==r36.*', '>=cu124'], default=True),\n    tensorrt_llm('0.10.dev0', src='sources/tensorrt_llm-0.10.0.dev0.tar.gz', requires=['==r36.*', '>=cu124'], default=False),\n    #tensorrt_llm('0.9.dev', '118b3d7', patch='patches/118b3d7.diff', requires=['==r36.*', '>=cu124'], default=False),\n    tensorrt_llm('0.5', patch='patches/0.5.diff', requires=['==r36.*', '==cu122'], default=True),\n]",
        "detail": "packages.llm.tensorrt_llm.config",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.llm.text-generation-inference.config",
        "description": "packages.llm.text-generation-inference.config",
        "peekOfCode": "package['build_args'] = {\n    'TORCH_CUDA_ARCH_LIST': ';'.join([f'{x/10:.1f}' for x in CUDA_ARCHITECTURES]),\n}",
        "detail": "packages.llm.text-generation-inference.config",
        "documentation": {}
    },
    {
        "label": "oobabooga",
        "kind": 2,
        "importPath": "packages.llm.text-generation-webui.config",
        "description": "packages.llm.text-generation-webui.config",
        "peekOfCode": "def oobabooga(version, branch=None, tag=None, sha=None, build_args=None, default=False):\n    twu = package.copy()\n    if default:\n        twu['alias'] = twu['name']\n    twu['name'] = f\"{twu['name']}:{version}\"\n    if branch:\n        ref = f\"refs/heads/{branch}\"\n    elif tag:\n        ref = f\"refs/tags/{tag}\"\n    elif sha:",
        "detail": "packages.llm.text-generation-webui.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.text-generation-webui.config",
        "description": "packages.llm.text-generation-webui.config",
        "peekOfCode": "package = [\n    oobabooga('main', branch='main', default=True),\n    oobabooga('1.7', tag='v1.7', build_args=dict(LD_PRELOAD_LIBS='/usr/local/lib/python3.8/dist-packages/sklearn/__check_build/../../scikit_learn.libs/libgomp-d22c30c5.so.1.0.0')),\n    oobabooga('6a7cd01', sha='6a7cd01ebf8021a8ee6da094643f09da41516ccd'), # last commit to support original server API\n]",
        "detail": "packages.llm.text-generation-webui.config",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default='distilgpt2')\nparser.add_argument('--prompt', type=str, default='Once upon a time,')\nparser.add_argument('--precision', type=str, default=None, choices=['fp32', 'fp16', 'fp4', 'int8'])\nparser.add_argument('--tokens', type=int, nargs='+', default=[128], help='number of output tokens to generate (not including the input prompt)')\nparser.add_argument('--token', type=str, default=os.environ.get('HUGGINGFACE_TOKEN', ''), help=\"HuggingFace account login token from https://huggingface.co/docs/hub/security-tokens (defaults to $HUGGINGFACE_TOKEN)\")\nparser.add_argument('--runs', type=int, default=2, help='the number of benchmark timing iterations')\nparser.add_argument('--warmup', type=int, default=2, help='the number of warmup iterations')\nparser.add_argument('--save', type=str, default='', help='CSV file to save benchmarking results to')\nargs = parser.parse_args()",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\n# select compute device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Running on device {device}')\n# log into huggingface hub\nif args.token:\n    print(\"Logging into HuggingFace Hub...\")\n    huggingface_hub.login(token=args.token)\n# detect the type of model it is",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Running on device {device}')\n# log into huggingface hub\nif args.token:\n    print(\"Logging into HuggingFace Hub...\")\n    huggingface_hub.login(token=args.token)\n# detect the type of model it is\nmodel_info = huggingface_hub.model_info(args.model)\nmodel_type = model_info.transformersInfo['auto_model']\nif model_type != 'AutoModelForCausalLM':",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "model_info = huggingface_hub.model_info(args.model)\nmodel_type = model_info.transformersInfo['auto_model']\nif model_type != 'AutoModelForCausalLM':\n    raise ValueError(f\"text-generation benchmark only supports CausalLM models (GPT,llama,ect) - {args.model} is {model_type}\")\n# end the prompt with a newline\n#args.prompt += '\\n'\n# create tokenizer\ntokenizer = AutoTokenizer.from_pretrained(args.model)\ninput_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\nprint('Input tokens:', input_ids, 'shape:', input_ids.shape)",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "model_type = model_info.transformersInfo['auto_model']\nif model_type != 'AutoModelForCausalLM':\n    raise ValueError(f\"text-generation benchmark only supports CausalLM models (GPT,llama,ect) - {args.model} is {model_type}\")\n# end the prompt with a newline\n#args.prompt += '\\n'\n# create tokenizer\ntokenizer = AutoTokenizer.from_pretrained(args.model)\ninput_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\nprint('Input tokens:', input_ids, 'shape:', input_ids.shape)\n# setup precision args",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(args.model)\ninput_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\nprint('Input tokens:', input_ids, 'shape:', input_ids.shape)\n# setup precision args\nkwargs = {}\nif args.precision == 'int8':\n    kwargs['load_in_8bit'] = True\n    #kwargs['int8_threshold'] = 0   # https://github.com/TimDettmers/bitsandbytes/issues/6#issuecomment-1225990890\nelif args.precision == 'fp4':\n    kwargs['load_in_4bit'] = True",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "input_ids",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "input_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\nprint('Input tokens:', input_ids, 'shape:', input_ids.shape)\n# setup precision args\nkwargs = {}\nif args.precision == 'int8':\n    kwargs['load_in_8bit'] = True\n    #kwargs['int8_threshold'] = 0   # https://github.com/TimDettmers/bitsandbytes/issues/6#issuecomment-1225990890\nelif args.precision == 'fp4':\n    kwargs['load_in_4bit'] = True\nelif args.precision == 'fp16':",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "kwargs",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "kwargs = {}\nif args.precision == 'int8':\n    kwargs['load_in_8bit'] = True\n    #kwargs['int8_threshold'] = 0   # https://github.com/TimDettmers/bitsandbytes/issues/6#issuecomment-1225990890\nelif args.precision == 'fp4':\n    kwargs['load_in_4bit'] = True\nelif args.precision == 'fp16':\n    kwargs['torch_dtype'] = torch.float16\nelif args.precision == 'fp32':\n    kwargs['torch_dtype'] = torch.float32",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.transformers.huggingface-benchmark",
        "description": "packages.llm.transformers.huggingface-benchmark",
        "peekOfCode": "model = AutoModelForCausalLM.from_pretrained(args.model, device_map=device, **kwargs) #AutoModelForCausalLM.from_pretrained(args.model, **kwargs)\n#if args.precision == 'fp32' or args.precision == 'fp16':\n#    model = model.to(device)   # int8/int4 already sets the device\n# run inference\nfor num_tokens in args.tokens:\n    print(f\"Generating {num_tokens} tokens with {args.model} on prompt:  {args.prompt}\")\n    time_avg = 0\n    for run in range(args.runs + args.warmup):\n        time_begin = time.perf_counter()\n        generated_ids = model.generate(input_ids, do_sample=False, min_length=num_tokens+input_ids.shape[1], max_length=num_tokens+input_ids.shape[1]) #min_new_tokens=num_tokens, max_new_tokens=num_tokens)  # greedy generation of fixed # of tokens",
        "detail": "packages.llm.transformers.huggingface-benchmark",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.llm.transformers.test",
        "description": "packages.llm.transformers.test",
        "peekOfCode": "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='cuda')\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nstreamer = TextIteratorStreamer(tokenizer)\nprompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=256, streamer=streamer)).start()",
        "detail": "packages.llm.transformers.test",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "packages.llm.transformers.test",
        "description": "packages.llm.transformers.test",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(model_name)\nstreamer = TextIteratorStreamer(tokenizer)\nprompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=256, streamer=streamer)).start()\nfor text in streamer:",
        "detail": "packages.llm.transformers.test",
        "documentation": {}
    },
    {
        "label": "streamer",
        "kind": 5,
        "importPath": "packages.llm.transformers.test",
        "description": "packages.llm.transformers.test",
        "peekOfCode": "streamer = TextIteratorStreamer(tokenizer)\nprompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=256, streamer=streamer)).start()\nfor text in streamer:\n    print(text, end='', flush=True)",
        "detail": "packages.llm.transformers.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "packages.llm.transformers.test",
        "description": "packages.llm.transformers.test",
        "peekOfCode": "prompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=256, streamer=streamer)).start()\nfor text in streamer:\n    print(text, end='', flush=True)",
        "detail": "packages.llm.transformers.test",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "packages.llm.transformers.test",
        "description": "packages.llm.transformers.test",
        "peekOfCode": "inputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\nThread(target=lambda: model.generate(inputs, max_new_tokens=256, streamer=streamer)).start()\nfor text in streamer:\n    print(text, end='', flush=True)",
        "detail": "packages.llm.transformers.test",
        "documentation": {}
    },
    {
        "label": "xformers",
        "kind": 2,
        "importPath": "packages.llm.xformers.config",
        "description": "packages.llm.xformers.config",
        "peekOfCode": "def xformers(version, requires=None, default=False):\n    pkg = package.copy()\n    if requires:\n        pkg['requires'] = requires   \n    pkg['name'] = f'xformers:{version}'\n    pkg['build_args'] = {\n        'XFORMERS_VERSION': version,\n    }\n    builder = pkg.copy()\n    builder['name'] = f'xformers:{version}-builder'",
        "detail": "packages.llm.xformers.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.llm.xformers.config",
        "description": "packages.llm.xformers.config",
        "peekOfCode": "package = [\n    xformers('0.0.26', default=True),\n]",
        "detail": "packages.llm.xformers.config",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.nemo.config",
        "description": "packages.nemo.config",
        "peekOfCode": "package['build_args'] = {\n    'TORCH_CUDA_ARCH_LIST': ';'.join([f'{x/10:.1f}' for x in CUDA_ARCHITECTURES])\n}\nif L4T_VERSION.major <= 32:\n    package['dockerfile'] = 'Dockerfile.jp4'\n    package['depends'].extend(['rust', 'protobuf:apt'])\nelif L4T_VERSION.major <= 35:\n    package['build_args'] = {\n        'LD_PRELOAD_LIBS': '/usr/local/lib/python3.8/dist-packages/sklearn/__check_build/../../scikit_learn.libs/libgomp-d22c30c5.so.1.0.0'\n    }",
        "detail": "packages.nemo.config",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, required=True, help=\"path to .nemo file\")\nparser.add_argument('--config', type=str, default=\"/opt/nemo/examples/nlp/language_modeling/conf/megatron_gpt_inference.yaml\")\nparser.add_argument('--prompt', type=str, default='Once upon a time,')\nargs = parser.parse_args()\nprint(args)\n#print(MegatronGPTModel.list_available_models())\nprint(f\"-- loading config {args.config}\")\ncfg = OmegaConf.load(args.config)\nprint(OmegaConf.to_yaml(cfg))",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\n#print(MegatronGPTModel.list_available_models())\nprint(f\"-- loading config {args.config}\")\ncfg = OmegaConf.load(args.config)\nprint(OmegaConf.to_yaml(cfg))\ntrainer = Trainer(strategy=NLPDDPStrategy(), **cfg.trainer)\npretrained_cfg = MegatronGPTModel.restore_from(\n            restore_path=args.model,\n            trainer=trainer,",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "cfg = OmegaConf.load(args.config)\nprint(OmegaConf.to_yaml(cfg))\ntrainer = Trainer(strategy=NLPDDPStrategy(), **cfg.trainer)\npretrained_cfg = MegatronGPTModel.restore_from(\n            restore_path=args.model,\n            trainer=trainer,\n            return_config=True,\n        )\nOmegaConf.set_struct(pretrained_cfg, True)\nwith open_dict(pretrained_cfg):",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "trainer = Trainer(strategy=NLPDDPStrategy(), **cfg.trainer)\npretrained_cfg = MegatronGPTModel.restore_from(\n            restore_path=args.model,\n            trainer=trainer,\n            return_config=True,\n        )\nOmegaConf.set_struct(pretrained_cfg, True)\nwith open_dict(pretrained_cfg):\n    pretrained_cfg.sequence_parallel = False\n    pretrained_cfg.activations_checkpoint_granularity = None",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "pretrained_cfg",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "pretrained_cfg = MegatronGPTModel.restore_from(\n            restore_path=args.model,\n            trainer=trainer,\n            return_config=True,\n        )\nOmegaConf.set_struct(pretrained_cfg, True)\nwith open_dict(pretrained_cfg):\n    pretrained_cfg.sequence_parallel = False\n    pretrained_cfg.activations_checkpoint_granularity = None\n    pretrained_cfg.activations_checkpoint_method = None",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "model = MegatronGPTModel.restore_from(args.model, trainer=trainer, override_config_path=pretrained_cfg)\nmodel.freeze()\n# Have to turn off activations_checkpoint_method for inference\ntry:\n    model.model.language_model.encoder.activations_checkpoint_method = None\nexcept AttributeError:\n    pass\nlength_params: LengthParam = {\n        \"max_length\": cfg.inference.tokens_to_generate,\n        \"min_length\": cfg.inference.min_tokens_to_generate,",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "fp8_enabled",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "fp8_enabled = hasattr(model.cfg, \"fp8\") and (model.cfg.fp8 == True)\nif fp8_enabled:\n    print(\"-- fp8 enabled\")\n    raise NotImplementedError(\"fp8 padding not implemented\")\n    nb_paddings = 0\n    while len(cfg.prompts) % 8 != 0:\n        cfg.prompts.append(\"\")\n        nb_paddings += 1\nprint(args.prompt)\nresponse = model.generate(",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "packages.nemo.test_megatron",
        "description": "packages.nemo.test_megatron",
        "peekOfCode": "response = model.generate(\n    inputs=[args.prompt], length_params=length_params, sampling_params=sampling_params\n)\nprint(args.response)",
        "detail": "packages.nemo.test_megatron",
        "documentation": {}
    },
    {
        "label": "request",
        "kind": 5,
        "importPath": "packages.nemo.test_qa",
        "description": "packages.nemo.test_qa",
        "peekOfCode": "request = requests.get(\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/nlp/question_answering/get_squad.py\", allow_redirects=True)\nopen(DATA_DOWNLOADER, 'wb').write(request.content)\nsubprocess.run(f\"python3 {DATA_DOWNLOADER} --destDir={DATA_DIR}\", shell=True, check=True)\n# parse command-line options\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default='qa_squadv1.1_bertbase')\nparser.add_argument('--data', type=str, default=os.path.join(DATA_DIR, 'squad/v1.1/dev-v1.1.json'))\nparser.add_argument('--samples', type=int, default=5)\nargs = parser.parse_args()",
        "detail": "packages.nemo.test_qa",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.nemo.test_qa",
        "description": "packages.nemo.test_qa",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default='qa_squadv1.1_bertbase')\nparser.add_argument('--data', type=str, default=os.path.join(DATA_DIR, 'squad/v1.1/dev-v1.1.json'))\nparser.add_argument('--samples', type=int, default=5)\nargs = parser.parse_args()\n# list available models\nprint(QAModel.list_available_models())\n# load pre-trained model\nprint(f\"Loading pretrained model {args.model}\")\nmodel = QAModel.from_pretrained(args.model)    ",
        "detail": "packages.nemo.test_qa",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.nemo.test_qa",
        "description": "packages.nemo.test_qa",
        "peekOfCode": "args = parser.parse_args()\n# list available models\nprint(QAModel.list_available_models())\n# load pre-trained model\nprint(f\"Loading pretrained model {args.model}\")\nmodel = QAModel.from_pretrained(args.model)    \nprint(model)\n# runn inferencing\nprint(f\"Testing inference on {args.samples} samples from {args.data}\")\nall_preds, all_nbest = model.inference(args.data, num_samples=args.samples)",
        "detail": "packages.nemo.test_qa",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.nemo.test_qa",
        "description": "packages.nemo.test_qa",
        "peekOfCode": "model = QAModel.from_pretrained(args.model)    \nprint(model)\n# runn inferencing\nprint(f\"Testing inference on {args.samples} samples from {args.data}\")\nall_preds, all_nbest = model.inference(args.data, num_samples=args.samples)\nfor question_id in all_preds:\n    print(all_preds[question_id])\nprint('nemo OK\\n')",
        "detail": "packages.nemo.test_qa",
        "documentation": {}
    },
    {
        "label": "cu_discriminant",
        "kind": 2,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "def cu_discriminant(a, b, c):\n    return math.sqrt(b ** 2 - 4 * a * c)\nN = 10000\ndtype = np.float32\n# prepare the input\nA = np.array(np.random.sample(N), dtype=dtype)\nB = np.array(np.random.sample(N) + 10, dtype=dtype)\nC = np.array(np.random.sample(N), dtype=dtype)\nD = cu_discriminant(A, B, C)\nprint('cuda vectorized ufunc result:')",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "cu_add_arrays",
        "kind": 2,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "def cu_add_arrays(x, y, res):\n    for i in range(x.shape[0]):  # number of channels (3)\n        res[i] = x[i] + y[i]\nA = np.full((2,4,3), 1, dtype)\nB = np.full(A.shape, 2, dtype)\nC = cu_add_arrays(A, B)\nprint('cuda guvectorized ufunc result:')\nprint(C)  # results should be '3'\nprint('numba OK\\n')",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "N",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "N = 10000\ndtype = np.float32\n# prepare the input\nA = np.array(np.random.sample(N), dtype=dtype)\nB = np.array(np.random.sample(N) + 10, dtype=dtype)\nC = np.array(np.random.sample(N), dtype=dtype)\nD = cu_discriminant(A, B, C)\nprint('cuda vectorized ufunc result:')\nprint(D)  # print result\n# test array vectorization",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "dtype",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "dtype = np.float32\n# prepare the input\nA = np.array(np.random.sample(N), dtype=dtype)\nB = np.array(np.random.sample(N) + 10, dtype=dtype)\nC = np.array(np.random.sample(N), dtype=dtype)\nD = cu_discriminant(A, B, C)\nprint('cuda vectorized ufunc result:')\nprint(D)  # print result\n# test array vectorization\nprint('testing cuda guvectorized ufunc...')",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "A = np.array(np.random.sample(N), dtype=dtype)\nB = np.array(np.random.sample(N) + 10, dtype=dtype)\nC = np.array(np.random.sample(N), dtype=dtype)\nD = cu_discriminant(A, B, C)\nprint('cuda vectorized ufunc result:')\nprint(D)  # print result\n# test array vectorization\nprint('testing cuda guvectorized ufunc...')\n@guvectorize(['uint8[:], uint8[:], uint8[:]',\n              'float32[:], float32[:], float32[:]'], ",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "B",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "B = np.array(np.random.sample(N) + 10, dtype=dtype)\nC = np.array(np.random.sample(N), dtype=dtype)\nD = cu_discriminant(A, B, C)\nprint('cuda vectorized ufunc result:')\nprint(D)  # print result\n# test array vectorization\nprint('testing cuda guvectorized ufunc...')\n@guvectorize(['uint8[:], uint8[:], uint8[:]',\n              'float32[:], float32[:], float32[:]'], \n              '(n),(n)->(n)',",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "C",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "C = np.array(np.random.sample(N), dtype=dtype)\nD = cu_discriminant(A, B, C)\nprint('cuda vectorized ufunc result:')\nprint(D)  # print result\n# test array vectorization\nprint('testing cuda guvectorized ufunc...')\n@guvectorize(['uint8[:], uint8[:], uint8[:]',\n              'float32[:], float32[:], float32[:]'], \n              '(n),(n)->(n)',\n             target='cuda')",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "D",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "D = cu_discriminant(A, B, C)\nprint('cuda vectorized ufunc result:')\nprint(D)  # print result\n# test array vectorization\nprint('testing cuda guvectorized ufunc...')\n@guvectorize(['uint8[:], uint8[:], uint8[:]',\n              'float32[:], float32[:], float32[:]'], \n              '(n),(n)->(n)',\n             target='cuda')\ndef cu_add_arrays(x, y, res):",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "A = np.full((2,4,3), 1, dtype)\nB = np.full(A.shape, 2, dtype)\nC = cu_add_arrays(A, B)\nprint('cuda guvectorized ufunc result:')\nprint(C)  # results should be '3'\nprint('numba OK\\n')",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "B",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "B = np.full(A.shape, 2, dtype)\nC = cu_add_arrays(A, B)\nprint('cuda guvectorized ufunc result:')\nprint(C)  # results should be '3'\nprint('numba OK\\n')",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "C",
        "kind": 5,
        "importPath": "packages.numba.test",
        "description": "packages.numba.test",
        "peekOfCode": "C = cu_add_arrays(A, B)\nprint('cuda guvectorized ufunc result:')\nprint(C)  # results should be '3'\nprint('numba OK\\n')",
        "detail": "packages.numba.test",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 2,
        "importPath": "packages.onnxruntime.config",
        "description": "packages.onnxruntime.config",
        "peekOfCode": "def onnxruntime(version, branch=None, requires=None, default=False):\n    ort = package.copy()\n    ort['name'] = f'onnxruntime:{version}'\n    if requires:\n        ort['requires'] = requires\n    if len(version.split('.')) < 3:\n        version = version + '.0'\n    if not branch:\n        branch = 'v' + version\n    ort['build_args'] = {",
        "detail": "packages.onnxruntime.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.onnxruntime.config",
        "description": "packages.onnxruntime.config",
        "peekOfCode": "package = [\n    onnxruntime('1.19', requires=['>=36', '>=cu124'], default=True, branch='main'),\n    onnxruntime('1.17', requires=['>=36', '<=cu122'], default=True),\n    onnxruntime('1.16.3', requires='==35.*', default=True),\n    onnxruntime('1.11', requires='==32.*', default=True),\n]",
        "detail": "packages.onnxruntime.config",
        "documentation": {}
    },
    {
        "label": "test_infer",
        "kind": 2,
        "importPath": "packages.onnxruntime.test",
        "description": "packages.onnxruntime.test",
        "peekOfCode": "def test_infer(provider, model='resnet18.onnx', runs=100, warmup=10, verbose=False):\n    provider_options = {}\n    if provider == 'TensorrtExecutionProvider':\n        trt_cache_path = os.path.join(os.path.dirname(model), 'trt_cache')\n        os.makedirs(trt_cache_path, exist_ok=True)\n        provider_options = {\n            'trt_fp16_enable': True,\n            'trt_engine_cache_enable': True,\n            'trt_engine_cache_path': trt_cache_path\n        }",
        "detail": "packages.onnxruntime.test",
        "documentation": {}
    },
    {
        "label": "ort_version",
        "kind": 5,
        "importPath": "packages.onnxruntime.test",
        "description": "packages.onnxruntime.test",
        "peekOfCode": "ort_version = Version(ort.__version__)\nif ort_version > Version('1.10'):\n    print(ort.get_build_info())\n# verify execution providers\nproviders = ort.get_available_providers()\nprint(f'execution providers:  {providers}')\nfor provider in ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']:\n    if provider not in providers:\n        raise RuntimeError(f\"missing provider '{provider}' from available execution providers {providers}\")\n# test model inference",
        "detail": "packages.onnxruntime.test",
        "documentation": {}
    },
    {
        "label": "providers",
        "kind": 5,
        "importPath": "packages.onnxruntime.test",
        "description": "packages.onnxruntime.test",
        "peekOfCode": "providers = ort.get_available_providers()\nprint(f'execution providers:  {providers}')\nfor provider in ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']:\n    if provider not in providers:\n        raise RuntimeError(f\"missing provider '{provider}' from available execution providers {providers}\")\n# test model inference\ndef test_infer(provider, model='resnet18.onnx', runs=100, warmup=10, verbose=False):\n    provider_options = {}\n    if provider == 'TensorrtExecutionProvider':\n        trt_cache_path = os.path.join(os.path.dirname(model), 'trt_cache')",
        "detail": "packages.onnxruntime.test",
        "documentation": {}
    },
    {
        "label": "openai_triton",
        "kind": 2,
        "importPath": "packages.openai-triton.config",
        "description": "packages.openai-triton.config",
        "peekOfCode": "def openai_triton(version, branch=None, requires=None, default=False):\n    pkg = package.copy()\n    if not branch:\n        branch = f'v{version}'\n    if requires:\n        pkg['requires'] = requires   \n    pkg['name'] = f'openai-triton:{version}'\n    pkg['alias'] = [f'triton:{version}']\n    pkg['build_args'] = {\n        'OPENAITRITON_VERSION': version,",
        "detail": "packages.openai-triton.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.openai-triton.config",
        "description": "packages.openai-triton.config",
        "peekOfCode": "package = [\n    openai_triton('2.1.0'),\n    openai_triton('3.0.0', branch='main', default=True)\n]\n# from jetson_containers import find_container\n# builder = package.copy()\n# runtime = package.copy()\n# builder['name'] = 'openai-triton:builder'\n# builder['dockerfile'] = 'Dockerfile.builder'\n# print(\" ============== [openai-triton/config.py] =============== \")",
        "detail": "packages.openai-triton.config",
        "documentation": {}
    },
    {
        "label": "kernel",
        "kind": 2,
        "importPath": "packages.openai-triton.test",
        "description": "packages.openai-triton.test",
        "peekOfCode": "def kernel(X, stride_xm, stride_xn, BLOCK: tl.constexpr) -> None:\n    pass\nprint(\"testing triton...\")\nprint('triton version: ' + str(triton.__version__))\nprint(\"testing triton kernel...\")\nX = torch.randn(1, device=\"cuda\")\npgm = kernel[(1, )](X, 1, 1, BLOCK=1024)\nprint(pgm)\nprint(\"triton OK...\")",
        "detail": "packages.openai-triton.test",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "packages.openai-triton.test",
        "description": "packages.openai-triton.test",
        "peekOfCode": "X = torch.randn(1, device=\"cuda\")\npgm = kernel[(1, )](X, 1, 1, BLOCK=1024)\nprint(pgm)\nprint(\"triton OK...\")",
        "detail": "packages.openai-triton.test",
        "documentation": {}
    },
    {
        "label": "pgm",
        "kind": 5,
        "importPath": "packages.openai-triton.test",
        "description": "packages.openai-triton.test",
        "peekOfCode": "pgm = kernel[(1, )](X, 1, 1, BLOCK=1024)\nprint(pgm)\nprint(\"triton OK...\")",
        "detail": "packages.openai-triton.test",
        "documentation": {}
    },
    {
        "label": "opencv",
        "kind": 2,
        "importPath": "packages.opencv.config",
        "description": "packages.opencv.config",
        "peekOfCode": "def opencv(version, requires=None, default=False, url=None):\n    cv = package.copy()\n    cv['build_args'] = {\n        'OPENCV_VERSION': version,\n        'OPENCV_PYTHON': f\"{version.split('.')[0]}.x\",\n        'CUDA_ARCH_BIN': ','.join([f'{x/10:.1f}' for x in CUDA_ARCHITECTURES]),\n    }\n    if url:\n        cv['build_args']['OPENCV_URL'] = url\n        cv['name'] = f'opencv:{version}-deb'",
        "detail": "packages.opencv.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.opencv.config",
        "description": "packages.opencv.config",
        "peekOfCode": "package = [\n    # JetPack 6\n    opencv('4.10.0', '==36.*', default=True),\n    opencv('4.9.0', '==36.*', default=False), \n    # JetPack 5\n    opencv('4.10.0', '==35.*', default=True),\n    opencv('4.5.0', '==35.*', default=False),\n    # JetPack 4\n    opencv('4.5.0', '==32.*', default=True, url='https://nvidia.box.com/shared/static/5v89u6g5rb62fpz4lh0rz531ajo2t5ef.gz'),\n    # Debians (c++)",
        "detail": "packages.opencv.config",
        "documentation": {}
    },
    {
        "label": "img_url",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "img_url = 'https://raw.githubusercontent.com/dusty-nv/jetson-containers/59f840abbb99f22914a7b2471da829b3dd56122e/test/data/test_0.jpg'\nimg_path = '/tmp/test_0.jpg'\nrequest = requests.get(img_url, allow_redirects=True)\nopen(img_path, 'wb').write(request.content)\n# load image\nimg_cpu = cv2.imread(img_path)\nprint(f'loaded test image from {img_path}  {img_cpu.shape}  {img_cpu.dtype}')\n# test GPU processing\nimg_gpu = cv2.cuda_GpuMat()\nimg_gpu.upload(img_cpu)",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "img_path = '/tmp/test_0.jpg'\nrequest = requests.get(img_url, allow_redirects=True)\nopen(img_path, 'wb').write(request.content)\n# load image\nimg_cpu = cv2.imread(img_path)\nprint(f'loaded test image from {img_path}  {img_cpu.shape}  {img_cpu.dtype}')\n# test GPU processing\nimg_gpu = cv2.cuda_GpuMat()\nimg_gpu.upload(img_cpu)\nimg_gpu = cv2.cuda.resize(img_gpu, (int(img_cpu.shape[0]/2), int(img_cpu.shape[1]/2)))",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "request",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "request = requests.get(img_url, allow_redirects=True)\nopen(img_path, 'wb').write(request.content)\n# load image\nimg_cpu = cv2.imread(img_path)\nprint(f'loaded test image from {img_path}  {img_cpu.shape}  {img_cpu.dtype}')\n# test GPU processing\nimg_gpu = cv2.cuda_GpuMat()\nimg_gpu.upload(img_cpu)\nimg_gpu = cv2.cuda.resize(img_gpu, (int(img_cpu.shape[0]/2), int(img_cpu.shape[1]/2)))\nluv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2LUV).download()",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "img_cpu",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "img_cpu = cv2.imread(img_path)\nprint(f'loaded test image from {img_path}  {img_cpu.shape}  {img_cpu.dtype}')\n# test GPU processing\nimg_gpu = cv2.cuda_GpuMat()\nimg_gpu.upload(img_cpu)\nimg_gpu = cv2.cuda.resize(img_gpu, (int(img_cpu.shape[0]/2), int(img_cpu.shape[1]/2)))\nluv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2LUV).download()\nhsv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2HSV).download()\ngray = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2GRAY)\nimg_gpu = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8)).apply(gray, cv2.cuda_Stream.Null())",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "img_gpu",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "img_gpu = cv2.cuda_GpuMat()\nimg_gpu.upload(img_cpu)\nimg_gpu = cv2.cuda.resize(img_gpu, (int(img_cpu.shape[0]/2), int(img_cpu.shape[1]/2)))\nluv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2LUV).download()\nhsv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2HSV).download()\ngray = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2GRAY)\nimg_gpu = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8)).apply(gray, cv2.cuda_Stream.Null())\nimg_cpu = img_gpu.download()\nprint('OpenCV OK\\n')",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "img_gpu",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "img_gpu = cv2.cuda.resize(img_gpu, (int(img_cpu.shape[0]/2), int(img_cpu.shape[1]/2)))\nluv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2LUV).download()\nhsv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2HSV).download()\ngray = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2GRAY)\nimg_gpu = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8)).apply(gray, cv2.cuda_Stream.Null())\nimg_cpu = img_gpu.download()\nprint('OpenCV OK\\n')",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "luv",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "luv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2LUV).download()\nhsv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2HSV).download()\ngray = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2GRAY)\nimg_gpu = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8)).apply(gray, cv2.cuda_Stream.Null())\nimg_cpu = img_gpu.download()\nprint('OpenCV OK\\n')",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "hsv",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "hsv = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2HSV).download()\ngray = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2GRAY)\nimg_gpu = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8)).apply(gray, cv2.cuda_Stream.Null())\nimg_cpu = img_gpu.download()\nprint('OpenCV OK\\n')",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "gray",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "gray = cv2.cuda.cvtColor(img_gpu, cv2.COLOR_BGR2GRAY)\nimg_gpu = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8)).apply(gray, cv2.cuda_Stream.Null())\nimg_cpu = img_gpu.download()\nprint('OpenCV OK\\n')",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "img_gpu",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "img_gpu = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8)).apply(gray, cv2.cuda_Stream.Null())\nimg_cpu = img_gpu.download()\nprint('OpenCV OK\\n')",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "img_cpu",
        "kind": 5,
        "importPath": "packages.opencv.test",
        "description": "packages.opencv.test",
        "peekOfCode": "img_cpu = img_gpu.download()\nprint('OpenCV OK\\n')",
        "detail": "packages.opencv.test",
        "documentation": {}
    },
    {
        "label": "Flattener",
        "kind": 6,
        "importPath": "packages.pytorch.torch2trt.patches.flattener",
        "description": "packages.pytorch.torch2trt.patches.flattener",
        "peekOfCode": "class Flattener(object):\n    def __init__(self, schema, size):\n        self._schema = schema\n        self._size = size\n    @staticmethod\n    def from_value(value, condition=_default_condition):\n        return Flattener(*_make_schema_from_value(value, condition))\n    @staticmethod\n    def from_dict(x):\n        return Flattener(x['schema'], x['size'])",
        "detail": "packages.pytorch.torch2trt.patches.flattener",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.pytorch.torch2trt.test",
        "description": "packages.pytorch.torch2trt.test",
        "peekOfCode": "model = alexnet(pretrained=True).eval().cuda()\n# create example data\nx = torch.ones((1, 3, 224, 224)).cuda()\n# convert to TensorRT feeding sample data as input\nmodel_trt = torch2trt(model, [x])\n# execute the returned TRTModule like the original PyTorch model\ny = model(x)\ny_trt = model_trt(x)\n# check the output against PyTorch\nprint(torch.max(torch.abs(y - y_trt)))",
        "detail": "packages.pytorch.torch2trt.test",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "packages.pytorch.torch2trt.test",
        "description": "packages.pytorch.torch2trt.test",
        "peekOfCode": "x = torch.ones((1, 3, 224, 224)).cuda()\n# convert to TensorRT feeding sample data as input\nmodel_trt = torch2trt(model, [x])\n# execute the returned TRTModule like the original PyTorch model\ny = model(x)\ny_trt = model_trt(x)\n# check the output against PyTorch\nprint(torch.max(torch.abs(y - y_trt)))\nprint('torch2trt OK\\n')",
        "detail": "packages.pytorch.torch2trt.test",
        "documentation": {}
    },
    {
        "label": "model_trt",
        "kind": 5,
        "importPath": "packages.pytorch.torch2trt.test",
        "description": "packages.pytorch.torch2trt.test",
        "peekOfCode": "model_trt = torch2trt(model, [x])\n# execute the returned TRTModule like the original PyTorch model\ny = model(x)\ny_trt = model_trt(x)\n# check the output against PyTorch\nprint(torch.max(torch.abs(y - y_trt)))\nprint('torch2trt OK\\n')",
        "detail": "packages.pytorch.torch2trt.test",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "packages.pytorch.torch2trt.test",
        "description": "packages.pytorch.torch2trt.test",
        "peekOfCode": "y = model(x)\ny_trt = model_trt(x)\n# check the output against PyTorch\nprint(torch.max(torch.abs(y - y_trt)))\nprint('torch2trt OK\\n')",
        "detail": "packages.pytorch.torch2trt.test",
        "documentation": {}
    },
    {
        "label": "y_trt",
        "kind": 5,
        "importPath": "packages.pytorch.torch2trt.test",
        "description": "packages.pytorch.torch2trt.test",
        "peekOfCode": "y_trt = model_trt(x)\n# check the output against PyTorch\nprint(torch.max(torch.abs(y - y_trt)))\nprint('torch2trt OK\\n')",
        "detail": "packages.pytorch.torch2trt.test",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.pytorch.torch_tensorrt.config",
        "description": "packages.pytorch.torch_tensorrt.config",
        "peekOfCode": "package['build_args'] = {\n    'PYTHON_VERSION': PYTHON_VERSION,\n    'JETPACK_MAJOR': JETPACK_VERSION.major,\n    'JETPACK_MINOR': 0 if JETPACK_VERSION.major >= 5 else 6,   # only 5.0 and 4.6 are recognized\n    'TORCH_TRT_VERSION': TORCH_TRT_VERSION,  \n}",
        "detail": "packages.pytorch.torch_tensorrt.config",
        "documentation": {}
    },
    {
        "label": "torchaudio",
        "kind": 2,
        "importPath": "packages.pytorch.torchaudio.config",
        "description": "packages.pytorch.torchaudio.config",
        "peekOfCode": "def torchaudio(version, pytorch=None, requires=None):\n    pkg = package.copy()\n    pkg['name'] = f\"torchaudio:{version.split('-')[0]}\"  # remove any -rc* suffix\n    if pytorch:\n        pkg['depends'] = update_dependencies(pkg['depends'], f\"pytorch:{pytorch}\")\n    else:\n        pytorch = PYTORCH_VERSION\n    if requires:\n        pkg['requires'] = requires\n    if not isinstance(pytorch, Version):",
        "detail": "packages.pytorch.torchaudio.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.pytorch.torchaudio.config",
        "description": "packages.pytorch.torchaudio.config",
        "peekOfCode": "package = [\n    # JetPack 5/6\n    torchaudio('2.0.1', pytorch='2.0', requires='==35.*'),\n    torchaudio('2.1.0', pytorch='2.1', requires='>=35'),\n    torchaudio('2.2.2', pytorch='2.2', requires='>=35'),\n    torchaudio('2.4.0', pytorch='2.4', requires='==36.*'),\n    # JetPack 4\n    torchaudio('0.10.0', pytorch='1.10', requires='==32.*'),\n    torchaudio('0.9.0', pytorch='1.9', requires='==32.*'),\n]",
        "detail": "packages.pytorch.torchaudio.config",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 2,
        "importPath": "packages.pytorch.torchvision.config",
        "description": "packages.pytorch.torchvision.config",
        "peekOfCode": "def torchvision(version, pytorch=None, requires=None):\n    pkg = package.copy()\n    pkg['name'] = f\"torchvision:{version.split('-')[0]}\"  # remove any -rc* suffix\n    if pytorch:\n        pkg['depends'] = update_dependencies(pkg['depends'], f\"pytorch:{pytorch}\")\n    else:\n        pytorch = PYTORCH_VERSION  \n    if requires:\n        pkg['requires'] = requires\n    if len(version.split('.')) < 3:",
        "detail": "packages.pytorch.torchvision.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.pytorch.torchvision.config",
        "description": "packages.pytorch.torchvision.config",
        "peekOfCode": "package = [\n    # JetPack 5/6\n    torchvision('0.15.1', pytorch='2.0', requires='==35.*'),\n    torchvision('0.16.2', pytorch='2.1', requires='>=35'),\n    torchvision('0.17.2', pytorch='2.2', requires='>=35'),\n    torchvision('0.19.0', pytorch='2.4', requires='>=35'),\n    #torchvision('0.17.2', pytorch='2.2', requires='==36.*'),\n    #torchvision('0.18.0-rc1', pytorch='2.3', requires='==36.*'),\n    # JetPack 4\n    torchvision('0.11.1', pytorch='1.10', requires='==32.*'),",
        "detail": "packages.pytorch.torchvision.config",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "kind": 6,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "ProgressMeter",
        "kind": 6,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "class ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('  '.join(entries))\n    def _get_batch_fmtstr(self, num_batches):",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "test_nms",
        "kind": 2,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "def test_nms(N=128):\n    print('testing torchvision extensions...')\n    boxes = []\n    scores = []\n    for n in range(N):\n        boxes.append((n, n+1, n, n+1))\n        scores.append(n)\n    boxes = torch.Tensor(boxes)\n    scores = torch.Tensor(scores)\n    indices = torchvision.ops.nms(boxes, scores, 0.5)",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "def load_data(root):\n    return torch.utils.data.DataLoader(\n            datasets.ImageFolder(root, transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(args.resolution),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225]),\n            ])),\n            batch_size=args.batch_size, shuffle=False,",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "download_data",
        "kind": 2,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "def download_data(url, tar, workdir='/data/datasets'):\n    filename = os.path.join(workdir, tar)\n    folder = filename[:-7] if '.tar.gz' in filename else os.path.splitext(filename)[0]\n    if not os.path.isfile(filename):\n        print(f\"Downloading {url} to {filename}\")\n        request = requests.get(url, allow_redirects=True)\n        open(filename, 'wb').write(request.content)\n    if not os.path.isdir(folder):\n        print(f\"Extracting {filename} to {folder}\")\n        shutil.unpack_archive(filename, workdir)",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "test_model",
        "kind": 2,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "def test_model(model_info, data_loader):\n    model_name = model_info[0]\n    model_top1 = 100.0 - model_info[1]\n    model_top5 = 100.0 - model_info[2]\n    print(\"\\n\")\n    print(\"---------------------------------------------\")\n    print(\"-- \" + model_name)\n    print(\"---------------------------------------------\")\n    print(\"loading model '{:s}'\".format(model_name))\n    model = models.__dict__[model_name](pretrained=True, progress=False).eval()",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "print_results",
        "kind": 2,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "def print_results(results):\n    print(' ')\n    print(results[0])\n    print('   * Acc@1 {:.3f}  Expected {:.3f}   Delta {:.3f}'.format(results[1], results[2], results[3]))\n    print('   * Acc@5 {:.3f}  Expected {:.3f}   Delta {:.3f}'.format(results[4], results[5], results[6]))\n    print('   * Images/sec  {:.3f}'.format(results[7]))\n    print('   * {:s}'.format('PASS' if results[8] else 'FAIL'))\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n        res = []\n        for k in topk:",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "packages.pytorch.torchvision.test",
        "description": "packages.pytorch.torchvision.test",
        "peekOfCode": "model_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(\"__\")\n    and callable(models.__dict__[name]))\nprint('torchvision classification models: ' + ' | '.join(model_names) + '\\n')\ndef load_data(root):\n    return torch.utils.data.DataLoader(\n            datasets.ImageFolder(root, transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(args.resolution),\n                transforms.ToTensor(),",
        "detail": "packages.pytorch.torchvision.test",
        "documentation": {}
    },
    {
        "label": "pytorch_pip",
        "kind": 2,
        "importPath": "packages.pytorch.config",
        "description": "packages.pytorch.config",
        "peekOfCode": "def pytorch_pip(version, requires=None, alias=None):\n    \"\"\"\n    Install PyTorch from pip server with Dockerfile.pip\n    \"\"\"\n    pkg = package.copy()\n    short_version = Version(version.split('-')[0]) # remove any -rc* suffix\n    short_version = f\"{short_version.major}.{short_version.minor}\"\n    pkg['name'] = f'pytorch:{short_version}'    \n    pkg['dockerfile'] = 'Dockerfile.pip'\n    if len(version.split('.')) < 3:",
        "detail": "packages.pytorch.config",
        "documentation": {}
    },
    {
        "label": "pytorch_whl",
        "kind": 2,
        "importPath": "packages.pytorch.config",
        "description": "packages.pytorch.config",
        "peekOfCode": "def pytorch_whl(version, whl, url, requires, alias=None):\n    \"\"\"\n    Download & install PyTorch wheel with Dockerfile\n    \"\"\"\n    pkg = package.copy()\n    pkg['name'] = f'pytorch:{version}'\n    pkg['alias'] = [f'torch:{version}']\n    if Version(version) == PYTORCH_VERSION:\n        pkg['alias'].extend(['pytorch', 'torch'])\n    if alias:",
        "detail": "packages.pytorch.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.pytorch.config",
        "description": "packages.pytorch.config",
        "peekOfCode": "package = [\n    # JetPack 5/6\n    pytorch_pip('2.0', requires='==35.*'),\n    pytorch_pip('2.1', requires='>=35'),\n    pytorch_pip('2.2', requires='>=35'),\n    pytorch_pip('2.4', requires='==36.*'),\n    # JetPack 4\n    pytorch_whl('1.10', 'torch-1.10.0-cp36-cp36m-linux_aarch64.whl', 'https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl', '==32.*'),\n    pytorch_whl('1.9', 'torch-1.9.0-cp36-cp36m-linux_aarch64.whl', 'https://nvidia.box.com/shared/static/h1z9sw4bb1ybi0rm3tu8qdj8hs05ljbm.whl', '==32.*'),\n]",
        "detail": "packages.pytorch.config",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "a = torch.cuda.FloatTensor(2).zero_()\nprint('Tensor a = ' + str(a))\nb = torch.randn(2).cuda()\nprint('Tensor b = ' + str(b))\nc = a + b\nprint('Tensor c = ' + str(c))\n# LAPACK test\nprint('testing LAPACK (OpenBLAS)...')\na = torch.randn(2, 3, 1, 4, 4)\nb = torch.randn(2, 3, 1, 4, 4)",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "b = torch.randn(2).cuda()\nprint('Tensor b = ' + str(b))\nc = a + b\nprint('Tensor c = ' + str(c))\n# LAPACK test\nprint('testing LAPACK (OpenBLAS)...')\na = torch.randn(2, 3, 1, 4, 4)\nb = torch.randn(2, 3, 1, 4, 4)\nx, lu = torch.linalg.solve(b, a)\nprint('done testing LAPACK (OpenBLAS)')",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "c = a + b\nprint('Tensor c = ' + str(c))\n# LAPACK test\nprint('testing LAPACK (OpenBLAS)...')\na = torch.randn(2, 3, 1, 4, 4)\nb = torch.randn(2, 3, 1, 4, 4)\nx, lu = torch.linalg.solve(b, a)\nprint('done testing LAPACK (OpenBLAS)')\n# torch.nn test\nprint('testing torch.nn (cuDNN)...')",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "a = torch.randn(2, 3, 1, 4, 4)\nb = torch.randn(2, 3, 1, 4, 4)\nx, lu = torch.linalg.solve(b, a)\nprint('done testing LAPACK (OpenBLAS)')\n# torch.nn test\nprint('testing torch.nn (cuDNN)...')\nimport torch.nn\nmodel = torch.nn.Conv2d(3,3,3)\ndata = torch.zeros(1,3,10,10)\nmodel = model.cuda()",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "b = torch.randn(2, 3, 1, 4, 4)\nx, lu = torch.linalg.solve(b, a)\nprint('done testing LAPACK (OpenBLAS)')\n# torch.nn test\nprint('testing torch.nn (cuDNN)...')\nimport torch.nn\nmodel = torch.nn.Conv2d(3,3,3)\ndata = torch.zeros(1,3,10,10)\nmodel = model.cuda()\ndata = data.cuda()",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "model = torch.nn.Conv2d(3,3,3)\ndata = torch.zeros(1,3,10,10)\nmodel = model.cuda()\ndata = data.cuda()\nout = model(data)\n#print(out)\nprint('done testing torch.nn (cuDNN)')\n# CPU test (https://github.com/pytorch/pytorch/issues/47098)\nprint('testing CPU tensor vector operations...')\nimport torch.nn.functional as F",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "data = torch.zeros(1,3,10,10)\nmodel = model.cuda()\ndata = data.cuda()\nout = model(data)\n#print(out)\nprint('done testing torch.nn (cuDNN)')\n# CPU test (https://github.com/pytorch/pytorch/issues/47098)\nprint('testing CPU tensor vector operations...')\nimport torch.nn.functional as F\ncpu_x = torch.tensor([12.345])",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "model = model.cuda()\ndata = data.cuda()\nout = model(data)\n#print(out)\nprint('done testing torch.nn (cuDNN)')\n# CPU test (https://github.com/pytorch/pytorch/issues/47098)\nprint('testing CPU tensor vector operations...')\nimport torch.nn.functional as F\ncpu_x = torch.tensor([12.345])\ncpu_y = F.softmax(cpu_x)",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "data = data.cuda()\nout = model(data)\n#print(out)\nprint('done testing torch.nn (cuDNN)')\n# CPU test (https://github.com/pytorch/pytorch/issues/47098)\nprint('testing CPU tensor vector operations...')\nimport torch.nn.functional as F\ncpu_x = torch.tensor([12.345])\ncpu_y = F.softmax(cpu_x)\nprint('Tensor cpu_x = ' + str(cpu_x))",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "out = model(data)\n#print(out)\nprint('done testing torch.nn (cuDNN)')\n# CPU test (https://github.com/pytorch/pytorch/issues/47098)\nprint('testing CPU tensor vector operations...')\nimport torch.nn.functional as F\ncpu_x = torch.tensor([12.345])\ncpu_y = F.softmax(cpu_x)\nprint('Tensor cpu_x = ' + str(cpu_x))\nprint('Tensor softmax = ' + str(cpu_y))",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "cpu_x",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "cpu_x = torch.tensor([12.345])\ncpu_y = F.softmax(cpu_x)\nprint('Tensor cpu_x = ' + str(cpu_x))\nprint('Tensor softmax = ' + str(cpu_y))\nif cpu_y != 1.0:\n    raise ValueError('PyTorch CPU tensor vector test failed (softmax)\\n')\n# https://github.com/pytorch/pytorch/issues/61110\nt_32 = torch.ones((3,3), dtype=torch.float32).exp()\nt_64 = torch.ones((3,3), dtype=torch.float64).exp()\ndiff = (t_32 - t_64).abs().sum().item()",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "cpu_y",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "cpu_y = F.softmax(cpu_x)\nprint('Tensor cpu_x = ' + str(cpu_x))\nprint('Tensor softmax = ' + str(cpu_y))\nif cpu_y != 1.0:\n    raise ValueError('PyTorch CPU tensor vector test failed (softmax)\\n')\n# https://github.com/pytorch/pytorch/issues/61110\nt_32 = torch.ones((3,3), dtype=torch.float32).exp()\nt_64 = torch.ones((3,3), dtype=torch.float64).exp()\ndiff = (t_32 - t_64).abs().sum().item()\nprint('Tensor exp (float32) = ' + str(t_32))",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "t_32",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "t_32 = torch.ones((3,3), dtype=torch.float32).exp()\nt_64 = torch.ones((3,3), dtype=torch.float64).exp()\ndiff = (t_32 - t_64).abs().sum().item()\nprint('Tensor exp (float32) = ' + str(t_32))\nprint('Tensor exp (float64) = ' + str(t_64))\nprint('Tensor exp (diff) = ' + str(diff))\nif diff > 0.1:\n    raise ValueError(f'PyTorch CPU tensor vector test failed (exp, diff={diff})')\nprint('PyTorch OK\\n')",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "t_64",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "t_64 = torch.ones((3,3), dtype=torch.float64).exp()\ndiff = (t_32 - t_64).abs().sum().item()\nprint('Tensor exp (float32) = ' + str(t_32))\nprint('Tensor exp (float64) = ' + str(t_64))\nprint('Tensor exp (diff) = ' + str(diff))\nif diff > 0.1:\n    raise ValueError(f'PyTorch CPU tensor vector test failed (exp, diff={diff})')\nprint('PyTorch OK\\n')",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "diff",
        "kind": 5,
        "importPath": "packages.pytorch.test",
        "description": "packages.pytorch.test",
        "peekOfCode": "diff = (t_32 - t_64).abs().sum().item()\nprint('Tensor exp (float32) = ' + str(t_32))\nprint('Tensor exp (float64) = ' + str(t_64))\nprint('Tensor exp (diff) = ' + str(diff))\nif diff > 0.1:\n    raise ValueError(f'PyTorch CPU tensor vector test failed (exp, diff={diff})')\nprint('PyTorch OK\\n')",
        "detail": "packages.pytorch.test",
        "documentation": {}
    },
    {
        "label": "pull_models",
        "kind": 2,
        "importPath": "packages.rag.jetson-copilot.app",
        "description": "packages.rag.jetson-copilot.app",
        "peekOfCode": "def pull_models():\n    models = [model[\"name\"] for model in ollama.list()[\"models\"]]\n    if \"llama3:latest\" not in models:\n        with st.spinner(\"Downloading llama3 model... (This will take 2-5 mins)\"):\n            ollama.pull('llama3')\n    if \"mxbai-embed-large:latest\" not in models: \n        with st.spinner(\"Downloading mxbai-embed-large model... (This will take 2-5 mins)\"):\n            ollama.pull('mxbai-embed-large')\n# Side bar\nwith st.sidebar:",
        "detail": "packages.rag.jetson-copilot.app",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "packages.rag.jetson-copilot.app",
        "description": "packages.rag.jetson-copilot.app",
        "peekOfCode": "def load_data():\n    with st.spinner(text=\"Loading and indexing the Jetson docs – hang tight! This should take 1-2 minutes.\"):\n        reader = SimpleDirectoryReader(input_dir=\"/data/documents/jetson\", recursive=True)\n        docs = reader.load_data()\n        index = VectorStoreIndex.from_documents(docs)\n        return index\nindex = load_data()\n# init models\nif \"chat_engine\" not in st.session_state.keys(): # Initialize the chat engine\n    st.session_state.chat_engine = index.as_chat_engine(",
        "detail": "packages.rag.jetson-copilot.app",
        "documentation": {}
    },
    {
        "label": "model_res_generator",
        "kind": 2,
        "importPath": "packages.rag.jetson-copilot.app",
        "description": "packages.rag.jetson-copilot.app",
        "peekOfCode": "def model_res_generator(prompt):\n    response_stream = st.session_state.chat_engine.stream_chat(prompt)\n    for chunk in response_stream.response_gen:\n        yield chunk\n# Display chat messages from history on app rerun\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"], avatar=message[\"avatar\"]):\n        st.markdown(message[\"content\"])\nif prompt := st.chat_input(\"Enter prompt here..\"):\n    # add latest message to history in format {role, content}",
        "detail": "packages.rag.jetson-copilot.app",
        "documentation": {}
    },
    {
        "label": "AVATAR_USER",
        "kind": 5,
        "importPath": "packages.rag.jetson-copilot.app",
        "description": "packages.rag.jetson-copilot.app",
        "peekOfCode": "AVATAR_USER = Image.open('./static/user-purple.png')\n@st.cache_resource(show_spinner=False)\ndef pull_models():\n    models = [model[\"name\"] for model in ollama.list()[\"models\"]]\n    if \"llama3:latest\" not in models:\n        with st.spinner(\"Downloading llama3 model... (This will take 2-5 mins)\"):\n            ollama.pull('llama3')\n    if \"mxbai-embed-large:latest\" not in models: \n        with st.spinner(\"Downloading mxbai-embed-large model... (This will take 2-5 mins)\"):\n            ollama.pull('mxbai-embed-large')",
        "detail": "packages.rag.jetson-copilot.app",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "packages.rag.jetson-copilot.app",
        "description": "packages.rag.jetson-copilot.app",
        "peekOfCode": "index = load_data()\n# init models\nif \"chat_engine\" not in st.session_state.keys(): # Initialize the chat engine\n    st.session_state.chat_engine = index.as_chat_engine(\n        chat_mode=\"context\", \n        streaming=True,\n        memory=ChatMemoryBuffer.from_defaults(token_limit=3900),\n        llm=Settings.llm,\n        context_prompt=(\"\"\"\n            You are a chatbot, able to have normal interactions, as well as talk about NVIDIA Jetson embedded AI computer.",
        "detail": "packages.rag.jetson-copilot.app",
        "documentation": {}
    },
    {
        "label": "samples",
        "kind": 5,
        "importPath": "packages.rag.langchain.config",
        "description": "packages.rag.langchain.config",
        "peekOfCode": "samples = package.copy()\nsamples['name'] = 'langchain:samples'\nsamples['dockerfile'] = 'Dockerfile.samples'\nsamples['depends'] = ['langchain:main', 'jupyterlab']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.langchain.config",
        "documentation": {}
    },
    {
        "label": "samples['name']",
        "kind": 5,
        "importPath": "packages.rag.langchain.config",
        "description": "packages.rag.langchain.config",
        "peekOfCode": "samples['name'] = 'langchain:samples'\nsamples['dockerfile'] = 'Dockerfile.samples'\nsamples['depends'] = ['langchain:main', 'jupyterlab']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.langchain.config",
        "documentation": {}
    },
    {
        "label": "samples['dockerfile']",
        "kind": 5,
        "importPath": "packages.rag.langchain.config",
        "description": "packages.rag.langchain.config",
        "peekOfCode": "samples['dockerfile'] = 'Dockerfile.samples'\nsamples['depends'] = ['langchain:main', 'jupyterlab']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.langchain.config",
        "documentation": {}
    },
    {
        "label": "samples['depends']",
        "kind": 5,
        "importPath": "packages.rag.langchain.config",
        "description": "packages.rag.langchain.config",
        "peekOfCode": "samples['depends'] = ['langchain:main', 'jupyterlab']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.langchain.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.rag.langchain.config",
        "description": "packages.rag.langchain.config",
        "peekOfCode": "package = [package, samples]",
        "detail": "packages.rag.langchain.config",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "reader = SimpleDirectoryReader(input_dir=\"/data/documents/L4T-README/\")\ndocuments = reader.load_data()\nprint(f\"Loaded {len(documents)} docs\")\n# bge embedding model\nSettings.embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")\n# ollama\nSettings.llm = Ollama(model=\"llama2:13b\", request_timeout=300.0)\n# Enlarge the chunk size from the default 1024\nSettings.chunk_size = 4096\nSettings.chunk_overlap = 200",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "documents = reader.load_data()\nprint(f\"Loaded {len(documents)} docs\")\n# bge embedding model\nSettings.embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")\n# ollama\nSettings.llm = Ollama(model=\"llama2:13b\", request_timeout=300.0)\n# Enlarge the chunk size from the default 1024\nSettings.chunk_size = 4096\nSettings.chunk_overlap = 200\nindex = VectorStoreIndex.from_documents(",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "Settings.embed_model",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")\n# ollama\nSettings.llm = Ollama(model=\"llama2:13b\", request_timeout=300.0)\n# Enlarge the chunk size from the default 1024\nSettings.chunk_size = 4096\nSettings.chunk_overlap = 200\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "Settings.llm",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "Settings.llm = Ollama(model=\"llama2:13b\", request_timeout=300.0)\n# Enlarge the chunk size from the default 1024\nSettings.chunk_size = 4096\nSettings.chunk_overlap = 200\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What IPv4 address Jetson device gets assigned when connected to a PC with a USB cable? And what file to edit in order to change the IP address to be assigned to Jetson itself in USB device mode? Plesae state which section you find the answer for each question.\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "Settings.chunk_size",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "Settings.chunk_size = 4096\nSettings.chunk_overlap = 200\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What IPv4 address Jetson device gets assigned when connected to a PC with a USB cable? And what file to edit in order to change the IP address to be assigned to Jetson itself in USB device mode? Plesae state which section you find the answer for each question.\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "Settings.chunk_overlap",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "Settings.chunk_overlap = 200\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What IPv4 address Jetson device gets assigned when connected to a PC with a USB cable? And what file to edit in order to change the IP address to be assigned to Jetson itself in USB device mode? Plesae state which section you find the answer for each question.\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "index = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What IPv4 address Jetson device gets assigned when connected to a PC with a USB cable? And what file to edit in order to change the IP address to be assigned to Jetson itself in USB device mode? Plesae state which section you find the answer for each question.\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "query_engine",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "query_engine = index.as_query_engine()\nresponse = query_engine.query(\"What IPv4 address Jetson device gets assigned when connected to a PC with a USB cable? And what file to edit in order to change the IP address to be assigned to Jetson itself in USB device mode? Plesae state which section you find the answer for each question.\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_l4t",
        "description": "packages.rag.llama-index.samples.llamaindex_l4t",
        "peekOfCode": "response = query_engine.query(\"What IPv4 address Jetson device gets assigned when connected to a PC with a USB cable? And what file to edit in order to change the IP address to be assigned to Jetson itself in USB device mode? Plesae state which section you find the answer for each question.\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_l4t",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_starter",
        "description": "packages.rag.llama-index.samples.llamaindex_starter",
        "peekOfCode": "reader = SimpleDirectoryReader(input_dir=\"/data/documents/paul_graham/\")\ndocuments = reader.load_data()\nprint(f\"Loaded {len(documents)} docs\")\n# bge embedding model\nSettings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n# ollama\nSettings.llm = Ollama(model=\"llama2\", request_timeout=60.0)\nindex = VectorStoreIndex.from_documents(\n    documents,\n)",
        "detail": "packages.rag.llama-index.samples.llamaindex_starter",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_starter",
        "description": "packages.rag.llama-index.samples.llamaindex_starter",
        "peekOfCode": "documents = reader.load_data()\nprint(f\"Loaded {len(documents)} docs\")\n# bge embedding model\nSettings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n# ollama\nSettings.llm = Ollama(model=\"llama2\", request_timeout=60.0)\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()",
        "detail": "packages.rag.llama-index.samples.llamaindex_starter",
        "documentation": {}
    },
    {
        "label": "Settings.embed_model",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_starter",
        "description": "packages.rag.llama-index.samples.llamaindex_starter",
        "peekOfCode": "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n# ollama\nSettings.llm = Ollama(model=\"llama2\", request_timeout=60.0)\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_starter",
        "documentation": {}
    },
    {
        "label": "Settings.llm",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_starter",
        "description": "packages.rag.llama-index.samples.llamaindex_starter",
        "peekOfCode": "Settings.llm = Ollama(model=\"llama2\", request_timeout=60.0)\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_starter",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_starter",
        "description": "packages.rag.llama-index.samples.llamaindex_starter",
        "peekOfCode": "index = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_starter",
        "documentation": {}
    },
    {
        "label": "query_engine",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_starter",
        "description": "packages.rag.llama-index.samples.llamaindex_starter",
        "peekOfCode": "query_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_starter",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "packages.rag.llama-index.samples.llamaindex_starter",
        "description": "packages.rag.llama-index.samples.llamaindex_starter",
        "peekOfCode": "response = query_engine.query(\"What did the author do growing up?\")\nprint(response)",
        "detail": "packages.rag.llama-index.samples.llamaindex_starter",
        "documentation": {}
    },
    {
        "label": "samples",
        "kind": 5,
        "importPath": "packages.rag.llama-index.config",
        "description": "packages.rag.llama-index.config",
        "peekOfCode": "samples = package.copy()\nsamples['name'] = 'llama-index:samples'\nsamples['dockerfile'] = 'Dockerfile.samples'\nsamples['depends'] = ['llama-index:main', 'jupyterlab:myst', 'ollama']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.llama-index.config",
        "documentation": {}
    },
    {
        "label": "samples['name']",
        "kind": 5,
        "importPath": "packages.rag.llama-index.config",
        "description": "packages.rag.llama-index.config",
        "peekOfCode": "samples['name'] = 'llama-index:samples'\nsamples['dockerfile'] = 'Dockerfile.samples'\nsamples['depends'] = ['llama-index:main', 'jupyterlab:myst', 'ollama']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.llama-index.config",
        "documentation": {}
    },
    {
        "label": "samples['dockerfile']",
        "kind": 5,
        "importPath": "packages.rag.llama-index.config",
        "description": "packages.rag.llama-index.config",
        "peekOfCode": "samples['dockerfile'] = 'Dockerfile.samples'\nsamples['depends'] = ['llama-index:main', 'jupyterlab:myst', 'ollama']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.llama-index.config",
        "documentation": {}
    },
    {
        "label": "samples['depends']",
        "kind": 5,
        "importPath": "packages.rag.llama-index.config",
        "description": "packages.rag.llama-index.config",
        "peekOfCode": "samples['depends'] = ['llama-index:main', 'jupyterlab:myst', 'ollama']\ndel samples['alias']\npackage = [package, samples]",
        "detail": "packages.rag.llama-index.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.rag.llama-index.config",
        "description": "packages.rag.llama-index.config",
        "peekOfCode": "package = [package, samples]",
        "detail": "packages.rag.llama-index.config",
        "documentation": {}
    },
    {
        "label": "build_cudf",
        "kind": 2,
        "importPath": "packages.rapids.cudf.config",
        "description": "packages.rapids.cudf.config",
        "peekOfCode": "def build_cudf(version, arrow='arrow', repo='dusty-nv/cudf', requires=None, default=False):\n    cudf = package.copy()\n    cudf['name'] = f'cudf:{version}'\n    cudf['group'] = 'rapids'\n    cudf['notes'] = 'installed under `/usr/local`'\n    cudf['build_args'] = {\n        'CUDF_REPO': repo,\n        'CUDF_VERSION': f'v{version}',\n        'CUDF_CMAKE_CUDA_ARCHITECTURES': ';'.join([str(x) for x in CUDA_ARCHITECTURES]),\n    }",
        "detail": "packages.rapids.cudf.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.rapids.cudf.config",
        "description": "packages.rapids.cudf.config",
        "peekOfCode": "package = [\n    build_cudf('23.10.03', 'arrow:12.0.1', requires='>=36', default=True),\n    build_cudf('21.10.02', requires='==35.*', default=True)\n]",
        "detail": "packages.rapids.cudf.config",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_csv",
        "description": "packages.rapids.cudf.test_csv",
        "peekOfCode": "url = \"https://github.com/plotly/datasets/raw/master/tips.csv\"\ncontent = requests.get(url).content.decode(\"utf-8\")\ntips_df = cudf.read_csv(StringIO(content))\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())",
        "detail": "packages.rapids.cudf.test_csv",
        "documentation": {}
    },
    {
        "label": "content",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_csv",
        "description": "packages.rapids.cudf.test_csv",
        "peekOfCode": "content = requests.get(url).content.decode(\"utf-8\")\ntips_df = cudf.read_csv(StringIO(content))\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())",
        "detail": "packages.rapids.cudf.test_csv",
        "documentation": {}
    },
    {
        "label": "tips_df",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_csv",
        "description": "packages.rapids.cudf.test_csv",
        "peekOfCode": "tips_df = cudf.read_csv(StringIO(content))\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())",
        "detail": "packages.rapids.cudf.test_csv",
        "documentation": {}
    },
    {
        "label": "tips_df[\"tip_percentage\"]",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_csv",
        "description": "packages.rapids.cudf.test_csv",
        "peekOfCode": "tips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())",
        "detail": "packages.rapids.cudf.test_csv",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_cudf",
        "description": "packages.rapids.cudf.test_cudf",
        "peekOfCode": "s = cudf.Series([1, 2, 3, None, 4])\nprint(s)\ndf = cudf.DataFrame(\n    {\n        \"a\": list(range(10)),\n        \"b\": list(reversed(range(10))),\n        \"c\": list(range(10)),\n    }\n)\nprint(df)",
        "detail": "packages.rapids.cudf.test_cudf",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_cudf",
        "description": "packages.rapids.cudf.test_cudf",
        "peekOfCode": "df = cudf.DataFrame(\n    {\n        \"a\": list(range(10)),\n        \"b\": list(reversed(range(10))),\n        \"c\": list(range(10)),\n    }\n)\nprint(df)\nprint(df.sort_values(by=\"b\"))\n#x = df.to_numpy()   # not in cudf 21.10",
        "detail": "packages.rapids.cudf.test_cudf",
        "documentation": {}
    },
    {
        "label": "#x",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_cudf",
        "description": "packages.rapids.cudf.test_cudf",
        "peekOfCode": "#x = df.to_numpy()   # not in cudf 21.10\nx = df.to_pandas()\nx = df.to_arrow()\ndf.to_parquet(\"/tmp/test_parquet\")\ndf.to_orc(\"/tmp/test_orc\")\nprint('\\ncudf OK\\n')",
        "detail": "packages.rapids.cudf.test_cudf",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_cudf",
        "description": "packages.rapids.cudf.test_cudf",
        "peekOfCode": "x = df.to_pandas()\nx = df.to_arrow()\ndf.to_parquet(\"/tmp/test_parquet\")\ndf.to_orc(\"/tmp/test_orc\")\nprint('\\ncudf OK\\n')",
        "detail": "packages.rapids.cudf.test_cudf",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_cudf",
        "description": "packages.rapids.cudf.test_cudf",
        "peekOfCode": "x = df.to_arrow()\ndf.to_parquet(\"/tmp/test_parquet\")\ndf.to_orc(\"/tmp/test_orc\")\nprint('\\ncudf OK\\n')",
        "detail": "packages.rapids.cudf.test_cudf",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_dask",
        "description": "packages.rapids.cudf.test_dask",
        "peekOfCode": "df = cudf.DataFrame(\n    {\n        \"a\": list(range(10)),\n        \"b\": list(reversed(range(10))),\n        \"c\": list(range(10)),\n    }\n)\nddf = dask_cudf.from_cudf(df, npartitions=2)\nprint(ddf)\nprint(ddf.head())",
        "detail": "packages.rapids.cudf.test_dask",
        "documentation": {}
    },
    {
        "label": "ddf",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_dask",
        "description": "packages.rapids.cudf.test_dask",
        "peekOfCode": "ddf = dask_cudf.from_cudf(df, npartitions=2)\nprint(ddf)\nprint(ddf.head())\nprint('\\ndask_cudf OK\\n')",
        "detail": "packages.rapids.cudf.test_dask",
        "documentation": {}
    },
    {
        "label": "test_cudf_pandas_loaded_to_cudf",
        "kind": 2,
        "importPath": "packages.rapids.cudf.test_pandas",
        "description": "packages.rapids.cudf.test_pandas",
        "peekOfCode": "def test_cudf_pandas_loaded_to_cudf(hybrid_df):\n    #hybrid_df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    cudf_df = cudf.from_pandas(hybrid_df)\n    pd.testing.assert_frame_equal(hybrid_df, cudf_df.to_pandas())\ntest_cudf_pandas_loaded_to_cudf(tips_df)\nprint(\"\\ncuDF <-> Pandas interoperability OK\")",
        "detail": "packages.rapids.cudf.test_pandas",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_pandas",
        "description": "packages.rapids.cudf.test_pandas",
        "peekOfCode": "url = \"https://github.com/plotly/datasets/raw/master/tips.csv\"\ncontent = requests.get(url).content.decode(\"utf-8\")\ntips_df = pd.read_csv(StringIO(content))\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())\n# https://github.com/rapidsai/cudf/blob/branch-23.12/python/cudf/cudf_pandas_tests/test_cudf_pandas_cudf_interop.py\ndef test_cudf_pandas_loaded_to_cudf(hybrid_df):\n    #hybrid_df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    cudf_df = cudf.from_pandas(hybrid_df)",
        "detail": "packages.rapids.cudf.test_pandas",
        "documentation": {}
    },
    {
        "label": "content",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_pandas",
        "description": "packages.rapids.cudf.test_pandas",
        "peekOfCode": "content = requests.get(url).content.decode(\"utf-8\")\ntips_df = pd.read_csv(StringIO(content))\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())\n# https://github.com/rapidsai/cudf/blob/branch-23.12/python/cudf/cudf_pandas_tests/test_cudf_pandas_cudf_interop.py\ndef test_cudf_pandas_loaded_to_cudf(hybrid_df):\n    #hybrid_df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    cudf_df = cudf.from_pandas(hybrid_df)\n    pd.testing.assert_frame_equal(hybrid_df, cudf_df.to_pandas())",
        "detail": "packages.rapids.cudf.test_pandas",
        "documentation": {}
    },
    {
        "label": "tips_df",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_pandas",
        "description": "packages.rapids.cudf.test_pandas",
        "peekOfCode": "tips_df = pd.read_csv(StringIO(content))\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())\n# https://github.com/rapidsai/cudf/blob/branch-23.12/python/cudf/cudf_pandas_tests/test_cudf_pandas_cudf_interop.py\ndef test_cudf_pandas_loaded_to_cudf(hybrid_df):\n    #hybrid_df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    cudf_df = cudf.from_pandas(hybrid_df)\n    pd.testing.assert_frame_equal(hybrid_df, cudf_df.to_pandas())\ntest_cudf_pandas_loaded_to_cudf(tips_df)",
        "detail": "packages.rapids.cudf.test_pandas",
        "documentation": {}
    },
    {
        "label": "tips_df[\"tip_percentage\"]",
        "kind": 5,
        "importPath": "packages.rapids.cudf.test_pandas",
        "description": "packages.rapids.cudf.test_pandas",
        "peekOfCode": "tips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())\n# https://github.com/rapidsai/cudf/blob/branch-23.12/python/cudf/cudf_pandas_tests/test_cudf_pandas_cudf_interop.py\ndef test_cudf_pandas_loaded_to_cudf(hybrid_df):\n    #hybrid_df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    cudf_df = cudf.from_pandas(hybrid_df)\n    pd.testing.assert_frame_equal(hybrid_df, cudf_df.to_pandas())\ntest_cudf_pandas_loaded_to_cudf(tips_df)\nprint(\"\\ncuDF <-> Pandas interoperability OK\")",
        "detail": "packages.rapids.cudf.test_pandas",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.rapids.cuml.config",
        "description": "packages.rapids.cuml.config",
        "peekOfCode": "package['build_args'] = {\n    'CUML_VERSION': 'v21.10.03',  # newer versions require CUDA >= 11.4 (this is a version with some patches in dustynv/cudf fork)\n    'CUML_CMAKE_CUDA_ARCHITECTURES': ';'.join([str(x) for x in CUDA_ARCHITECTURES]),\n}",
        "detail": "packages.rapids.cuml.config",
        "documentation": {}
    },
    {
        "label": "gdf_float",
        "kind": 5,
        "importPath": "packages.rapids.cuml.test",
        "description": "packages.rapids.cuml.test",
        "peekOfCode": "gdf_float = cudf.DataFrame()\ngdf_float['0'] = [1.0, 2.0, 5.0]\ngdf_float['1'] = [4.0, 2.0, 1.0]\ngdf_float['2'] = [4.0, 2.0, 1.0]\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\nprint(dbscan_float.labels_)\nprint('\\ncuml OK\\n')",
        "detail": "packages.rapids.cuml.test",
        "documentation": {}
    },
    {
        "label": "gdf_float['0']",
        "kind": 5,
        "importPath": "packages.rapids.cuml.test",
        "description": "packages.rapids.cuml.test",
        "peekOfCode": "gdf_float['0'] = [1.0, 2.0, 5.0]\ngdf_float['1'] = [4.0, 2.0, 1.0]\ngdf_float['2'] = [4.0, 2.0, 1.0]\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\nprint(dbscan_float.labels_)\nprint('\\ncuml OK\\n')",
        "detail": "packages.rapids.cuml.test",
        "documentation": {}
    },
    {
        "label": "gdf_float['1']",
        "kind": 5,
        "importPath": "packages.rapids.cuml.test",
        "description": "packages.rapids.cuml.test",
        "peekOfCode": "gdf_float['1'] = [4.0, 2.0, 1.0]\ngdf_float['2'] = [4.0, 2.0, 1.0]\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\nprint(dbscan_float.labels_)\nprint('\\ncuml OK\\n')",
        "detail": "packages.rapids.cuml.test",
        "documentation": {}
    },
    {
        "label": "gdf_float['2']",
        "kind": 5,
        "importPath": "packages.rapids.cuml.test",
        "description": "packages.rapids.cuml.test",
        "peekOfCode": "gdf_float['2'] = [4.0, 2.0, 1.0]\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\nprint(dbscan_float.labels_)\nprint('\\ncuml OK\\n')",
        "detail": "packages.rapids.cuml.test",
        "documentation": {}
    },
    {
        "label": "dbscan_float",
        "kind": 5,
        "importPath": "packages.rapids.cuml.test",
        "description": "packages.rapids.cuml.test",
        "peekOfCode": "dbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\nprint(dbscan_float.labels_)\nprint('\\ncuml OK\\n')",
        "detail": "packages.rapids.cuml.test",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.rapids.raft.config",
        "description": "packages.rapids.raft.config",
        "peekOfCode": "package['build_args'] = {\n    'CUDA_ARCHITECTURES': ';'.join([str(x) for x in CUDA_ARCHITECTURES]),\n}",
        "detail": "packages.rapids.raft.config",
        "documentation": {}
    },
    {
        "label": "ros_container",
        "kind": 2,
        "importPath": "packages.ros.config",
        "description": "packages.ros.config",
        "peekOfCode": "def ros_container(name, *packages, distros=ROS2_DISTROS, base_packages='desktop'):\n    if not isinstance(distros, (list, tuple)):\n        distros = [distros]\n    if not isinstance(packages, (list, tuple)):\n        packages = [packages]\n    if not isinstance(base_packages, (list, tuple)):\n        base_packages = [base_packages]\n    packages = ' '.join(packages)\n    if not packages:\n        return",
        "detail": "packages.ros.config",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "packages.ros.config",
        "description": "packages.ros.config",
        "peekOfCode": "template = package.copy()\ntemplate['group'] = 'ros'\ntemplate['depends'] = ['cuda', 'cudnn', 'tensorrt', 'opencv:deb', 'cmake']\ntemplate['postfix'] = f\"l4t-r{L4T_VERSION}\"\ntemplate['docs'] = \"docs.md\"\npackage = []\nfor ROS_DISTRO in ROS_DISTROS:\n    for ROS_PACKAGE in ROS_PACKAGES:\n        pkg = copy.deepcopy(template)\n        pkg['name'] = f\"ros:{ROS_DISTRO}-{ROS_PACKAGE.replace('_', '-')}\"",
        "detail": "packages.ros.config",
        "documentation": {}
    },
    {
        "label": "template['group']",
        "kind": 5,
        "importPath": "packages.ros.config",
        "description": "packages.ros.config",
        "peekOfCode": "template['group'] = 'ros'\ntemplate['depends'] = ['cuda', 'cudnn', 'tensorrt', 'opencv:deb', 'cmake']\ntemplate['postfix'] = f\"l4t-r{L4T_VERSION}\"\ntemplate['docs'] = \"docs.md\"\npackage = []\nfor ROS_DISTRO in ROS_DISTROS:\n    for ROS_PACKAGE in ROS_PACKAGES:\n        pkg = copy.deepcopy(template)\n        pkg['name'] = f\"ros:{ROS_DISTRO}-{ROS_PACKAGE.replace('_', '-')}\"\n        pkg['build_args'] = {",
        "detail": "packages.ros.config",
        "documentation": {}
    },
    {
        "label": "template['depends']",
        "kind": 5,
        "importPath": "packages.ros.config",
        "description": "packages.ros.config",
        "peekOfCode": "template['depends'] = ['cuda', 'cudnn', 'tensorrt', 'opencv:deb', 'cmake']\ntemplate['postfix'] = f\"l4t-r{L4T_VERSION}\"\ntemplate['docs'] = \"docs.md\"\npackage = []\nfor ROS_DISTRO in ROS_DISTROS:\n    for ROS_PACKAGE in ROS_PACKAGES:\n        pkg = copy.deepcopy(template)\n        pkg['name'] = f\"ros:{ROS_DISTRO}-{ROS_PACKAGE.replace('_', '-')}\"\n        pkg['build_args'] = {\n            'ROS_VERSION': ROS_DISTRO,",
        "detail": "packages.ros.config",
        "documentation": {}
    },
    {
        "label": "template['postfix']",
        "kind": 5,
        "importPath": "packages.ros.config",
        "description": "packages.ros.config",
        "peekOfCode": "template['postfix'] = f\"l4t-r{L4T_VERSION}\"\ntemplate['docs'] = \"docs.md\"\npackage = []\nfor ROS_DISTRO in ROS_DISTROS:\n    for ROS_PACKAGE in ROS_PACKAGES:\n        pkg = copy.deepcopy(template)\n        pkg['name'] = f\"ros:{ROS_DISTRO}-{ROS_PACKAGE.replace('_', '-')}\"\n        pkg['build_args'] = {\n            'ROS_VERSION': ROS_DISTRO,\n            'ROS_PACKAGE': ROS_PACKAGE",
        "detail": "packages.ros.config",
        "documentation": {}
    },
    {
        "label": "template['docs']",
        "kind": 5,
        "importPath": "packages.ros.config",
        "description": "packages.ros.config",
        "peekOfCode": "template['docs'] = \"docs.md\"\npackage = []\nfor ROS_DISTRO in ROS_DISTROS:\n    for ROS_PACKAGE in ROS_PACKAGES:\n        pkg = copy.deepcopy(template)\n        pkg['name'] = f\"ros:{ROS_DISTRO}-{ROS_PACKAGE.replace('_', '-')}\"\n        pkg['build_args'] = {\n            'ROS_VERSION': ROS_DISTRO,\n            'ROS_PACKAGE': ROS_PACKAGE\n        }",
        "detail": "packages.ros.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.ros.config",
        "description": "packages.ros.config",
        "peekOfCode": "package = []\nfor ROS_DISTRO in ROS_DISTROS:\n    for ROS_PACKAGE in ROS_PACKAGES:\n        pkg = copy.deepcopy(template)\n        pkg['name'] = f\"ros:{ROS_DISTRO}-{ROS_PACKAGE.replace('_', '-')}\"\n        pkg['build_args'] = {\n            'ROS_VERSION': ROS_DISTRO,\n            'ROS_PACKAGE': ROS_PACKAGE\n        }\n        if ROS_DISTRO == 'melodic':",
        "detail": "packages.ros.config",
        "documentation": {}
    },
    {
        "label": "ROS1_DISTROS",
        "kind": 5,
        "importPath": "packages.ros.version",
        "description": "packages.ros.version",
        "peekOfCode": "ROS1_DISTROS = ['melodic', 'noetic']\nROS2_DISTROS = ['foxy', 'galactic', 'humble', 'iron', 'jazzy']\nROS_DISTROS = ROS1_DISTROS + ROS2_DISTROS\nROS_PACKAGES = ['ros_base', 'ros_core', 'desktop']",
        "detail": "packages.ros.version",
        "documentation": {}
    },
    {
        "label": "ROS2_DISTROS",
        "kind": 5,
        "importPath": "packages.ros.version",
        "description": "packages.ros.version",
        "peekOfCode": "ROS2_DISTROS = ['foxy', 'galactic', 'humble', 'iron', 'jazzy']\nROS_DISTROS = ROS1_DISTROS + ROS2_DISTROS\nROS_PACKAGES = ['ros_base', 'ros_core', 'desktop']",
        "detail": "packages.ros.version",
        "documentation": {}
    },
    {
        "label": "ROS_DISTROS",
        "kind": 5,
        "importPath": "packages.ros.version",
        "description": "packages.ros.version",
        "peekOfCode": "ROS_DISTROS = ROS1_DISTROS + ROS2_DISTROS\nROS_PACKAGES = ['ros_base', 'ros_core', 'desktop']",
        "detail": "packages.ros.version",
        "documentation": {}
    },
    {
        "label": "ROS_PACKAGES",
        "kind": 5,
        "importPath": "packages.ros.version",
        "description": "packages.ros.version",
        "peekOfCode": "ROS_PACKAGES = ['ros_base', 'ros_core', 'desktop']",
        "detail": "packages.ros.version",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "packages.sim.mimicgen.test",
        "description": "packages.sim.mimicgen.test",
        "peekOfCode": "def generate(task='Stack_D0', robot='Panda', gripper='PandaGripper', \n             camera='frontview', camera_width=512, camera_height=512, \n             frames=20, output=None, **kwargs):\n    \"\"\"\n    Generate a sample video of the environment\n    \"\"\"\n    env = rs.make(\n        env_name=task, \n        robots=robot,\n        gripper_types=gripper,",
        "detail": "packages.sim.mimicgen.test",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.sim.mimicgen.test",
        "description": "packages.sim.mimicgen.test",
        "peekOfCode": "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--tasks', type=str, nargs='+', default=['Stack_D0', 'Stack_D1'], choices=list(rs.ALL_ENVIRONMENTS), help=\"one or more tasks to generate episodes for\")\nparser.add_argument('--robots', type=str, nargs='+', default=['Panda'], choices=list(rs.ALL_ROBOTS), help=\"one or more robots to generate episodes for\")\nparser.add_argument('--grippers', type=str, nargs='+', default=['PandaGripper'], choices=list(rs.ALL_GRIPPERS), help=\"one or more grippers to generate episodes for\")\nparser.add_argument('--cameras', type=str, nargs='+', default=['agentview', 'frontview', 'robot0_eye_in_hand'], help=\"one or more camera views to render\")\nparser.add_argument('--camera-width', type=int, default=512, help=\"the width (in pixels) of the camera\")\nparser.add_argument('--camera-height', type=int, default=512, help=\"the height (in pixels) of the camera\")\nparser.add_argument('--frames', type=int, default=60, help=\"the number of frames to generate per configuration\")\nparser.add_argument('--parallel', type=int, default=4, help=\"the number of parallel processes to run\")\nparser.add_argument('--output', type=str, default=\"/data/sim/mimicgen/test\", help=\"output directory of the demo videos\")",
        "detail": "packages.sim.mimicgen.test",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.sim.mimicgen.test",
        "description": "packages.sim.mimicgen.test",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\ndef generate(task='Stack_D0', robot='Panda', gripper='PandaGripper', \n             camera='frontview', camera_width=512, camera_height=512, \n             frames=20, output=None, **kwargs):\n    \"\"\"\n    Generate a sample video of the environment\n    \"\"\"\n    env = rs.make(\n        env_name=task, ",
        "detail": "packages.sim.mimicgen.test",
        "documentation": {}
    },
    {
        "label": "generations",
        "kind": 5,
        "importPath": "packages.sim.mimicgen.test",
        "description": "packages.sim.mimicgen.test",
        "peekOfCode": "generations = 0\nfor task in args.tasks:\n    for robot in args.robots:\n        for gripper in args.grippers:\n            for camera in args.cameras:\n                config = dict(task=task, robot=robot, gripper=gripper, camera=camera, **vars(args))\n                if args.parallel:\n                    pool_results.append(pool.apply_async(\n                        generate, kwds=config\n                    ))",
        "detail": "packages.sim.mimicgen.test",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "packages.sim.robosuite.test",
        "description": "packages.sim.robosuite.test",
        "peekOfCode": "def generate(task='Lift', robot='Panda', gripper='PandaGripper', \n             camera='frontview', camera_width=512, camera_height=512, \n             frames=20, output=None, **kwargs):\n    \"\"\"\n    Generate a sample video of the environment\n    \"\"\"\n    env = rs.make(\n        env_name=task, \n        robots=robot,\n        gripper_types=gripper,",
        "detail": "packages.sim.robosuite.test",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.sim.robosuite.test",
        "description": "packages.sim.robosuite.test",
        "peekOfCode": "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--tasks', type=str, nargs='+', default=['Lift', 'Stack'], choices=list(rs.ALL_ENVIRONMENTS), help=\"one or more tasks to generate episodes for\")\nparser.add_argument('--robots', type=str, nargs='+', default=['Panda'], choices=list(rs.ALL_ROBOTS), help=\"one or more robots to generate episodes for\")\nparser.add_argument('--grippers', type=str, nargs='+', default=['PandaGripper'], choices=list(rs.ALL_GRIPPERS), help=\"one or more grippers to generate episodes for\")\nparser.add_argument('--cameras', type=str, nargs='+', default=['agentview', 'frontview', 'robot0_eye_in_hand'], help=\"one or more camera views to render\")\nparser.add_argument('--camera-width', type=int, default=512, help=\"the width (in pixels) of the camera\")\nparser.add_argument('--camera-height', type=int, default=512, help=\"the height (in pixels) of the camera\")\nparser.add_argument('--frames', type=int, default=60, help=\"the number of frames to generate per configuration\")\nparser.add_argument('--parallel', type=int, default=8, help=\"the number of parallel processes to run\")\nparser.add_argument('--output', type=str, default=\"/data/sim/robosuite/test\", help=\"output directory of the demo videos\")",
        "detail": "packages.sim.robosuite.test",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.sim.robosuite.test",
        "description": "packages.sim.robosuite.test",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\ndef generate(task='Lift', robot='Panda', gripper='PandaGripper', \n             camera='frontview', camera_width=512, camera_height=512, \n             frames=20, output=None, **kwargs):\n    \"\"\"\n    Generate a sample video of the environment\n    \"\"\"\n    env = rs.make(\n        env_name=task, ",
        "detail": "packages.sim.robosuite.test",
        "documentation": {}
    },
    {
        "label": "generations",
        "kind": 5,
        "importPath": "packages.sim.robosuite.test",
        "description": "packages.sim.robosuite.test",
        "peekOfCode": "generations = 0\nfor task in args.tasks:\n    for robot in args.robots:\n        for gripper in args.grippers:\n            for camera in args.cameras:\n                config = dict(task=task, robot=robot, gripper=gripper, camera=camera, **vars(args))\n                if args.parallel:\n                    pool_results.append(pool.apply_async(\n                        generate, kwds=config\n                    ))",
        "detail": "packages.sim.robosuite.test",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.smart-home.homeassistant-base.config",
        "description": "packages.smart-home.homeassistant-base.config",
        "peekOfCode": "package['build_args'] = {\n    'BASHIO_VERSION': '0.16.2',\n    'TEMPIO_VERSION': '2021.09.0',\n    'S6_OVERLAY_VERSION': '3.1.6.2',\n}",
        "detail": "packages.smart-home.homeassistant-base.config",
        "documentation": {}
    },
    {
        "label": "get_latest_stable_version",
        "kind": 2,
        "importPath": "packages.smart-home.homeassistant-core.config",
        "description": "packages.smart-home.homeassistant-core.config",
        "peekOfCode": "def get_latest_stable_version(fallback='2024.3.1'):\n    try:\n        response = requests.get('https://version.home-assistant.io/stable.json')\n        if response.status_code == 200:\n            data = response.json()\n            return data.get('homeassistant', { 'default': fallback }).get('default', fallback).strip()\n        else:\n            print(\"Failed to fetch version information. Status code:\", response.status_code)\n            return fallback\n    except Exception as e:",
        "detail": "packages.smart-home.homeassistant-core.config",
        "documentation": {}
    },
    {
        "label": "create_package",
        "kind": 2,
        "importPath": "packages.smart-home.homeassistant-core.config",
        "description": "packages.smart-home.homeassistant-core.config",
        "peekOfCode": "def create_package(version, default=False) -> list:\n    pkg = package.copy()\n    wanted_version = get_latest_stable_version() if version == 'latest' else version\n    pkg['name'] = f'homeassistant-core:{version}'\n    ha_version = Version(wanted_version)\n    if ha_version.major >= 2024:\n        if ha_version.minor >= 4:\n            required_python = 'python:3.12'\n        else:\n            required_python = 'python:3.11'",
        "detail": "packages.smart-home.homeassistant-core.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.smart-home.homeassistant-core.config",
        "description": "packages.smart-home.homeassistant-core.config",
        "peekOfCode": "package = [\n    # latest\n    create_package('latest', default=True),\n    # specific version\n    create_package('2024.4.2', default=False),\n    # create_package('2024.3.1', default=True),\n]",
        "detail": "packages.smart-home.homeassistant-core.config",
        "documentation": {}
    },
    {
        "label": "iso_string",
        "kind": 5,
        "importPath": "packages.smart-home.homeassistant-core.test_ciso8601",
        "description": "packages.smart-home.homeassistant-core.test_ciso8601",
        "peekOfCode": "iso_string = \"2024-03-18T12:00:00Z\"\nparsed_date = ciso8601.parse_datetime(iso_string)\nprint(\"ciso8601 OK\")",
        "detail": "packages.smart-home.homeassistant-core.test_ciso8601",
        "documentation": {}
    },
    {
        "label": "parsed_date",
        "kind": 5,
        "importPath": "packages.smart-home.homeassistant-core.test_ciso8601",
        "description": "packages.smart-home.homeassistant-core.test_ciso8601",
        "peekOfCode": "parsed_date = ciso8601.parse_datetime(iso_string)\nprint(\"ciso8601 OK\")",
        "detail": "packages.smart-home.homeassistant-core.test_ciso8601",
        "documentation": {}
    },
    {
        "label": "get_latest_stable_version",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.assist-microphone.config",
        "description": "packages.smart-home.wyoming.assist-microphone.config",
        "peekOfCode": "def get_latest_stable_version(fallback=\"1.2.0\") -> str:\n    try:\n        response = requests.get('https://raw.githubusercontent.com/rhasspy/wyoming-satellite/master/wyoming_satellite/VERSION')\n        if response.status_code == 200:\n            return response.text.strip()\n        else:\n            print(\"Failed to fetch version information. Status code:\", response.status_code)\n            return fallback\n    except Exception as e:\n        print(\"An error occurred:\", e)",
        "detail": "packages.smart-home.wyoming.assist-microphone.config",
        "documentation": {}
    },
    {
        "label": "create_package",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.assist-microphone.config",
        "description": "packages.smart-home.wyoming.assist-microphone.config",
        "peekOfCode": "def create_package(version, default=False) -> list:\n    pkg = package.copy()\n    wanted_version = get_latest_stable_version() if version == 'latest' else version\n    pkg['name'] = f'wyoming-assist-microphone:{version}'\n    pkg['build_args'] = {\n        'SATELLITE_VERSION': wanted_version,\n    }\n    if default:\n        pkg['alias'] = 'wyoming-assist-microphone'\n    return pkg",
        "detail": "packages.smart-home.wyoming.assist-microphone.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.smart-home.wyoming.assist-microphone.config",
        "description": "packages.smart-home.wyoming.assist-microphone.config",
        "peekOfCode": "package = [\n    create_package(\"latest\", default=True),\n]",
        "detail": "packages.smart-home.wyoming.assist-microphone.config",
        "documentation": {}
    },
    {
        "label": "get_latest_stable_version",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.openwakeword.config",
        "description": "packages.smart-home.wyoming.openwakeword.config",
        "peekOfCode": "def get_latest_stable_version(fallback=\"1.10.0\") -> str:\n    try:\n        response = requests.get('https://raw.githubusercontent.com/rhasspy/wyoming-openwakeword/master/wyoming_openwakeword/VERSION')\n        if response.status_code == 200:\n            return response.text.strip()\n        else:\n            print(\"Failed to fetch version information. Status code:\", response.status_code)\n            return fallback\n    except Exception as e:\n        print(\"An error occurred:\", e)",
        "detail": "packages.smart-home.wyoming.openwakeword.config",
        "documentation": {}
    },
    {
        "label": "create_package",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.openwakeword.config",
        "description": "packages.smart-home.wyoming.openwakeword.config",
        "peekOfCode": "def create_package(version, default=False) -> list:\n    pkg = package.copy()\n    wanted_version = get_latest_stable_version() if version == 'latest' else version\n    pkg['name'] = f'wyoming-openwakeword:{version}'\n    pkg['build_args'] = {\n        'WYOMING_OPENWAKEWORD_VERSION': wanted_version,\n    }\n    if default:\n        pkg['alias'] = 'wyoming-openwakeword'\n    return pkg",
        "detail": "packages.smart-home.wyoming.openwakeword.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.smart-home.wyoming.openwakeword.config",
        "description": "packages.smart-home.wyoming.openwakeword.config",
        "peekOfCode": "package = [\n    create_package(\"latest\", default=True),\n]",
        "detail": "packages.smart-home.wyoming.openwakeword.config",
        "documentation": {}
    },
    {
        "label": "get_latest_stable_version",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.piper.config",
        "description": "packages.smart-home.wyoming.piper.config",
        "peekOfCode": "def get_latest_stable_version(fallback=\"v1.5.0\") -> str:\n    try:\n        response = requests.get('https://raw.githubusercontent.com/rhasspy/wyoming-piper/master/wyoming_piper/VERSION')\n        if response.status_code == 200:\n            return f\"v{response.text.strip()}\"\n        else:\n            print(\"Failed to fetch version information. Status code:\", response.status_code)\n            return fallback\n    except Exception as e:\n        print(\"An error occurred:\", e)",
        "detail": "packages.smart-home.wyoming.piper.config",
        "documentation": {}
    },
    {
        "label": "create_package",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.piper.config",
        "description": "packages.smart-home.wyoming.piper.config",
        "peekOfCode": "def create_package(version, default=False) -> list:\n    pkg = package.copy()\n    wanted_version = get_latest_stable_version() if version == 'latest' else version\n    pkg['name'] = f'wyoming-piper:{version}'\n    pkg['build_args'] = {\n        'WYOMING_PIPER_VERSION': wanted_version,\n    }\n    if default:\n        pkg['alias'] = 'wyoming-piper'\n    return pkg",
        "detail": "packages.smart-home.wyoming.piper.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.smart-home.wyoming.piper.config",
        "description": "packages.smart-home.wyoming.piper.config",
        "peekOfCode": "package = [\n    create_package(\"master\", default=True),\n]",
        "detail": "packages.smart-home.wyoming.piper.config",
        "documentation": {}
    },
    {
        "label": "get_latest_stable_version",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.wyoming-whisper.config",
        "description": "packages.smart-home.wyoming.wyoming-whisper.config",
        "peekOfCode": "def get_latest_stable_version(fallback=\"2.1.0\") -> str:\n    try:\n        response = requests.get('https://raw.githubusercontent.com/rhasspy/wyoming-faster-whisper/master/wyoming_faster_whisper/VERSION')\n        if response.status_code == 200:\n            return response.text.strip()\n        else:\n            print(\"Failed to fetch version information. Status code:\", response.status_code)\n            return fallback\n    except Exception as e:\n        print(\"An error occurred:\", e)",
        "detail": "packages.smart-home.wyoming.wyoming-whisper.config",
        "documentation": {}
    },
    {
        "label": "create_package",
        "kind": 2,
        "importPath": "packages.smart-home.wyoming.wyoming-whisper.config",
        "description": "packages.smart-home.wyoming.wyoming-whisper.config",
        "peekOfCode": "def create_package(version, default=False) -> list:\n    pkg = package.copy()\n    wanted_version = get_latest_stable_version() if version == 'latest' else version\n    pkg['name'] = f'wyoming-whisper:{version}'\n    pkg['build_args'] = {\n        'WYOMING_WHISPER_VERSION': wanted_version,\n    }\n    if default:\n        pkg['alias'] = 'wyoming-whisper'\n    return pkg",
        "detail": "packages.smart-home.wyoming.wyoming-whisper.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.smart-home.wyoming.wyoming-whisper.config",
        "description": "packages.smart-home.wyoming.wyoming-whisper.config",
        "peekOfCode": "package = [\n    create_package(\"latest\", default=True),\n]",
        "detail": "packages.smart-home.wyoming.wyoming-whisper.config",
        "documentation": {}
    },
    {
        "label": "tf_pack",
        "kind": 5,
        "importPath": "packages.tensorflow.config",
        "description": "packages.tensorflow.config",
        "peekOfCode": "tf_pack = package\npackage = {}\nif TENSORFLOW1_WHL:\n    tf1 = tf_pack.copy()\n    tf1['build_args'] = {\n        'TENSORFLOW_URL': TENSORFLOW1_URL,\n        'TENSORFLOW_WHL': TENSORFLOW1_WHL\n    }\n    package['tensorflow'] = tf1\nif TENSORFLOW2_WHL:",
        "detail": "packages.tensorflow.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.tensorflow.config",
        "description": "packages.tensorflow.config",
        "peekOfCode": "package = {}\nif TENSORFLOW1_WHL:\n    tf1 = tf_pack.copy()\n    tf1['build_args'] = {\n        'TENSORFLOW_URL': TENSORFLOW1_URL,\n        'TENSORFLOW_WHL': TENSORFLOW1_WHL\n    }\n    package['tensorflow'] = tf1\nif TENSORFLOW2_WHL:\n    tf2 = tf_pack.copy()",
        "detail": "packages.tensorflow.config",
        "documentation": {}
    },
    {
        "label": "tensorrt_deb",
        "kind": 2,
        "importPath": "packages.tensorrt.config",
        "description": "packages.tensorrt.config",
        "peekOfCode": "def tensorrt_deb(version, url, deb, cudnn=None, packages=None, requires=None):\n    \"\"\"\n    Generate containers for a particular version of TensorRT installed from debian packages\n    \"\"\"\n    if not packages:\n        packages = os.environ.get('TENSORRT_PACKAGES', 'tensorrt tensorrt-libs python3-libnvinfer-dev')\n    tensorrt = package.copy()\n    tensorrt['name'] = f'tensorrt:{version}'\n    tensorrt['dockerfile'] = 'Dockerfile.deb'\n    tensorrt['build_args'] = {",
        "detail": "packages.tensorrt.config",
        "documentation": {}
    },
    {
        "label": "tensorrt_tar",
        "kind": 2,
        "importPath": "packages.tensorrt.config",
        "description": "packages.tensorrt.config",
        "peekOfCode": "def tensorrt_tar(version, url, cudnn=None, requires=None):\n    \"\"\"\n    Generate containers for a particular version of TensorRT installed from tar.gz file\n    \"\"\"\n    tensorrt = package.copy()\n    tensorrt['name'] = f'tensorrt:{version}'\n    tensorrt['dockerfile'] = 'Dockerfile.tar'\n    tensorrt['build_args'] = {'TENSORRT_URL': url}\n    if Version(version) == TENSORRT_VERSION:\n        tensorrt['alias'] = 'tensorrt'",
        "detail": "packages.tensorrt.config",
        "documentation": {}
    },
    {
        "label": "tensorrt_builtin",
        "kind": 2,
        "importPath": "packages.tensorrt.config",
        "description": "packages.tensorrt.config",
        "peekOfCode": "def tensorrt_builtin(version=None, requires=None, default=False):\n    \"\"\"\n    Backwards-compatability for when TensorRT already installed in base container (like l4t-jetpack)\n    \"\"\"\n    passthrough = package.copy()\n    if version is not None:\n        if not isinstance(version, str):\n            version = f'{version.major}.{version.minor}'\n        if default:\n            passthrough['alias'] = 'tensorrt'  ",
        "detail": "packages.tensorrt.config",
        "documentation": {}
    },
    {
        "label": "package['depends']",
        "kind": 5,
        "importPath": "packages.tensorrt.config",
        "description": "packages.tensorrt.config",
        "peekOfCode": "package['depends'] = ['cuda', 'cudnn', 'python']\npackage['test'] = ['test.sh']\nif 'TENSORRT_VERSION' in os.environ and len(os.environ['TENSORRT_VERSION']) > 0:\n    TENSORRT_VERSION = Version(os.environ['TENSORRT_VERSION'])\nelse:\n    if L4T_VERSION.major >= 36:\n        if CUDA_VERSION >= Version('12.4'):\n            TENSORRT_VERSION = Version('10.0')\n        else:\n            TENSORRT_VERSION = Version('8.6')",
        "detail": "packages.tensorrt.config",
        "documentation": {}
    },
    {
        "label": "package['test']",
        "kind": 5,
        "importPath": "packages.tensorrt.config",
        "description": "packages.tensorrt.config",
        "peekOfCode": "package['test'] = ['test.sh']\nif 'TENSORRT_VERSION' in os.environ and len(os.environ['TENSORRT_VERSION']) > 0:\n    TENSORRT_VERSION = Version(os.environ['TENSORRT_VERSION'])\nelse:\n    if L4T_VERSION.major >= 36:\n        if CUDA_VERSION >= Version('12.4'):\n            TENSORRT_VERSION = Version('10.0')\n        else:\n            TENSORRT_VERSION = Version('8.6')\n    elif L4T_VERSION.major >= 34:",
        "detail": "packages.tensorrt.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.tensorrt.config",
        "description": "packages.tensorrt.config",
        "peekOfCode": "package = [\n    # JetPack 6\n    tensorrt_deb('8.6', 'https://nvidia.box.com/shared/static/hmwr57hm88bxqrycvlyma34c3k4c53t9.deb', 'nv-tensorrt-local-repo-l4t-8.6.2-cuda-12.2', cudnn='8.9', requires=['==r36.*', '==cu122']), \n    #tensorrt_tar('9.3', 'https://nvidia.box.com/shared/static/fp3o14iq7qbm67qjuqivdrdch7009axu.gz', cudnn='8.9', requires=['==r36.*', '==cu122']), \n    tensorrt_tar('10.0', 'https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.0.1/tars/TensorRT-10.0.1.6.l4t.aarch64-gnu.cuda-12.4.tar.gz', cudnn='9.0', requires=['==r36.*', '==cu124']), \n    # JetPack 4-5 (TensorRT installed in base container)\n    tensorrt_builtin(requires='<36', default=True),\n]",
        "detail": "packages.tensorrt.config",
        "documentation": {}
    },
    {
        "label": "TRITON_CLIENTS",
        "kind": 5,
        "importPath": "packages.tritonserver.config",
        "description": "packages.tritonserver.config",
        "peekOfCode": "TRITON_CLIENTS = 'clients'\nif L4T_VERSION >= Version('36.2.0'): # JetPack 6.0 DP\n    # https://github.com/triton-inference-server/server/releases/tag/v2.42.0\n    TRITON_URL = 'https://github.com/triton-inference-server/server/releases/download/v2.42.0/tritonserver2.42.0-igpu.tar.gz'\n    TRITON_TAR = 'tritonserver2.42.0-igpu.tar.gz'\n    TRITON_CLIENTS = 'tritonserver/clients'\nelif L4T_VERSION >= Version('35.3.1'): # JetPack 5.1.1\n    # https://github.com/triton-inference-server/server/releases/tag/v2.35.0\n    TRITON_URL = 'https://github.com/triton-inference-server/server/releases/download/v2.35.0/tritonserver2.35.0-jetpack5.1.2-update-1.tgz'\n    TRITON_TAR = 'tritonserver2.35.0-jetpack5.1.2-update-1.tgz'",
        "detail": "packages.tritonserver.config",
        "documentation": {}
    },
    {
        "label": "package['build_args']",
        "kind": 5,
        "importPath": "packages.tvm.config",
        "description": "packages.tvm.config",
        "peekOfCode": "package['build_args'] = {\n    'CUDAARCHS': ';'.join([str(x) for x in CUDA_ARCHITECTURES]),\n    'TORCH_CUDA_ARCH_LIST': ';'.join([f'{x/10:.1f}' for x in CUDA_ARCHITECTURES])\n}",
        "detail": "packages.tvm.config",
        "documentation": {}
    },
    {
        "label": "print_index_stats",
        "kind": 2,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "def print_index_stats():      \n    print(f\"{args.index} index size:       ({index.ntotal}, {args.dim})\")\n    print(f\"{args.index} index time:       {avg_index_time*1000:.2f} ms\")\n    print(f\"{args.index} index rate:       {avg_index_rate:.1f} vectors/sec\") \n    print(f\"{args.index} index bandwidth:  {avg_index_rate*vector_size:.1f} MB/s\") \n    print(f\"{args.index} index trained:    {index.is_trained}\")\n# profile search\navg_search_time = 0\navg_search_rate = 0\navg_factor = 1.0 / (args.search_queries / args.search_batch)",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "print_search_stats",
        "kind": 2,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "def print_search_stats():\n    print(f\"{args.index} search size:      ({args.search_batch}, {args.dim})\")\n    print(f\"{args.index} search time:      {avg_search_time*1000:.2f} ms\")\n    print(f\"{args.index} search rate:      {avg_search_rate:.1f} vectors/sec\") \n    print(f\"{args.index} search bandwidth: {avg_search_rate*vector_size:.1f} MB/s\") \nprint(\"\\n\")\nprint_index_stats()\nprint(\"\")\nprint_search_stats()\n# https://github.com/facebookresearch/faiss/wiki/FAQ#why-does-the-ram-usage-not-go-down-when-i-delete-an-index",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-k', type=int, default=4, help='the number of nearest neighbors to search for')\nparser.add_argument('-d', '--dim', type=int, default=5120, help='the dimensionality of the embedding vectors')  # 2621440\nparser.add_argument('--index', type=str, default='Flat', help='the type of index to use')  # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\nparser.add_argument('--index-size', type=int, default=4096, help='the number of vectors to add to the index')\nparser.add_argument('--index-batch', type=int, default=1, help='the number of vectors to add to index at a time')\nparser.add_argument('--search-queries', type=int, default=4096, help='the number of search queries to run')\nparser.add_argument('--search-batch', type=int, default=1, help='the number of search queries to run at a time')\nparser.add_argument('--dtype', type=str, default='float32', help='datatype of the vectors')\nparser.add_argument('--seed', type=int, default=1234, help='change the random seed used')",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\nnp.random.seed(args.seed)\nprint(f\"building random numpy arrays ({args.index_size}, {args.dim})\")\nxb = np.random.random((args.index_size, args.dim)).astype(args.dtype)\nxb[:, 0] += np.arange(args.index_size) / 1000.\nxq = np.random.random((args.search_queries, args.dim)).astype(args.dtype)\nxq[:, 0] += np.arange(args.search_queries) / 1000.\nprint(xb.shape, xb.dtype)\nvector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "xb",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "xb = np.random.random((args.index_size, args.dim)).astype(args.dtype)\nxb[:, 0] += np.arange(args.index_size) / 1000.\nxq = np.random.random((args.search_queries, args.dim)).astype(args.dtype)\nxq[:, 0] += np.arange(args.search_queries) / 1000.\nprint(xb.shape, xb.dtype)\nvector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB\nprint(f\"vector size:         {vector_size*1024*1024:.0f} bytes\")\nprint(f\"numpy array size:    {(xb.size * xb.itemsize) / (1024*1024):.3f} MB\")\nprint(f\"creating index type: {args.index}\")\nindex = faiss.index_factory(args.dim, args.index) #faiss.IndexFlatL2(args.dim)",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "xq",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "xq = np.random.random((args.search_queries, args.dim)).astype(args.dtype)\nxq[:, 0] += np.arange(args.search_queries) / 1000.\nprint(xb.shape, xb.dtype)\nvector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB\nprint(f\"vector size:         {vector_size*1024*1024:.0f} bytes\")\nprint(f\"numpy array size:    {(xb.size * xb.itemsize) / (1024*1024):.3f} MB\")\nprint(f\"creating index type: {args.index}\")\nindex = faiss.index_factory(args.dim, args.index) #faiss.IndexFlatL2(args.dim)\nif not args.cpu:\n    res = faiss.StandardGpuResources()  # use a single GPU",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "vector_size",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "vector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB\nprint(f\"vector size:         {vector_size*1024*1024:.0f} bytes\")\nprint(f\"numpy array size:    {(xb.size * xb.itemsize) / (1024*1024):.3f} MB\")\nprint(f\"creating index type: {args.index}\")\nindex = faiss.index_factory(args.dim, args.index) #faiss.IndexFlatL2(args.dim)\nif not args.cpu:\n    res = faiss.StandardGpuResources()  # use a single GPU\n    index = faiss.index_cpu_to_gpu(res, 0, index)\nif not index.is_trained:\n    print(f\"training index {args.index}\")",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "index = faiss.index_factory(args.dim, args.index) #faiss.IndexFlatL2(args.dim)\nif not args.cpu:\n    res = faiss.StandardGpuResources()  # use a single GPU\n    index = faiss.index_cpu_to_gpu(res, 0, index)\nif not index.is_trained:\n    print(f\"training index {args.index}\")\n    index.train(xb)\n# profile indexing\navg_index_time = 0\navg_index_rate = 0",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_index_time",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "avg_index_time = 0\navg_index_rate = 0\navg_factor = 1.0 / (args.index_size / args.index_batch)\nfor i in range(0, args.index_size, args.index_batch):\n    time_begin = time.perf_counter()\n    index.add(xb[i:i+args.index_batch])\n    index_time = time.perf_counter() - time_begin\n    index_rate = args.index_batch / index_time\n    avg_index_time += index_time * avg_factor\n    avg_index_rate += index_rate * avg_factor",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_index_rate",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "avg_index_rate = 0\navg_factor = 1.0 / (args.index_size / args.index_batch)\nfor i in range(0, args.index_size, args.index_batch):\n    time_begin = time.perf_counter()\n    index.add(xb[i:i+args.index_batch])\n    index_time = time.perf_counter() - time_begin\n    index_rate = args.index_batch / index_time\n    avg_index_time += index_time * avg_factor\n    avg_index_rate += index_rate * avg_factor\n    if i % 32 == 0:",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_factor",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "avg_factor = 1.0 / (args.index_size / args.index_batch)\nfor i in range(0, args.index_size, args.index_batch):\n    time_begin = time.perf_counter()\n    index.add(xb[i:i+args.index_batch])\n    index_time = time.perf_counter() - time_begin\n    index_rate = args.index_batch / index_time\n    avg_index_time += index_time * avg_factor\n    avg_index_rate += index_rate * avg_factor\n    if i % 32 == 0:\n        print(f\"added ({args.index_batch}, {args.dim}) vectors:  {index_time*1000:.2f} ms,  {index_rate:.1f} vectors/sec,  {index_rate*vector_size:.1f} MB/s\")",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_search_time",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "avg_search_time = 0\navg_search_rate = 0\navg_factor = 1.0 / (args.search_queries / args.search_batch)\nfor i in range(0, args.search_queries, args.search_batch): \n    time_begin = time.perf_counter()\n    D, I = index.search(xq[i:i+args.search_batch], args.k)\n    search_time = time.perf_counter() - time_begin\n    search_rate = args.search_batch / search_time\n    avg_search_time += search_time * avg_factor\n    avg_search_rate += search_rate * avg_factor",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_search_rate",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "avg_search_rate = 0\navg_factor = 1.0 / (args.search_queries / args.search_batch)\nfor i in range(0, args.search_queries, args.search_batch): \n    time_begin = time.perf_counter()\n    D, I = index.search(xq[i:i+args.search_batch], args.k)\n    search_time = time.perf_counter() - time_begin\n    search_rate = args.search_batch / search_time\n    avg_search_time += search_time * avg_factor\n    avg_search_rate += search_rate * avg_factor\n    #if i % 32 == 0:",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_factor",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "avg_factor = 1.0 / (args.search_queries / args.search_batch)\nfor i in range(0, args.search_queries, args.search_batch): \n    time_begin = time.perf_counter()\n    D, I = index.search(xq[i:i+args.search_batch], args.k)\n    search_time = time.perf_counter() - time_begin\n    search_rate = args.search_batch / search_time\n    avg_search_time += search_time * avg_factor\n    avg_search_rate += search_rate * avg_factor\n    #if i % 32 == 0:\n    print(f\"search ({args.search_batch}, {args.dim}) vectors:  {search_time*1000:.2f} ms,  {search_rate:.1f} vectors/sec,  {search_rate*vector_size:.1f} MB/s\")",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.benchmark",
        "description": "packages.vectordb.faiss.benchmark",
        "peekOfCode": "memory_usage = faiss.get_mem_usage_kb() / 1024\nprint(f\"\\nPeak memory usage:     {memory_usage:.1f} MB\")\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, api, device, index, dtype, vector_dim, num_vectors, \")\n            file.write(f\"index_batch, index_time, index_rate, index_bandwidth, \")\n            file.write(f\"search_batch, search_time, search_rate, search_bandwidth, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, faiss, \")",
        "detail": "packages.vectordb.faiss.benchmark",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 2,
        "importPath": "packages.vectordb.faiss.config",
        "description": "packages.vectordb.faiss.config",
        "peekOfCode": "def faiss(version, branch=None, requires=None, default=False):\n    pkg = package.copy()\n    pkg['name'] = f'faiss:{version}'\n    if len(version.split('.')) < 3:\n        version = version + '.0'\n    if not branch:\n        branch = 'v' + version\n    pkg['build_args'] = {\n        'FAISS_VERSION': version,\n        'FAISS_BRANCH': branch,",
        "detail": "packages.vectordb.faiss.config",
        "documentation": {}
    },
    {
        "label": "package",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.config",
        "description": "packages.vectordb.faiss.config",
        "peekOfCode": "package = [\n    faiss('1.7.3'),\n    faiss('1.7.4', default=True),\n    #faiss('v1.8.0'),  # encounters type_info build error sometime after be12427 (12/12/2023)\n    #faiss('be12427', default=True),  # known good build on JP5/JP6 from 12/12/2023\n]",
        "detail": "packages.vectordb.faiss.config",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 2,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "def search(queries, k=args.k):\n    time_begin = time.perf_counter()\n    D, I = index.search(queries, k) # sanity check\n    print(I)\n    print(D)\n    print(f\"time to search {len(queries)}:  {time.perf_counter()-time_begin:.3} sec\")\n\"\"\"\nSanity check on the first 5 vectors:\n[[  0 393 363  78]\n [  1 555 277 364]",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-k', type=int, default=4)\nparser.add_argument('-d', '--dim', type=int, default=64)  # 2621440\nparser.add_argument('--num-vectors', type=int, default=100000)  # 512\nparser.add_argument('--num-queries', type=int, default=1)\nparser.add_argument('--seed', type=int, default=1234)\nparser.add_argument('--cpu', action='store_true')\nargs = parser.parse_args()\nprint(args)\nnp.random.seed(args.seed)",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\nnp.random.seed(args.seed)\nprint(f\"building random numpy arrays ({args.num_vectors}, {args.dim})\")\nxb = np.random.random((args.num_vectors, args.dim)).astype('float32')\nxb[:, 0] += np.arange(args.num_vectors) / 1000.\nxq = np.random.random((args.num_queries, args.dim)).astype('float32')\nxq[:, 0] += np.arange(args.num_queries) / 1000.\nprint(f\"numpy array size:  {(xb.size * xb.itemsize) / (1024*1024):.3f} MB\")\nprint(f\"creating index\")",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "xb",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "xb = np.random.random((args.num_vectors, args.dim)).astype('float32')\nxb[:, 0] += np.arange(args.num_vectors) / 1000.\nxq = np.random.random((args.num_queries, args.dim)).astype('float32')\nxq[:, 0] += np.arange(args.num_queries) / 1000.\nprint(f\"numpy array size:  {(xb.size * xb.itemsize) / (1024*1024):.3f} MB\")\nprint(f\"creating index\")\nindex = faiss.IndexFlatL2(args.dim)   # build the index\nif not args.cpu:\n    res = faiss.StandardGpuResources()  # use a single GPU\n    index = faiss.index_cpu_to_gpu(res, 0, index)",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "xq",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "xq = np.random.random((args.num_queries, args.dim)).astype('float32')\nxq[:, 0] += np.arange(args.num_queries) / 1000.\nprint(f\"numpy array size:  {(xb.size * xb.itemsize) / (1024*1024):.3f} MB\")\nprint(f\"creating index\")\nindex = faiss.IndexFlatL2(args.dim)   # build the index\nif not args.cpu:\n    res = faiss.StandardGpuResources()  # use a single GPU\n    index = faiss.index_cpu_to_gpu(res, 0, index)\n# https://github.com/facebookresearch/faiss/wiki/FAQ#why-does-the-ram-usage-not-go-down-when-i-delete-an-index\nprint(f\"mem usage:  {faiss.get_mem_usage_kb() / 1024:.3f} MB\")",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "index = faiss.IndexFlatL2(args.dim)   # build the index\nif not args.cpu:\n    res = faiss.StandardGpuResources()  # use a single GPU\n    index = faiss.index_cpu_to_gpu(res, 0, index)\n# https://github.com/facebookresearch/faiss/wiki/FAQ#why-does-the-ram-usage-not-go-down-when-i-delete-an-index\nprint(f\"mem usage:  {faiss.get_mem_usage_kb() / 1024:.3f} MB\")\nprint(index.is_trained)\ntime_begin = time.perf_counter()\nindex.add(xb[:-1])                  # add vectors to the index\nprint(f\"time to add {xb.shape} vectors:  {time.perf_counter()-time_begin:.3} sec\")",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "time_begin",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "time_begin = time.perf_counter()\nindex.add(xb[:-1])                  # add vectors to the index\nprint(f\"time to add {xb.shape} vectors:  {time.perf_counter()-time_begin:.3} sec\")\nprint(index.ntotal)\ntime_begin = time.perf_counter()\nindex.add(xb[-1:])                  # add vectors to the index\nprint(f\"time to add 1 vector:  {time.perf_counter()-time_begin:.3} sec\")\nprint(index.ntotal)\ndef search(queries, k=args.k):\n    time_begin = time.perf_counter()",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "time_begin",
        "kind": 5,
        "importPath": "packages.vectordb.faiss.test",
        "description": "packages.vectordb.faiss.test",
        "peekOfCode": "time_begin = time.perf_counter()\nindex.add(xb[-1:])                  # add vectors to the index\nprint(f\"time to add 1 vector:  {time.perf_counter()-time_begin:.3} sec\")\nprint(index.ntotal)\ndef search(queries, k=args.k):\n    time_begin = time.perf_counter()\n    D, I = index.search(queries, k) # sanity check\n    print(I)\n    print(D)\n    print(f\"time to search {len(queries)}:  {time.perf_counter()-time_begin:.3} sec\")",
        "detail": "packages.vectordb.faiss.test",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-d', '--dim', type=int, default=1024, help='the dimensionality of the embedding vectors') \nparser.add_argument('-n', '--num-vectors', type=int, default=64, help='the number of vectors to add to the index')\nparser.add_argument('-k', type=int, default=4, help='the number of nearest-neighbors to find')\nparser.add_argument('--dtype', type=str, default='float32', choices=['float32', 'float16'], help='datatype of the vectors')\nparser.add_argument('--metric', type=str, default='l2', choices=DistanceMetrics, help='the distance metric to use during search')\nparser.add_argument('--num-queries', type=int, default=1) \nparser.add_argument('--seed', type=int, default=1234, help='change the random seed used')\nparser.add_argument('--runs', type=int, default=15)\nparser.add_argument('--warmup', type=int, default=3)",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\nnp.random.seed(args.seed)\nif args.dtype == 'float32':\n    dtype = np.float32\nelif args.dtype.lower() == 'float16':\n    dtype = np.float16\nelse:\n    raise ValueError(f\"unsupported dtype:  {args.dtype}\")\ndsize = np.dtype(dtype).itemsize     ",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "dsize",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "dsize = np.dtype(dtype).itemsize     \nprint(\"-- using datatype:\", dtype, dsize)\n_, device_props = cudaGetDeviceProperties(0)\nprint(f\"-- cuda device:  {device_props.name}\")\nxb = np.random.random((args.num_vectors, args.dim)).astype(dtype)\n#xb[:, 0] += np.arange(args.num_vectors) / 1000.\nxq = np.random.random((args.num_queries, 1, args.dim)).astype(dtype)\n#xq[:, 0] += np.arange(args.num_queries) / 1000.\nvector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB\nvectors = cudaAllocMapped((args.num_vectors, args.dim), dtype)",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "xb",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "xb = np.random.random((args.num_vectors, args.dim)).astype(dtype)\n#xb[:, 0] += np.arange(args.num_vectors) / 1000.\nxq = np.random.random((args.num_queries, 1, args.dim)).astype(dtype)\n#xq[:, 0] += np.arange(args.num_queries) / 1000.\nvector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB\nvectors = cudaAllocMapped((args.num_vectors, args.dim), dtype)\nqueries = cudaAllocMapped((args.num_queries, args.dim), dtype)\ndistances = cudaAllocMapped((args.num_queries, args.k), np.float32)\nindexes = cudaAllocMapped((args.num_queries, args.k), np.int64)\nfor n in range(args.num_vectors):",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "xq",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "xq = np.random.random((args.num_queries, 1, args.dim)).astype(dtype)\n#xq[:, 0] += np.arange(args.num_queries) / 1000.\nvector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB\nvectors = cudaAllocMapped((args.num_vectors, args.dim), dtype)\nqueries = cudaAllocMapped((args.num_queries, args.dim), dtype)\ndistances = cudaAllocMapped((args.num_queries, args.k), np.float32)\nindexes = cudaAllocMapped((args.num_queries, args.k), np.int64)\nfor n in range(args.num_vectors):\n    vectors.array[n] = xb[n]\nfor n in range(args.num_queries):",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "vector_size",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "vector_size = (args.dim * xb.itemsize) / (1024*1024)  # size of one vector in MB\nvectors = cudaAllocMapped((args.num_vectors, args.dim), dtype)\nqueries = cudaAllocMapped((args.num_queries, args.dim), dtype)\ndistances = cudaAllocMapped((args.num_queries, args.k), np.float32)\nindexes = cudaAllocMapped((args.num_queries, args.k), np.int64)\nfor n in range(args.num_vectors):\n    vectors.array[n] = xb[n]\nfor n in range(args.num_queries):\n    queries.array[n] = xq[n]\nvector_norms = None",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "vectors",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "vectors = cudaAllocMapped((args.num_vectors, args.dim), dtype)\nqueries = cudaAllocMapped((args.num_queries, args.dim), dtype)\ndistances = cudaAllocMapped((args.num_queries, args.k), np.float32)\nindexes = cudaAllocMapped((args.num_queries, args.k), np.int64)\nfor n in range(args.num_vectors):\n    vectors.array[n] = xb[n]\nfor n in range(args.num_queries):\n    queries.array[n] = xq[n]\nvector_norms = None\navg_l2_time = 0",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "queries",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "queries = cudaAllocMapped((args.num_queries, args.dim), dtype)\ndistances = cudaAllocMapped((args.num_queries, args.k), np.float32)\nindexes = cudaAllocMapped((args.num_queries, args.k), np.int64)\nfor n in range(args.num_vectors):\n    vectors.array[n] = xb[n]\nfor n in range(args.num_queries):\n    queries.array[n] = xq[n]\nvector_norms = None\navg_l2_time = 0\navg_l2_rate = 0",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "distances",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "distances = cudaAllocMapped((args.num_queries, args.k), np.float32)\nindexes = cudaAllocMapped((args.num_queries, args.k), np.int64)\nfor n in range(args.num_vectors):\n    vectors.array[n] = xb[n]\nfor n in range(args.num_queries):\n    queries.array[n] = xq[n]\nvector_norms = None\navg_l2_time = 0\navg_l2_rate = 0\nif args.metric == 'l2':",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "indexes",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "indexes = cudaAllocMapped((args.num_queries, args.k), np.int64)\nfor n in range(args.num_vectors):\n    vectors.array[n] = xb[n]\nfor n in range(args.num_queries):\n    queries.array[n] = xq[n]\nvector_norms = None\navg_l2_time = 0\navg_l2_rate = 0\nif args.metric == 'l2':\n    vector_norms = cudaAllocMapped(args.num_vectors, np.float32)",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "vector_norms",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "vector_norms = None\navg_l2_time = 0\navg_l2_rate = 0\nif args.metric == 'l2':\n    vector_norms = cudaAllocMapped(args.num_vectors, np.float32)\n    if not args.skip_validation:\n        assert(cudaL2Norm(\n            C.cast(vectors.ptr, C.c_void_p),\n            dsize, args.num_vectors, args.dim,\n            C.cast(vector_norms.ptr, C.POINTER(C.c_float)),",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_l2_time",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "avg_l2_time = 0\navg_l2_rate = 0\nif args.metric == 'l2':\n    vector_norms = cudaAllocMapped(args.num_vectors, np.float32)\n    if not args.skip_validation:\n        assert(cudaL2Norm(\n            C.cast(vectors.ptr, C.c_void_p),\n            dsize, args.num_vectors, args.dim,\n            C.cast(vector_norms.ptr, C.POINTER(C.c_float)),\n            True, None",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_l2_rate",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "avg_l2_rate = 0\nif args.metric == 'l2':\n    vector_norms = cudaAllocMapped(args.num_vectors, np.float32)\n    if not args.skip_validation:\n        assert(cudaL2Norm(\n            C.cast(vectors.ptr, C.c_void_p),\n            dsize, args.num_vectors, args.dim,\n            C.cast(vector_norms.ptr, C.POINTER(C.c_float)),\n            True, None\n        ))",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_time",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "avg_time = 0\nfor r in range(args.runs + args.warmup):\n    time_begin = time.perf_counter()\n    assert(cudaKNN(\n        C.cast(vectors.ptr, C.c_void_p),\n        C.cast(queries.ptr, C.c_void_p),\n        dsize,\n        args.num_vectors,\n        args.num_queries,\n        args.dim,",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "avg_rate",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "avg_rate = args.num_queries * 1000 / avg_time\nprint(\"\")\nprint(f\"N={args.num_vectors} M={args.num_queries} D={args.dim} K={args.k} metric={args.metric} dtype={args.dtype}\\n\")\nprint(f\"average search time:   {avg_time:.3f} ms, {avg_rate:.1f} vectors/sec, {avg_rate*vector_size:.1f} MB/s\")\nif args.metric == 'l2':\n    avg_l2_rate = 1000 / avg_l2_time\n    print(f\"average l2 norm time:  {avg_l2_time:.3f} ms, {avg_l2_rate:.1f} vectors/sec, {avg_l2_rate*vector_size:.1f} MB/s\")\nmemory_usage = (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \nprint(f\"peak memory usage:     {memory_usage:.1f} MB\")\nif args.save:",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "memory_usage",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.benchmark",
        "description": "packages.vectordb.faiss_lite.benchmark",
        "peekOfCode": "memory_usage = (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \nprint(f\"peak memory usage:     {memory_usage:.1f} MB\")\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, api, metric, dtype, vector_dim, num_vectors, \")\n            file.write(f\"index_batch, index_time, index_rate, index_bandwidth, \")\n            file.write(f\"search_batch, search_time, search_rate, search_bandwidth, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, faiss_lite, \")",
        "detail": "packages.vectordb.faiss_lite.benchmark",
        "documentation": {}
    },
    {
        "label": "AttrDict",
        "kind": 6,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "class AttrDict(dict):\n    \"\"\"\n    A dict where keys are available as attributes\n    https://stackoverflow.com/a/14620633\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\nclass cudaArrayInterface():\n    \"\"\"",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaArrayInterface",
        "kind": 6,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "class cudaArrayInterface():\n    \"\"\"\n    Exposes __cuda_array_interface__ - typically used as a temporary view into a larger buffer\n    https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html\n    \"\"\"\n    def __init__(self, data, shape, dtype=np.float32):\n        self.__cuda_array_interface__ = {\n            'data': (data, False),  # R/W\n            'shape': shape,\n            'typestr': np.dtype(dtype).str,",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "dtype_to_ctype",
        "kind": 2,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "def dtype_to_ctype(dtype):\n    if isinstance(dtype, str):\n        dtype = np.dtype(dtype)\n    if dtype == np.float16:\n        return C.c_ushort\n    else:\n        return np.ctypeslib.as_ctypes_type(dtype)\ndef cudaAllocMapped(shape, dtype, map_numpy=True, map_torch=True, return_dict=True):\n    \"\"\"\n    Allocate cudaMallocManaged() memory and map it to a numpy array and PyTorch tensor",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaAllocMapped",
        "kind": 2,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "def cudaAllocMapped(shape, dtype, map_numpy=True, map_torch=True, return_dict=True):\n    \"\"\"\n    Allocate cudaMallocManaged() memory and map it to a numpy array and PyTorch tensor\n    If return dict is true, these will be returned in a dict-like DictAttr object\n    with keys for 'ptr', 'array' (if map_numpy is True), and 'tensor' (if map_torch is True).\n    Otherwise, a tuple will be returned with (ptr, array, tensor)\n    \"\"\"\n    dsize = np.dtype(dtype).itemsize\n    if isinstance(shape, int):\n        size = shape * dsize",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaToNumpy",
        "kind": 2,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "def cudaToNumpy(ptr, shape, dtype):\n    \"\"\"\n    Map a shared CUDA pointer into np.ndarray with the given shape and datatype.\n    The pointer should have been allocated with cudaMallocManaged() or using cudaHostAllocMapped,\n    and the user is responsible for any CPU/GPU synchronization (i.e. by using cudaStreams)\n    \"\"\"\n    array = np.ctypeslib.as_array(C.cast(ptr, C.POINTER(dtype_to_ctype(dtype))), shape=shape)\n    if dtype == np.float16:\n        array.dtype = np.float16\n    return array",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaToTorch",
        "kind": 2,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "def cudaToTorch(ptr, shape, dtype):\n    \"\"\"\n    Map a shared CUDA pointer into np.ndarray with the given shape and datatype.\n    The pointer should have been allocated with cudaMallocManaged() or using cudaHostAllocMapped,\n    and the user is responsible for any CPU/GPU synchronization (i.e. by using cudaStreams)\n    \"\"\"\n    return torch.as_tensor(cudaArrayInterface(ptr, shape, dtype), device='cuda')\ndef assert_cuda(err):\n    \"\"\"\n    Throw a runtime exception if a CUDA error occurred",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "assert_cuda",
        "kind": 2,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "def assert_cuda(err):\n    \"\"\"\n    Throw a runtime exception if a CUDA error occurred\n    \"\"\"\n    if isinstance(err, tuple) and len(err) == 1:\n        err = err[0]\n    if isinstance(err, cuda.CUresult):\n        if err != cuda.CUresult.CUDA_SUCCESS:\n            raise RuntimeError(f\"CUDA Error {err} -- {cudaGetErrorString(err)[1]}\")\n    elif isinstance(err, cudaError_t):",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "_lib",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "_lib = C.CDLL('/opt/faiss_lite/build/libfaiss_lite.so')\n# https://github.com/facebookresearch/faiss/blob/main/faiss/MetricType.h\nDistanceMetrics = {\n    'inner_product': 0,\n    'l2': 1,\n    'l1': 2,\n    'Linf': 3,\n    'canberra': 20,\n    'braycurtis': 21,\n    'jensenshannon': 22,",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "DistanceMetrics",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "DistanceMetrics = {\n    'inner_product': 0,\n    'l2': 1,\n    'l1': 2,\n    'Linf': 3,\n    'canberra': 20,\n    'braycurtis': 21,\n    'jensenshannon': 22,\n    'jaccard': 23,\n    'cosine': 99,  # custom: normalized inner_product",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaKNN",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "cudaKNN = _cudaKNN()\ncudaL2Norm = _cudaL2Norm()\nclass AttrDict(dict):\n    \"\"\"\n    A dict where keys are available as attributes\n    https://stackoverflow.com/a/14620633\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "cudaL2Norm",
        "kind": 5,
        "importPath": "packages.vectordb.faiss_lite.faiss_lite",
        "description": "packages.vectordb.faiss_lite.faiss_lite",
        "peekOfCode": "cudaL2Norm = _cudaL2Norm()\nclass AttrDict(dict):\n    \"\"\"\n    A dict where keys are available as attributes\n    https://stackoverflow.com/a/14620633\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\nclass cudaArrayInterface():",
        "detail": "packages.vectordb.faiss_lite.faiss_lite",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def load_image(data_path: str, mode=\"rgb\") -> np.ndarray:\n    img = Image.open(data_path)\n    if mode == \"rgb\":\n        img = img.convert(\"RGB\")\n    return np.array(img)\ndef cat_images(image_list: List[np.ndarray], axis=1, pad=20) -> np.ndarray:\n    shape_list = [image.shape for image in image_list]\n    max_h = max([shape[0] for shape in shape_list]) + pad * 2\n    max_w = max([shape[1] for shape in shape_list]) + pad * 2\n    for i, image in enumerate(image_list):",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "cat_images",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def cat_images(image_list: List[np.ndarray], axis=1, pad=20) -> np.ndarray:\n    shape_list = [image.shape for image in image_list]\n    max_h = max([shape[0] for shape in shape_list]) + pad * 2\n    max_w = max([shape[1] for shape in shape_list]) + pad * 2\n    for i, image in enumerate(image_list):\n        canvas = np.zeros((max_h, max_w, 3), dtype=np.uint8)\n        h, w, _ = image.shape\n        crop_y = (max_h - h) // 2\n        crop_x = (max_w - w) // 2\n        canvas[crop_y : crop_y + h, crop_x : crop_x + w] = image",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "show_anns",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def show_anns(anns) -> None:\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    img = np.ones((sorted_anns[0][\"segmentation\"].shape[0], sorted_anns[0][\"segmentation\"].shape[1], 4))\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        m = ann[\"segmentation\"]",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "draw_binary_mask",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def draw_binary_mask(raw_image: np.ndarray, binary_mask: np.ndarray, mask_color=(0, 0, 255)) -> np.ndarray:\n    color_mask = np.zeros_like(raw_image, dtype=np.uint8)\n    color_mask[binary_mask == 1] = mask_color\n    mix = color_mask * 0.5 + raw_image * (1 - 0.5)\n    binary_mask = np.expand_dims(binary_mask, axis=2)\n    canvas = binary_mask * mix + (1 - binary_mask) * raw_image\n    canvas = np.asarray(canvas, dtype=np.uint8)\n    return canvas\ndef draw_bbox(\n    image: np.ndarray,",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "draw_bbox",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def draw_bbox(\n    image: np.ndarray,\n    bbox: List[List[int]],\n    color: str or List[str] = \"g\",\n    linewidth=1,\n    tmp_name=\".tmp.png\",\n) -> np.ndarray:\n    dpi = 300\n    oh, ow, _ = image.shape\n    plt.close()",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "draw_scatter",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def draw_scatter(\n    image: np.ndarray,\n    points: List[List[int]],\n    color: str or List[str] = \"g\",\n    marker=\"*\",\n    s=10,\n    ew=0.25,\n    tmp_name=\".tmp.png\",\n) -> np.ndarray:\n    dpi = 300",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "get_max_rss",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def get_max_rss():  # peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"l2\")\n    parser.add_argument(\"--weight_url\", type=str, default=\"/data/models/efficientvit/sam/l2.pt\")\n    parser.add_argument(\"--multimask\", action=\"store_true\")\n    parser.add_argument(\"--image_path\", type=str, default=\"assets/fig/cat.jpg\")\n    parser.add_argument(\"--output_path\", type=str, default=\"/data/benchmarks/efficientvit_sam_demo.png\")\n    parser.add_argument('-i', '--images', action='append', nargs='*', help=\"Paths to images to test\")",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "packages.vit.efficientvit.benchmark",
        "description": "packages.vit.efficientvit.benchmark",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"l2\")\n    parser.add_argument(\"--weight_url\", type=str, default=\"/data/models/efficientvit/sam/l2.pt\")\n    parser.add_argument(\"--multimask\", action=\"store_true\")\n    parser.add_argument(\"--image_path\", type=str, default=\"assets/fig/cat.jpg\")\n    parser.add_argument(\"--output_path\", type=str, default=\"/data/benchmarks/efficientvit_sam_demo.png\")\n    parser.add_argument('-i', '--images', action='append', nargs='*', help=\"Paths to images to test\")\n    parser.add_argument(\"--mode\", type=str, default=\"box\", choices=[\"point\", \"box\", \"all\"])\n    parser.add_argument(\"--point\", type=str, default=None)",
        "detail": "packages.vit.efficientvit.benchmark",
        "documentation": {}
    },
    {
        "label": "get_max_rss",
        "kind": 2,
        "importPath": "packages.vit.nanosam.benchmark",
        "description": "packages.vit.nanosam.benchmark",
        "peekOfCode": "def get_max_rss():  # peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \n# Instantiate TensorRT predictor\npredictor = Predictor(\n    args.image_encoder,\n    args.mask_decoder\n)\navg_encoder=0\navg_latency=0\npil_image=None",
        "detail": "packages.vit.nanosam.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.vit.nanosam.benchmark",
        "description": "packages.vit.nanosam.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"--image_encoder\", type=str, default=\"/opt/nanosam/data/resnet18_image_encoder.engine\")\nparser.add_argument(\"--mask_decoder\", type=str, default=\"/opt/nanosam/data/mobile_sam_mask_decoder.engine\")\nparser.add_argument('-i', '--images', action='append', nargs='*', help=\"Paths to images to test\")\nparser.add_argument('-r', '--runs', type=int, default=2, help=\"Number of inferencing runs to do (for timing)\")\nparser.add_argument('-w', '--warmup', type=int, default=1, help='the number of warmup iterations')\nparser.add_argument('-s', '--save', type=str, default='', help='CSV file to save benchmarking results to')\nargs = parser.parse_args()\nif not args.images:\n    args.images = [",
        "detail": "packages.vit.nanosam.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.vit.nanosam.benchmark",
        "description": "packages.vit.nanosam.benchmark",
        "peekOfCode": "args = parser.parse_args()\nif not args.images:\n    args.images = [\n        \"/opt/nanosam/assets/dogs.jpg\",\n        \"/data/images/hoover.jpg\",\n        \"/data/images/lake.jpg\",\n    ]\nprint(args)\ndef get_max_rss():  # peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  ",
        "detail": "packages.vit.nanosam.benchmark",
        "documentation": {}
    },
    {
        "label": "predictor",
        "kind": 5,
        "importPath": "packages.vit.nanosam.benchmark",
        "description": "packages.vit.nanosam.benchmark",
        "peekOfCode": "predictor = Predictor(\n    args.image_encoder,\n    args.mask_decoder\n)\navg_encoder=0\navg_latency=0\npil_image=None\nmask=None\nfor run in range(args.runs + args.warmup):\n    for image in args.images:",
        "detail": "packages.vit.nanosam.benchmark",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "packages.vit.nanosam.benchmark",
        "description": "packages.vit.nanosam.benchmark",
        "peekOfCode": "mask = (mask[0, 0] > 0).detach().cpu().numpy()\n# Draw resykts\nplt.imshow(pil_image)\nplt.imshow(mask, alpha=0.5)\nx = [bbox[0], bbox[2], bbox[2], bbox[0], bbox[0]]\ny = [bbox[1], bbox[1], bbox[3], bbox[3], bbox[1]]\nplt.plot(x, y, 'g-')\nplt.savefig(\"data/benchmark_last_image.jpg\")\nif args.save:\n    if not os.path.isfile(args.save):  # csv header",
        "detail": "packages.vit.nanosam.benchmark",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "packages.vit.nanosam.benchmark",
        "description": "packages.vit.nanosam.benchmark",
        "peekOfCode": "x = [bbox[0], bbox[2], bbox[2], bbox[0], bbox[0]]\ny = [bbox[1], bbox[1], bbox[3], bbox[3], bbox[1]]\nplt.plot(x, y, 'g-')\nplt.savefig(\"data/benchmark_last_image.jpg\")\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, api, image_encoder, mask_decoder, time_encoder, latency, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, \")",
        "detail": "packages.vit.nanosam.benchmark",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "packages.vit.nanosam.benchmark",
        "description": "packages.vit.nanosam.benchmark",
        "peekOfCode": "y = [bbox[1], bbox[1], bbox[3], bbox[3], bbox[1]]\nplt.plot(x, y, 'g-')\nplt.savefig(\"data/benchmark_last_image.jpg\")\nif args.save:\n    if not os.path.isfile(args.save):  # csv header\n        with open(args.save, 'w') as file:\n            file.write(f\"timestamp, hostname, api, image_encoder, mask_decoder, time_encoder, latency, memory\\n\")\n    with open(args.save, 'a') as file:\n        file.write(f\"{datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')}, {socket.gethostname()}, \")\n        file.write(f\"nanosam-python, {args.image_encoder}, {args.mask_decoder}, {avg_encoder}, {avg_latency}, {memory_usage}\\n\")",
        "detail": "packages.vit.nanosam.benchmark",
        "documentation": {}
    },
    {
        "label": "download_from_url",
        "kind": 2,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "def download_from_url(url, filename=None):\n    if filename is None:\n        filename = os.path.basename(urlparse(url).path)\n    if not os.path.isfile(filename):\n        response = requests.get(url, stream=True)\n        total_size_in_bytes= int(response.headers.get('content-length', 0))\n        block_size = 1024 # 1Kibibyte\n        print(f\"Downloading {filename} :\")\n        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n        with open(filename, 'wb') as file:",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "get_max_rss",
        "kind": 2,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "def get_max_rss():  # peak memory usage in MB (max RSS - https://stackoverflow.com/a/7669482)\n    return (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss + resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss) / 1024  \ndef save_anns(cv2_image, anns):\n    plt.imshow(cv2_image)\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "save_anns",
        "kind": 2,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "def save_anns(cv2_image, anns):\n    plt.imshow(cv2_image)\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n    img[:,:,3] = 0\n    for ann in sorted_anns:",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"--checkpoint\", type=str, default=\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\")\nparser.add_argument('-i', '--images', action='append', nargs='*', help=\"Paths to images to test\")\nparser.add_argument('-r', '--runs', type=int, default=2, help=\"Number of inferencing runs to do (for timing)\")\nparser.add_argument('-w', '--warmup', type=int, default=1, help='the number of warmup iterations')\nparser.add_argument('-s', '--save', type=str, default='', help='CSV file to save benchmarking results to')\nargs = parser.parse_args()\nif not args.images:\n    args.images = [\n        \"https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\",",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "args = parser.parse_args()\nif not args.images:\n    args.images = [\n        \"https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\",\n        \"https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\",\n        \"https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\",\n    ]\nelse:\n    args.images = [x[0] for x in args.images]\nprint(args)",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "CHECKPOINT_URL",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\nFILENAME = os.path.basename(urlparse(args.checkpoint).path)\ndownload_from_url(args.checkpoint, FILENAME)\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nimagepaths = []",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "FILENAME",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "FILENAME = os.path.basename(urlparse(args.checkpoint).path)\ndownload_from_url(args.checkpoint, FILENAME)\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nimagepaths = []\nfor imageurl in args.images:",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "sam_checkpoint",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nimagepaths = []\nfor imageurl in args.images:\n    imagepaths.append(download_from_url(imageurl))\nfor run in range(args.runs + args.warmup):",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "model_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nimagepaths = []\nfor imageurl in args.images:\n    imagepaths.append(download_from_url(imageurl))\nfor run in range(args.runs + args.warmup):\n    for imagepath in imagepaths:",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "device = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nimagepaths = []\nfor imageurl in args.images:\n    imagepaths.append(download_from_url(imageurl))\nfor run in range(args.runs + args.warmup):\n    for imagepath in imagepaths:\n        cv2_image = cv2.imread(imagepath)",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "sam",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nimagepaths = []\nfor imageurl in args.images:\n    imagepaths.append(download_from_url(imageurl))\nfor run in range(args.runs + args.warmup):\n    for imagepath in imagepaths:\n        cv2_image = cv2.imread(imagepath)\n        cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "mask_generator",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "mask_generator = SamAutomaticMaskGenerator(sam)\nimagepaths = []\nfor imageurl in args.images:\n    imagepaths.append(download_from_url(imageurl))\nfor run in range(args.runs + args.warmup):\n    for imagepath in imagepaths:\n        cv2_image = cv2.imread(imagepath)\n        cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n        time_begin=time.perf_counter()\n        masks = mask_generator.generate(cv2_image)",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    },
    {
        "label": "imagepaths",
        "kind": 5,
        "importPath": "packages.vit.sam.benchmark",
        "description": "packages.vit.sam.benchmark",
        "peekOfCode": "imagepaths = []\nfor imageurl in args.images:\n    imagepaths.append(download_from_url(imageurl))\nfor run in range(args.runs + args.warmup):\n    for imagepath in imagepaths:\n        cv2_image = cv2.imread(imagepath)\n        cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n        time_begin=time.perf_counter()\n        masks = mask_generator.generate(cv2_image)\n        time_elapsed=time.perf_counter() - time_begin",
        "detail": "packages.vit.sam.benchmark",
        "documentation": {}
    }
]