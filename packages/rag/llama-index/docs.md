
* llama-index from https://www.llamaindex.ai/

### Starting llama-index container

```bash
jetson-containers run $(./autotag llama-index)
```

This will start the `ollama` server as well as Jupyter Lab server inside the container.

### Running a starter RAG example with Ollama (Jupyter notebook)

This is based on the [official tutorial for local models](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/).

On your web browser, open <>, and open `LlamaIndex_Local-Models.ipynb`.

Follow the guide on the Jupyter notebook.